{"2025-09-11T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2509.09674v1","updated":"2025-09-11T17:59:17Z","published":"2025-09-11T17:59:17Z","title":"SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning","summary":"  Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL\n","authors":["Haozhan Li","Yuxin Zuo","Jiale Yu","Yuhao Zhang","Zhaohui Yang","Kaiyan Zhang","Xuekai Zhu","Yuchen Zhang","Tianxing Chen","Ganqu Cui","Dehui Wang","Dingxiang Luo","Yuchen Fan","Youbang Sun","Jia Zeng","Jiangmiao Pang","Shanghang Zhang","Yu Wang","Yao Mu","Bowen Zhou","Ning Ding"],"pdf_url":"https://arxiv.org/pdf/2509.09674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09671v1","updated":"2025-09-11T17:59:07Z","published":"2025-09-11T17:59:07Z","title":"Dexplore: Scalable Neural Control for Dexterous Manipulation from\n  Reference-Scoped Exploration","summary":"  Hand-object motion-capture (MoCap) repositories offer large-scale,\ncontact-rich demonstrations and hold promise for scaling dexterous robotic\nmanipulation. Yet demonstration inaccuracies and embodiment gaps between human\nand robot hands limit the straightforward use of these data. Existing methods\nadopt a three-stage workflow, including retargeting, tracking, and residual\ncorrection, which often leaves demonstrations underused and compound errors\nacross stages. We introduce Dexplore, a unified single-loop optimization that\njointly performs retargeting and tracking to learn robot control policies\ndirectly from MoCap at scale. Rather than treating demonstrations as ground\ntruth, we use them as soft guidance. From raw trajectories, we derive adaptive\nspatial scopes, and train with reinforcement learning to keep the policy\nin-scope while minimizing control effort and accomplishing the task. This\nunified formulation preserves demonstration intent, enables robot-specific\nstrategies to emerge, improves robustness to noise, and scales to large\ndemonstration corpora. We distill the scaled tracking policy into a\nvision-based, skill-conditioned generative controller that encodes diverse\nmanipulation skills in a rich latent representation, supporting generalization\nacross objects and real-world deployment. Taken together, these contributions\nposition Dexplore as a principled bridge that transforms imperfect\ndemonstrations into effective training signals for dexterous manipulation.\n","authors":["Sirui Xu","Yu-Wei Chao","Liuyu Bian","Arsalan Mousavian","Yu-Xiong Wang","Liang-Yan Gui","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2509.09671v1.pdf","comment":"CoRL 2025"},{"id":"http://arxiv.org/abs/2509.09613v1","updated":"2025-09-11T16:55:00Z","published":"2025-09-11T16:55:00Z","title":"MOFU: Development of a MOrphing Fluffy Unit with Expansion and\n  Contraction Capabilities and Evaluation of the Animacy of Its Movements","summary":"  Robots for therapy and social interaction are often intended to evoke\n\"animacy\" in humans. While many robots imitate appearance and joint movements,\nlittle attention has been given to whole-body expansion-contraction,\nvolume-changing movements observed in living organisms, and their effect on\nanimacy perception. We developed a mobile robot called \"MOFU (Morphing Fluffy\nUnit),\" capable of whole-body expansion-contraction with a single motor and\ncovered with a fluffy exterior. MOFU employs a \"Jitterbug\" structure, a\ngeometric transformation mechanism that enables smooth volume change in\ndiameter from 210 to 280 mm using one actuator. It is also equipped with a\ndifferential two-wheel drive mechanism for locomotion. To evaluate the effect\nof expansion-contraction movements, we conducted an online survey using videos\nof MOFU's behavior. Participants rated impressions with the Godspeed\nQuestionnaire Series. First, we compared videos of MOFU in a stationary state\nwith and without expansion-contraction and turning, finding that\nexpansion-contraction significantly increased perceived animacy. Second, we\nhypothesized that presenting two MOFUs would increase animacy compared with a\nsingle robot; however, this was not supported, as no significant difference\nemerged. Exploratory analyses further compared four dual-robot motion\nconditions. Third, when expansion-contraction was combined with locomotion,\nanimacy ratings were higher than locomotion alone. These results suggest that\nvolume-changing movements such as expansion and contraction enhance perceived\nanimacy in robots and should be considered an important design element in\nfuture robot development aimed at shaping human impressions.\n","authors":["Taisei Mogi","Mari Saito","Yoshihiro Nakata"],"pdf_url":"https://arxiv.org/pdf/2509.09613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09594v1","updated":"2025-09-11T16:34:17Z","published":"2025-09-11T16:34:17Z","title":"ObjectReact: Learning Object-Relative Control for Visual Navigation","summary":"  Visual navigation using only a single camera and a topological map has\nrecently become an appealing alternative to methods that require additional\nsensors and 3D maps. This is typically achieved through an \"image-relative\"\napproach to estimating control from a given pair of current observation and\nsubgoal image. However, image-level representations of the world have\nlimitations because images are strictly tied to the agent's pose and\nembodiment. In contrast, objects, being a property of the map, offer an\nembodiment- and trajectory-invariant world representation. In this work, we\npresent a new paradigm of learning \"object-relative\" control that exhibits\nseveral desirable characteristics: a) new routes can be traversed without\nstrictly requiring to imitate prior experience, b) the control prediction\nproblem can be decoupled from solving the image matching problem, and c) high\ninvariance can be achieved in cross-embodiment deployment for variations across\nboth training-testing and mapping-execution settings. We propose a topometric\nmap representation in the form of a \"relative\" 3D scene graph, which is used to\nobtain more informative object-level global path planning costs. We train a\nlocal controller, dubbed \"ObjectReact\", conditioned directly on a high-level\n\"WayObject Costmap\" representation that eliminates the need for an explicit RGB\ninput. We demonstrate the advantages of learning object-relative control over\nits image-relative counterpart across sensor height variations and multiple\nnavigation tasks that challenge the underlying spatial understanding\ncapability, e.g., navigating a map trajectory in the reverse direction. We\nfurther show that our sim-only policy is able to generalize well to real-world\nindoor environments. Code and supplementary material are accessible via project\npage: https://object-react.github.io/\n","authors":["Sourav Garg","Dustin Craggs","Vineeth Bhat","Lachlan Mares","Stefan Podgorski","Madhava Krishna","Feras Dayoub","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2509.09594v1.pdf","comment":"CoRL 2025; 23 pages including appendix"},{"id":"http://arxiv.org/abs/2507.14456v4","updated":"2025-09-11T16:24:09Z","published":"2025-07-19T03:04:28Z","title":"GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for\n  End-to-End Autonomous Driving","summary":"  End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert and a Scene-Adaptive Experts Group, equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves both adaptability and robustness across diverse scenarios.\nGEMINUS outperforms existing methods in the Bench2Drive closed-loop benchmark\nand achieves state-of-the-art performance in Driving Score and Success Rate,\neven with only monocular vision input. The code is available at\nhttps://github.com/newbrains1/GEMINUS.\n","authors":["Chi Wan","Yixin Cui","Jiatong Du","Shuo Yang","Yulong Bai","Peng Yi","Nan Li","Yanjun Huang"],"pdf_url":"https://arxiv.org/pdf/2507.14456v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09584v1","updated":"2025-09-11T16:21:59Z","published":"2025-09-11T16:21:59Z","title":"Visual Grounding from Event Cameras","summary":"  Event cameras capture changes in brightness with microsecond precision and\nremain reliable under motion blur and challenging illumination, offering clear\nadvantages for modeling highly dynamic scenes. Yet, their integration with\nnatural language understanding has received little attention, leaving a gap in\nmultimodal perception. To address this, we introduce Talk2Event, the first\nlarge-scale benchmark for language-driven object grounding using event data.\nBuilt on real-world driving scenarios, Talk2Event comprises 5,567 scenes,\n13,458 annotated objects, and more than 30,000 carefully validated referring\nexpressions. Each expression is enriched with four structured attributes --\nappearance, status, relation to the viewer, and relation to surrounding objects\n-- that explicitly capture spatial, temporal, and relational cues. This\nattribute-centric design supports interpretable and compositional grounding,\nenabling analysis that moves beyond simple object recognition to contextual\nreasoning in dynamic environments. We envision Talk2Event as a foundation for\nadvancing multimodal and temporally-aware perception, with applications\nspanning robotics, human-AI interaction, and so on.\n","authors":["Lingdong Kong","Dongyue Lu","Ao Liang","Rong Li","Yuhao Dong","Tianshuai Hu","Lai Xing Ng","Wei Tsang Ooi","Benoit R. Cottereau"],"pdf_url":"https://arxiv.org/pdf/2509.09584v1.pdf","comment":"Abstract Paper (Non-Archival) @ ICCV 2025 NeVi Workshop"},{"id":"http://arxiv.org/abs/2509.07996v2","updated":"2025-09-11T15:54:05Z","published":"2025-09-04T17:59:58Z","title":"3D and 4D World Modeling: A Survey","summary":"  World modeling has become a cornerstone in AI research, enabling agents to\nunderstand, represent, and predict the dynamic environments they inhabit. While\nprior work largely emphasizes generative methods for 2D image and video data,\nthey overlook the rapidly growing body of work that leverages native 3D and 4D\nrepresentations such as RGB-D imagery, occupancy grids, and LiDAR point clouds\nfor large-scale scene modeling. At the same time, the absence of a standardized\ndefinition and taxonomy for ``world models'' has led to fragmented and\nsometimes inconsistent claims in the literature. This survey addresses these\ngaps by presenting the first comprehensive review explicitly dedicated to 3D\nand 4D world modeling and generation. We establish precise definitions,\nintroduce a structured taxonomy spanning video-based (VideoGen),\noccupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and\nsystematically summarize datasets and evaluation metrics tailored to 3D/4D\nsettings. We further discuss practical applications, identify open challenges,\nand highlight promising research directions, aiming to provide a coherent and\nfoundational reference for advancing the field. A systematic summary of\nexisting literature is available at https://github.com/worldbench/survey\n","authors":["Lingdong Kong","Wesley Yang","Jianbiao Mei","Youquan Liu","Ao Liang","Dekai Zhu","Dongyue Lu","Wei Yin","Xiaotao Hu","Mingkai Jia","Junyuan Deng","Kaiwen Zhang","Yang Wu","Tianyi Yan","Shenyuan Gao","Song Wang","Linfeng Li","Liang Pan","Yong Liu","Jianke Zhu","Wei Tsang Ooi","Steven C. H. Hoi","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2509.07996v2.pdf","comment":"Survey; 34 pages, 10 figures, 14 tables; GitHub Repo at\n  https://github.com/worldbench/survey"},{"id":"http://arxiv.org/abs/2509.09546v1","updated":"2025-09-11T15:38:31Z","published":"2025-09-11T15:38:31Z","title":"A Neuromorphic Incipient Slip Detection System using Papillae Morphology","summary":"  Detecting incipient slip enables early intervention to prevent object\nslippage and enhance robotic manipulation safety. However, deploying such\nsystems on edge platforms remains challenging, particularly due to energy\nconstraints. This work presents a neuromorphic tactile sensing system based on\nthe NeuroTac sensor with an extruding papillae-based skin and a spiking\nconvolutional neural network (SCNN) for slip-state classification. The SCNN\nmodel achieves 94.33% classification accuracy across three classes (no slip,\nincipient slip, and gross slip) in slip conditions induced by sensor motion.\nUnder the dynamic gravity-induced slip validation conditions, after temporal\nsmoothing of the SCNN's final-layer spike counts, the system detects incipient\nslip at least 360 ms prior to gross slip across all trials, consistently\nidentifying incipient slip before gross slip occurs. These results demonstrate\nthat this neuromorphic system has stable and responsive incipient slip\ndetection capability.\n","authors":["Yanhui Lu","Zeyu Deng","Stephen J. Redmond","Efi Psomopoulou","Benjamin Ward-Cherrier"],"pdf_url":"https://arxiv.org/pdf/2509.09546v1.pdf","comment":"7 pages, 12 figures. Submitted to IEEE Robotics and Automation\n  Letters (RAL), under review"},{"id":"http://arxiv.org/abs/2508.17986v2","updated":"2025-09-11T15:22:48Z","published":"2025-08-25T12:55:38Z","title":"No Need to Look! Locating and Grasping Objects by a Robot Arm Covered\n  with Sensitive Skin","summary":"  Locating and grasping of objects by robots is typically performed using\nvisual sensors. Haptic feedback from contacts with the environment is only\nsecondary if present at all. In this work, we explored an extreme case of\nsearching for and grasping objects in complete absence of visual input, relying\non haptic feedback only. The main novelty lies in the use of contacts over the\ncomplete surface of a robot manipulator covered with sensitive skin. The search\nis divided into two phases: (1) coarse workspace exploration with the complete\nrobot surface, followed by (2) precise localization using the end-effector\nequipped with a force/torque sensor. We systematically evaluated this method in\nsimulation and on the real robot, demonstrating that diverse objects can be\nlocated, grasped, and put in a basket. The overall success rate on the real\nrobot for one object was 85.7% with failures mainly while grasping specific\nobjects. The method using whole-body contacts is six times faster compared to a\nbaseline that uses haptic feedback only on the end-effector. We also show\nlocating and grasping multiple objects on the table. This method is not\nrestricted to our specific setup and can be deployed on any platform with the\nability of sensing contacts over the entire body surface. This work holds\npromise for diverse applications in areas with challenging visual perception\n(due to lighting, dust, smoke, occlusion) such as in agriculture when fruits or\nvegetables need to be located inside foliage and picked.\n","authors":["Karel Bartunek","Lukas Rustler","Matej Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2508.17986v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2411.11405v3","updated":"2025-09-11T15:11:27Z","published":"2024-11-18T09:27:49Z","title":"Extended Neural Contractive Dynamical Systems: On Multiple Tasks and\n  Riemannian Safety Regions","summary":"  Stability guarantees are crucial when ensuring that a fully autonomous robot\ndoes not take undesirable or potentially harmful actions. We recently proposed\nthe Neural Contractive Dynamical Systems (NCDS), which is a neural network\narchitecture that guarantees contractive stability. With this,\nlearning-from-demonstrations approaches can trivially provide stability\nguarantees. However, our early work left several unanswered questions, which we\nhere address. Beyond providing an in-depth explanation of NCDS, this paper\nextends the framework with more careful regularization, a conditional variant\nof the framework for handling multiple tasks, and an uncertainty-driven\napproach to latent obstacle avoidance. Experiments verify that the developed\nsystem has the flexibility of ordinary neural networks while providing the\nstability guarantees needed for autonomous robotics.\n","authors":["Hadi Beik Mohammadi","Søren Hauberg","Georgios Arvanitidis","Gerhard Neumann","Leonel Rozo"],"pdf_url":"https://arxiv.org/pdf/2411.11405v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2401.09352"},{"id":"http://arxiv.org/abs/2506.04867v2","updated":"2025-09-11T14:52:08Z","published":"2025-06-05T10:38:28Z","title":"LLMs for sensory-motor control: Combining in-context and iterative\n  learning","summary":"  We propose a method that enables large language models (LLMs) to control\nembodied agents by directly mapping continuous observation vectors to\ncontinuous action vectors. At the outset, the LLMs generate a control strategy\nbased on a textual description of the agent, its environment, and the intended\ngoal. This strategy is then iteratively refined through a learning process in\nwhich the LLMs are repeatedly prompted to improve the current strategy, using\nperformance feedback and sensory-motor data collected during its evaluation.\nThe method is validated on classic control tasks from the Gymnasium library and\nthe inverted pendulum task from the MuJoCo library. The approach proves\neffective with relatively compact models such as Gpt-oss:120b and Qwen2.5:72b.\nIn most cases, it successfully identifies optimal or near-optimal solutions by\nintegrating symbolic knowledge derived through reasoning with sub-symbolic\nsensory-motor data gathered as the agent interacts with its environment.\n","authors":["Jônata Tyska Carvalho","Stefano Nolfi"],"pdf_url":"https://arxiv.org/pdf/2506.04867v2.pdf","comment":"Article updated with results from gpt-oss:120b. 24 pages (13 pages\n  are from appendix), 6 figures, code for experiments replication and\n  supplementary material provided at\n  https://github.com/jtyska/llm-robotics-article/"},{"id":"http://arxiv.org/abs/2509.09509v1","updated":"2025-09-11T14:50:56Z","published":"2025-09-11T14:50:56Z","title":"SMapper: A Multi-Modal Data Acquisition Platform for SLAM Benchmarking","summary":"  Advancing research in fields like Simultaneous Localization and Mapping\n(SLAM) and autonomous navigation critically depends on reliable and\nreproducible multimodal datasets. While several influential datasets have\ndriven progress in these domains, they often suffer from limitations in sensing\nmodalities, environmental diversity, and the reproducibility of the underlying\nhardware setups. To address these challenges, this paper introduces SMapper, a\nnovel open-hardware, multi-sensor platform designed explicitly for, though not\nlimited to, SLAM research. The device integrates synchronized LiDAR,\nmulti-camera, and inertial sensing, supported by a robust calibration and\nsynchronization pipeline that ensures precise spatio-temporal alignment across\nmodalities. Its open and replicable design allows researchers to extend its\ncapabilities and reproduce experiments across both handheld and robot-mounted\nscenarios. To demonstrate its practicality, we additionally release\nSMapper-light, a publicly available SLAM dataset containing representative\nindoor and outdoor sequences. The dataset includes tightly synchronized\nmultimodal data and ground-truth trajectories derived from offline LiDAR-based\nSLAM with sub-centimeter accuracy, alongside dense 3D reconstructions.\nFurthermore, the paper contains benchmarking results on state-of-the-art LiDAR\nand visual SLAM frameworks using the SMapper-light dataset. By combining\nopen-hardware design, reproducible data collection, and comprehensive\nbenchmarking, SMapper establishes a robust foundation for advancing SLAM\nalgorithm development, evaluation, and reproducibility.\n","authors":["Pedro Miguel Bastos Soares","Ali Tourani","Miguel Fernandez-Cortizas","Asier Bikandi Noya","Jose Luis Sanchez-Lopez","Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2509.09509v1.pdf","comment":"12 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2509.09484v1","updated":"2025-09-11T14:15:20Z","published":"2025-09-11T14:15:20Z","title":"BagIt! An Adaptive Dual-Arm Manipulation of Fabric Bags for Object\n  Bagging","summary":"  Bagging tasks, commonly found in industrial scenarios, are challenging\nconsidering deformable bags' complicated and unpredictable nature. This paper\npresents an automated bagging system from the proposed adaptive\nStructure-of-Interest (SOI) manipulation strategy for dual robot arms. The\nsystem dynamically adjusts its actions based on real-time visual feedback,\nremoving the need for pre-existing knowledge of bag properties. Our framework\nincorporates Gaussian Mixture Models (GMM) for estimating SOI states,\noptimization techniques for SOI generation, motion planning via Constrained\nBidirectional Rapidly-exploring Random Tree (CBiRRT), and dual-arm coordination\nusing Model Predictive Control (MPC). Extensive experiments validate the\ncapability of our system to perform precise and robust bagging across various\nobjects, showcasing its adaptability. This work offers a new solution for\nrobotic deformable object manipulation (DOM), particularly in automated bagging\ntasks. Video of this work is available at https://youtu.be/6JWjCOeTGiQ.\n","authors":["Peng Zhou","Jiaming Qi","Hongmin Wu","Chen Wang","Yizhou Chen","Zeqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.09484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04308v3","updated":"2025-09-11T12:49:34Z","published":"2025-03-06T10:51:04Z","title":"Shaken, Not Stirred: A Novel Dataset for Visual Understanding of Glasses\n  in Human-Robot Bartending Tasks","summary":"  Datasets for object detection often do not account for enough variety of\nglasses, due to their transparent and reflective properties. Specifically,\nopen-vocabulary object detectors, widely used in embodied robotic agents, fail\nto distinguish subclasses of glasses. This scientific gap poses an issue for\nrobotic applications that suffer from accumulating errors between detection,\nplanning, and action execution. This paper introduces a novel method for\nacquiring real-world data from RGB-D sensors that minimizes human effort. We\npropose an auto-labeling pipeline that generates labels for all the acquired\nframes based on the depth measurements. We provide a novel real-world glass\nobject dataset GlassNICOLDataset that was collected on the Neuro-Inspired\nCOLlaborator (NICOL), a humanoid robot platform. The dataset consists of 7850\nimages recorded from five different cameras. We show that our trained baseline\nmodel outperforms state-of-the-art open-vocabulary approaches. In addition, we\ndeploy our baseline model in an embodied agent approach to the NICOL platform,\non which it achieves a success rate of 81% in a human-robot bartending\nscenario.\n","authors":["Lukáš Gajdošech","Hassan Ali","Jan-Gerrit Habekost","Martin Madaras","Matthias Kerzel","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2503.04308v3.pdf","comment":"Submitted and Accepted for Presentation at the IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2509.01106v2","updated":"2025-09-11T12:40:54Z","published":"2025-09-01T03:53:47Z","title":"Robix: A Unified Model for Robot Interaction, Reasoning and Planning","summary":"  We introduce Robix, a unified model that integrates robot reasoning, task\nplanning, and natural language interaction within a single vision-language\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\nsystem, Robix dynamically generates atomic commands for the low-level\ncontroller and verbal responses for human interaction, enabling robots to\nfollow complex instructions, plan long-horizon tasks, and interact naturally\nwith human within an end-to-end framework. Robix further introduces novel\ncapabilities such as proactive dialogue, real-time interruption handling, and\ncontext-aware commonsense reasoning during task execution. At its core, Robix\nleverages chain-of-thought reasoning and adopts a three-stage training\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\nabilities including 3D spatial understanding, visual grounding, and\ntask-centric reasoning; (2) supervised finetuning to model human-robot\ninteraction and task planning as a unified reasoning-action sequence; and (3)\nreinforcement learning to improve reasoning-action consistency and long-horizon\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\ninteractive task execution, demonstrating strong generalization across diverse\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\ninterrupted) and various user-involved tasks such as table bussing, grocery\nshopping, and dietary filtering.\n","authors":["Huang Fang","Mengxi Zhang","Heng Dong","Wei Li","Zixuan Wang","Qifeng Zhang","Xueyun Tian","Yucheng Hu","Hang Li"],"pdf_url":"https://arxiv.org/pdf/2509.01106v2.pdf","comment":"Tech report. Project page: https://robix-seed.github.io/robix/"},{"id":"http://arxiv.org/abs/2509.09404v1","updated":"2025-09-11T12:38:02Z","published":"2025-09-11T12:38:02Z","title":"A Hybrid Hinge-Beam Continuum Robot with Passive Safety Capping for\n  Real-Time Fatigue Awareness","summary":"  Cable-driven continuum robots offer high flexibility and lightweight design,\nmaking them well-suited for tasks in constrained and unstructured environments.\nHowever, prolonged use can induce mechanical fatigue from plastic deformation\nand material degradation, compromising performance and risking structural\nfailure. In the state of the art, fatigue estimation of continuum robots\nremains underexplored, limiting long-term operation. To address this, we\npropose a fatigue-aware continuum robot with three key innovations: (1) a\nHybrid Hinge-Beam structure where TwistBeam and BendBeam decouple torsion and\nbending: passive revolute joints in the BendBeam mitigate stress concentration,\nwhile TwistBeam's limited torsional deformation reduces BendBeam stress\nmagnitude, enhancing durability; (2) a Passive Stopper that safely constrains\nmotion via mechanical constraints and employs motor torque sensing to detect\ncorresponding limit torque, ensuring safety and enabling data collection; and\n(3) a real-time fatigue-awareness method that estimates stiffness from motor\ntorque at the limit pose, enabling online fatigue estimation without additional\nsensors. Experiments show that the proposed design reduces fatigue accumulation\nby about 49% compared with a conventional design, while passive mechanical\nlimiting combined with motor-side sensing allows accurate estimation of\nstructural fatigue and damage. These results confirm the effectiveness of the\nproposed architecture for safe and reliable long-term operation.\n","authors":["Tongshun Chen","Zezhou Sun","Yanhan Sun","Yuhao Wang","Dezhen Song","Ke Wu"],"pdf_url":"https://arxiv.org/pdf/2509.09404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.05728v2","updated":"2025-09-11T11:47:58Z","published":"2025-09-06T14:21:27Z","title":"LiDAR-BIND-T: Improved and Temporally Consistent Sensor Modality\n  Translation and Fusion for Robotic Applications","summary":"  This paper extends LiDAR-BIND, a modular multi-modal fusion framework that\nbinds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,\nwith mechanisms that explicitly enforce temporal consistency. We introduce\nthree contributions: (i) temporal embedding similarity that aligns consecutive\nlatent representations, (ii) a motion-aligned transformation loss that matches\ndisplacement between predictions and ground truth LiDAR, and (iii) windowed\ntemporal fusion using a specialised temporal module. We further update the\nmodel architecture to better preserve spatial structure. Evaluations on\nradar/sonar-to-LiDAR translation demonstrate improved temporal and spatial\ncoherence, yielding lower absolute trajectory error and better occupancy map\naccuracy in Cartographer-based SLAM (Simultaneous Localisation and Mapping). We\npropose different metrics based on the Fr\\'echet Video Motion Distance (FVMD)\nand a correlation-peak distance metric providing practical temporal quality\nindicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or\nLiDAR-BIND-T, maintains plug-and-play modality fusion while substantially\nenhancing temporal stability, resulting in improved robustness and performance\nfor downstream SLAM.\n","authors":["Niels Balemans","Ali Anwar","Jan Steckel","Siegfried Mercelis"],"pdf_url":"https://arxiv.org/pdf/2509.05728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09372v1","updated":"2025-09-11T11:42:21Z","published":"2025-09-11T11:42:21Z","title":"VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action\n  Model","summary":"  Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.\n","authors":["Yihao Wang","Pengxiang Ding","Lingxiao Li","Can Cui","Zirui Ge","Xinyang Tong","Wenxuan Song","Han Zhao","Wei Zhao","Pengxu Hou","Siteng Huang","Yifan Tang","Wenhui Wang","Ru Zhang","Jianyi Liu","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2509.09372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09364v1","updated":"2025-09-11T11:29:35Z","published":"2025-09-11T11:29:35Z","title":"AGILOped: Agile Open-Source Humanoid Robot for Research","summary":"  With academic and commercial interest for humanoid robots peaking, multiple\nplatforms are being developed. Through a high level of customization, they\nshowcase impressive performance. Most of these systems remain closed-source or\nhave high acquisition and maintenance costs, however. In this work, we present\nAGILOped - an open-source humanoid robot that closes the gap between high\nperformance and accessibility. Our robot is driven by off-the-shelf\nbackdrivable actuators with high power density and uses standard electronic\ncomponents. With a height of 110 cm and weighing only 14.5 kg, AGILOped can be\noperated without a gantry by a single person. Experiments in walking, jumping,\nimpact mitigation and getting-up demonstrate its viability for use in research.\n","authors":["Grzegorz Ficht","Luis Denninger","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2509.09364v1.pdf","comment":"10th IEEE International Conference on Advanced Robotics and\n  Mechatronics (ARM), Portsmouth, UK, August 2025"},{"id":"http://arxiv.org/abs/2504.11580v3","updated":"2025-09-11T11:29:16Z","published":"2025-04-15T19:56:53Z","title":"RESPLE: Recursive Spline Estimation for LiDAR-Based Odometry","summary":"  We present a novel recursive Bayesian estimation framework using B-splines\nfor continuous-time 6-DoF dynamic motion estimation. The state vector consists\nof a recurrent set of position control points and orientation control point\nincrements, enabling efficient estimation via a modified iterated extended\nKalman filter without involving error-state formulations. The resulting\nrecursive spline estimator (RESPLE) is further leveraged to develop a versatile\nsuite of direct LiDAR-based odometry solutions, supporting the integration of\none or multiple LiDARs and an IMU. We conduct extensive real-world evaluations\nusing public datasets and our own experiments, covering diverse sensor setups,\nplatforms, and environments. Compared to existing systems, RESPLE achieves\ncomparable or superior estimation accuracy and robustness, while attaining\nreal-time efficiency. Our results and analysis demonstrate RESPLE's strength in\nhandling highly dynamic motions and complex scenes within a lightweight and\nflexible design, showing strong potential as a universal framework for\nmulti-sensor motion estimation. We release the source code and experimental\ndatasets at https://github.com/ASIG-X/RESPLE .\n","authors":["Ziyu Cao","William Talbot","Kailai Li"],"pdf_url":"https://arxiv.org/pdf/2504.11580v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09356v1","updated":"2025-09-11T11:10:08Z","published":"2025-09-11T11:10:08Z","title":"Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement\n  Learning","summary":"  Navigating and understanding complex and unknown environments autonomously\ndemands more than just basic perception and movement from embodied agents.\nTruly effective exploration requires agents to possess higher-level cognitive\nabilities, the ability to reason about their surroundings, and make more\ninformed decisions regarding exploration strategies. However, traditional RL\napproaches struggle to balance efficient exploration and semantic understanding\ndue to limited cognitive capabilities embedded in the small policies for the\nagents, leading often to human drivers when dealing with semantic exploration.\nIn this paper, we address this challenge by presenting a novel Deep\nReinforcement Learning (DRL) architecture that is specifically designed for\nresource efficient semantic exploration. A key methodological contribution is\nthe integration of a Vision-Language Model (VLM) common-sense through a layered\nreward function. The VLM query is modeled as a dedicated action, allowing the\nagent to strategically query the VLM only when deemed necessary for gaining\nexternal guidance, thereby conserving resources. This mechanism is combined\nwith a curriculum learning strategy designed to guide learning at different\nlevels of complexity to ensure robust and stable learning. Our experimental\nevaluation results convincingly demonstrate that our agent achieves\nsignificantly enhanced object discovery rates and develops a learned capability\nto effectively navigate towards semantically rich regions. Furthermore, it also\nshows a strategic mastery of when to prompt for external environmental\ninformation. By demonstrating a practical and scalable method for embedding\ncommon-sense semantic reasoning with autonomous agents, this research provides\na novel approach to pursuing a fully intelligent and self-guided exploration in\nrobotics.\n","authors":["Abdel Hakim Drid","Vincenzo Suriani","Daniele Nardi","Abderrezzak Debilou"],"pdf_url":"https://arxiv.org/pdf/2509.09356v1.pdf","comment":"The 19th International Conference on Intelligent Autonomous Systems\n  (IAS 19), 2025, Genoa"},{"id":"http://arxiv.org/abs/2509.09349v1","updated":"2025-09-11T11:05:14Z","published":"2025-09-11T11:05:14Z","title":"Classification of Driver Behaviour Using External Observation Techniques\n  for Autonomous Vehicles","summary":"  Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behavior classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviors such as excessive lateral movement and erratic trajectory patterns by\nimplementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioral analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions.\n","authors":["Ian Nell","Shane Gilroy"],"pdf_url":"https://arxiv.org/pdf/2509.09349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09332v1","updated":"2025-09-11T10:32:22Z","published":"2025-09-11T10:32:22Z","title":"OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and\n  Embodiment-aware Reasoning","summary":"  Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible.To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io\n","authors":["Yuecheng Liu","Dafeng Chi","Shiguang Wu","Zhanguang Zhang","Yuzheng Zhuang","Bowen Yang","He Zhu","Lingfeng Zhang","Pengwei Xie","David Gamaliel Arcos Bravo","Yingxue Zhang","Jianye Hao","Xingyue Quan"],"pdf_url":"https://arxiv.org/pdf/2509.09332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09297v1","updated":"2025-09-11T09:40:06Z","published":"2025-09-11T09:40:06Z","title":"Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable\n  UAV Perception","summary":"  Open-set detection is crucial for robust UAV autonomy in air-to-air object\ndetection under real-world conditions. Traditional closed-set detectors degrade\nsignificantly under domain shifts and flight data corruption, posing risks to\nsafety-critical applications. We propose a novel, model-agnostic open-set\ndetection framework designed specifically for embedding-based detectors. The\nmethod explicitly handles unknown object rejection while maintaining robustness\nagainst corrupted flight data. It estimates semantic uncertainty via entropy\nmodeling in the embedding space and incorporates spectral normalization and\ntemperature scaling to enhance open-set discrimination. We validate our\napproach on the challenging AOT aerial benchmark and through extensive\nreal-world flight tests. Comprehensive ablation studies demonstrate consistent\nimprovements over baseline methods, achieving up to a 10\\% relative AUROC gain\ncompared to standard YOLO-based detectors. Additionally, we show that\nbackground rejection further strengthens robustness without compromising\ndetection accuracy, making our solution particularly well-suited for reliable\nUAV perception in dynamic air-to-air environments.\n","authors":["Spyridon Loukovitis","Anastasios Arsenos","Vasileios Karampinis","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2509.09297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09283v1","updated":"2025-09-11T09:17:50Z","published":"2025-09-11T09:17:50Z","title":"RENet: Fault-Tolerant Motion Control for Quadruped Robots via Redundant\n  Estimator Networks under Visual Collapse","summary":"  Vision-based locomotion in outdoor environments presents significant\nchallenges for quadruped robots. Accurate environmental prediction and\neffective handling of depth sensor noise during real-world deployment remain\ndifficult, severely restricting the outdoor applications of such algorithms. To\naddress these deployment challenges in vision-based motion control, this letter\nproposes the Redundant Estimator Network (RENet) framework. The framework\nemploys a dual-estimator architecture that ensures robust motion performance\nwhile maintaining deployment stability during onboard vision failures. Through\nan online estimator adaptation, our method enables seamless transitions between\nestimation modules when handling visual perception uncertainties. Experimental\nvalidation on a real-world robot demonstrates the framework's effectiveness in\ncomplex outdoor environments, showing particular advantages in scenarios with\ndegraded visual perception. This framework demonstrates its potential as a\npractical solution for reliable robotic deployment in challenging field\nconditions. Project website: https://RENet-Loco.github.io/\n","authors":["Yueqi Zhang","Quancheng Qian","Taixian Hou","Peng Zhai","Xiaoyi Wei","Kangmai Hu","Jiafu Yi","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.09283v1.pdf","comment":"Accepted for IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2507.23682v2","updated":"2025-09-11T09:15:53Z","published":"2025-07-31T15:57:46Z","title":"villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models","summary":"  Visual-Language-Action (VLA) models have emerged as a popular paradigm for\nlearning robot manipulation policies that can follow language instructions and\ngeneralize to novel scenarios. Recent work has begun to explore the\nincorporation of latent actions, an abstract representation of visual change\nbetween two frames, into VLA pre-training. In this paper, we introduce villa-X,\na novel Visual-Language-Latent-Action (ViLLA) framework that advances latent\naction modeling for learning generalizable robot manipulation policies. Our\napproach improves both how latent actions are learned and how they are\nincorporated into VLA pre-training. Together, these contributions enable\nvilla-X to achieve superior performance across simulated environments including\nSIMPLER and LIBERO, as well as on two real-world robot setups including gripper\nand dexterous hand manipulation. We believe the ViLLA paradigm holds\nsignificant promise, and that our villa-X provides a strong foundation for\nfuture research.\n","authors":["Xiaoyu Chen","Hangxing Wei","Pushi Zhang","Chuheng Zhang","Kaixin Wang","Yanjiang Guo","Rushuai Yang","Yucen Wang","Xinquan Xiao","Li Zhao","Jianyu Chen","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2507.23682v2.pdf","comment":"Project page: https://aka.ms/villa-x"},{"id":"http://arxiv.org/abs/2503.03509v2","updated":"2025-09-11T08:46:27Z","published":"2025-03-05T13:57:05Z","title":"Sampling-Based Multi-Modal Multi-Robot Multi-Goal Path Planning","summary":"  In many robotics applications, multiple robots are working in a shared\nworkspace to complete a set of tasks as fast as possible. Such settings can be\ntreated as multi-modal multi-robot multi-goal path planning problems, where\neach robot has to reach a set of goals. Existing approaches to this type of\nproblem solve this using prioritization or assume synchronous task completion,\nand are thus neither optimal nor complete. We formalize this problem as a\nsingle centralized path planning problem and present planners that are\nprobabilistically complete and asymptotically optimal. The planners plan in the\ncomposite space of all robots and are modifications of standard sampling-based\nplanners with the required changes to work in our multi-modal, multi-robot,\nmulti-goal setting. We validate the planners on a diverse range of problems\nincluding scenarios with various robots, planning horizons, and collaborative\ntasks such as handovers, and compare the planners against a suboptimal\nprioritized planner.\n  Videos and code for the planners and the benchmark is available at\nhttps://vhartmann.com/mrmg-planning/.\n","authors":["Valentin N. Hartmann","Tirza Heinle","Yijiang Huang","Stelian Coros"],"pdf_url":"https://arxiv.org/pdf/2503.03509v2.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2509.09238v1","updated":"2025-09-11T08:20:30Z","published":"2025-09-11T08:20:30Z","title":"Global Optimization of Stochastic Black-Box Functions with Arbitrary\n  Noise Distributions using Wilson Score Kernel Density Estimation","summary":"  Many optimization problems in robotics involve the optimization of\ntime-expensive black-box functions, such as those involving complex simulations\nor evaluation of real-world experiments. Furthermore, these functions are often\nstochastic as repeated experiments are subject to unmeasurable disturbances.\nBayesian optimization can be used to optimize such methods in an efficient\nmanner by deploying a probabilistic function estimator to estimate with a given\nconfidence so that regions of the search space can be pruned away.\nConsequently, the success of the Bayesian optimization depends on the function\nestimator's ability to provide informative confidence bounds. Existing function\nestimators require many function evaluations to infer the underlying confidence\nor depend on modeling of the disturbances. In this paper, it is shown that the\nconfidence bounds provided by the Wilson Score Kernel Density Estimator\n(WS-KDE) are applicable as excellent bounds to any stochastic function with an\noutput confined to the closed interval [0;1] regardless of the distribution of\nthe output. This finding opens up the use of WS-KDE for stable global\noptimization on a wider range of cost functions. The properties of WS-KDE in\nthe context of Bayesian optimization are demonstrated in simulation and applied\nto the problem of automated trap design for vibrational part feeders.\n","authors":["Thorbjørn Mosekjær Iversen","Lars Carøe Sørensen","Simon Faarvang Mathiesen","Henrik Gordon Petersen"],"pdf_url":"https://arxiv.org/pdf/2509.09238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09210v1","updated":"2025-09-11T07:36:54Z","published":"2025-09-11T07:36:54Z","title":"ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint\n  Multi-agent Motion Forecasting","summary":"  Accurate motion prediction of surrounding agents is crucial for the safe\nplanning of autonomous vehicles. Recent advancements have extended prediction\ntechniques from individual agents to joint predictions of multiple interacting\nagents, with various strategies to address complex interactions within future\nmotions of agents. However, these methods overlook the evolving nature of these\ninteractions. To address this limitation, we propose a novel progressive\nmulti-scale decoding strategy, termed ProgD, with the help of dynamic\nheterogeneous graph-based scenario modeling. In particular, to explicitly and\ncomprehensively capture the evolving social interactions in future scenarios,\ngiven their inherent uncertainty, we design a progressive modeling of scenarios\nwith dynamic heterogeneous graphs. With the unfolding of such dynamic\nheterogeneous graphs, a factorized architecture is designed to process the\nspatio-temporal dependencies within future scenarios and progressively\neliminate uncertainty in future motions of multiple agents. Furthermore, a\nmulti-scale decoding procedure is incorporated to improve on the future\nscenario modeling and consistent prediction of agents' future motion. The\nproposed ProgD achieves state-of-the-art performance on the INTERACTION\nmulti-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2\nmulti-world forecasting benchmark.\n","authors":["Xing Gao","Zherui Huang","Weiyao Lin","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2509.09210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09206v1","updated":"2025-09-11T07:29:19Z","published":"2025-09-11T07:29:19Z","title":"Occupancy-aware Trajectory Planning for Autonomous Valet Parking in\n  Uncertain Dynamic Environments","summary":"  Accurately reasoning about future parking spot availability and integrated\nplanning is critical for enabling safe and efficient autonomous valet parking\nin dynamic, uncertain environments. Unlike existing methods that rely solely on\ninstantaneous observations or static assumptions, we present an approach that\npredicts future parking spot occupancy by explicitly distinguishing between\ninitially vacant and occupied spots, and by leveraging the predicted motion of\ndynamic agents. We introduce a probabilistic spot occupancy estimator that\nincorporates partial and noisy observations within a limited Field-of-View\n(FoV) model and accounts for the evolving uncertainty of unobserved regions.\nCoupled with this, we design a strategy planner that adaptively balances\ngoal-directed parking maneuvers with exploratory navigation based on\ninformation gain, and intelligently incorporates wait-and-go behaviors at\npromising spots. Through randomized simulations emulating large parking lots,\nwe demonstrate that our framework significantly improves parking efficiency,\nsafety margins, and trajectory smoothness compared to existing approaches.\n","authors":["Farhad Nawaz","Faizan M. Tariq","Sangjae Bae","David Isele","Avinash Singh","Nadia Figueroa","Nikolai Matni","Jovin D'sa"],"pdf_url":"https://arxiv.org/pdf/2509.09206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.19805v3","updated":"2025-09-11T07:09:42Z","published":"2025-08-27T11:47:29Z","title":"Beyond Pairwise Comparisons: Unveiling Structural Landscape of Mobile\n  Robot Models","summary":"  Understanding the computational power of mobile robot systems is a\nfundamental challenge in distributed computing. While prior work has focused on\npairwise separations between models, we explore how robot capabilities, light\nobservability, and scheduler synchrony interact in more complex ways.\n  We first show that the Exponential Times Expansion (ETE) problem is solvable\nonly in the strongest model -- fully-synchronous robots with full mutual lights\n($\\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and\nTAR(d)* problems to demonstrate how internal memory and lights interact with\nsynchrony: under weak synchrony, internal memory alone is insufficient, while\nfull synchrony can substitute for both lights and memory.\n  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and\nZCC to show fine-grained separations between $\\mathcal{FSTA}$ and\n$\\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and\nLeave Place Convergence (LP-Cv), illustrating the limitations of internal\nmemory in symmetric settings.\n  These results extend the known separation map of 14 canonical robot models,\nrevealing structural phenomena only visible through higher-order comparisons.\nOur work provides new impossibility criteria and deepens the understanding of\nhow observability, memory, and synchrony collectively shape the computational\npower of mobile robots.\n","authors":["Shota Naito","Tsukasa Ninomiya","Koichi Wada"],"pdf_url":"https://arxiv.org/pdf/2508.19805v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17434v2","updated":"2025-09-11T06:17:45Z","published":"2025-02-24T18:59:50Z","title":"V-HOP: Visuo-Haptic 6D Object Pose Tracking","summary":"  Humans naturally integrate vision and haptics for robust object perception\nduring manipulation. The loss of either modality significantly degrades\nperformance. Inspired by this multisensory integration, prior object pose\nestimation research has attempted to combine visual and haptic/tactile\nfeedback. Although these works demonstrate improvements in controlled\nenvironments or synthetic datasets, they often underperform vision-only\napproaches in real-world settings due to poor generalization across diverse\ngrippers, sensor layouts, or sim-to-real environments. Furthermore, they\ntypically estimate the object pose for each frame independently, resulting in\nless coherent tracking over sequences in real-world deployments. To address\nthese limitations, we introduce a novel unified haptic representation that\neffectively handles multiple gripper embodiments. Building on this\nrepresentation, we introduce a new visuo-haptic transformer-based object pose\ntracker that seamlessly integrates visual and haptic input. We validate our\nframework in our dataset and the Feelsight dataset, demonstrating significant\nperformance improvement on challenging sequences. Notably, our method achieves\nsuperior generalization and robustness across novel embodiments, objects, and\nsensor types (both taxel-based and vision-based tactile sensors). In real-world\nexperiments, we demonstrate that our approach outperforms state-of-the-art\nvisual trackers by a large margin. We further show that we can achieve precise\nmanipulation tasks by incorporating our real-time object tracking result into\nmotion plans, underscoring the advantages of visuo-haptic perception. Project\nwebsite: https://ivl.cs.brown.edu/research/v-hop\n","authors":["Hongyu Li","Mingxi Jia","Tuluhan Akbulut","Yu Xiang","George Konidaris","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2502.17434v2.pdf","comment":"Accepted by RSS 2025"},{"id":"http://arxiv.org/abs/2509.09141v1","updated":"2025-09-11T04:29:48Z","published":"2025-09-11T04:29:48Z","title":"AEOS: Active Environment-aware Optimal Scanning Control for UAV\n  LiDAR-Inertial Odometry in Complex Scenes","summary":"  LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)\nare fundamentally limited by the narrow field of view (FoV) of compact LiDAR\nsensors and the payload constraints that preclude multi-sensor configurations.\nTraditional motorized scanning systems with fixed-speed rotations lack scene\nawareness and task-level adaptability, leading to degraded odometry and mapping\nperformance in complex, occluded environments. Inspired by the active sensing\nbehavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),\na biologically inspired and computationally efficient framework for adaptive\nLiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines model\npredictive control (MPC) and reinforcement learning (RL) in a hybrid\narchitecture: an analytical uncertainty model predicts future pose\nobservability for exploitation, while a lightweight neural network learns an\nimplicit cost map from panoramic depth representations to guide exploration. To\nsupport scalable training and generalization, we develop a point cloud-based\nsimulation environment with real-world LiDAR maps across diverse scenes,\nenabling sim-to-real transfer. Extensive experiments in both simulation and\nreal-world environments demonstrate that AEOS significantly improves odometry\naccuracy compared to fixed-rate, optimization-only, and fully learned\nbaselines, while maintaining real-time performance under onboard computational\nconstraints. The project page can be found at\nhttps://kafeiyin00.github.io/AEOS/.\n","authors":["Jianping Li","Xinhang Xu","Zhongyuan Liu","Shenghai Yuan","Muqing Cao","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2509.09141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.13459v3","updated":"2025-09-11T04:17:20Z","published":"2025-08-19T02:33:15Z","title":"Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and\n  Algorithms","summary":"  The ``Last Mile Challenge'' has long been considered an important, yet\nunsolved, challenge for autonomous vehicles, public service robots, and\ndelivery robots. A central issue in this challenge is the ability of robots to\nnavigate constrained and cluttered environments that have high agency (e.g.,\ndoorways, hallways, corridor intersections), often while competing for space\nwith other robots and humans. We refer to these environments as ``Social\nMini-Games'' (SMGs). Traditional navigation approaches designed for MRN do not\nperform well in SMGs, which has led to focused research on dedicated SMG\nsolvers. However, publications on SMG navigation research make different\nassumptions (on centralized versus decentralized, observability, communication,\ncooperation, etc.), and have different objective functions (safety versus\nliveness). These assumptions and objectives are sometimes implicitly assumed or\ndescribed informally. This makes it difficult to establish appropriate\nbaselines for comparison in research papers, as well as making it difficult for\npractitioners to find the papers relevant to their concrete application. Such\nad-hoc representation of the field also presents a barrier to new researchers\nwanting to start research in this area. SMG navigation research requires its\nown taxonomy, definitions, and evaluation protocols to guide effective research\nmoving forward. This survey is the first to catalog SMG solvers using a\nwell-defined and unified taxonomy and to classify existing methods accordingly.\nIt also discusses the essential properties of SMG solvers, defines what SMGs\nare and how they appear in practice, outlines how to evaluate SMG solvers, and\nhighlights the differences between SMG solvers and general navigation systems.\nThe survey concludes with an overview of future directions and open challenges\nin the field. Our project is open-sourced at\nhttps://socialminigames.github.io/.\n","authors":["Rohan Chandra","Shubham Singh","Wenhao Luo","Katia Sycara"],"pdf_url":"https://arxiv.org/pdf/2508.13459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07815v3","updated":"2025-09-11T03:49:17Z","published":"2025-05-12T17:59:11Z","title":"Imagine, Verify, Execute: Memory-guided Agentic Exploration with\n  Vision-Language Models","summary":"  Exploration is essential for general-purpose robotic learning, especially in\nopen-ended environments where dense rewards, explicit goals, or task-specific\nsupervision are scarce. Vision-language models (VLMs), with their semantic\nreasoning over objects, spatial relations, and potential outcomes, present a\ncompelling foundation for generating high-level exploratory behaviors. However,\ntheir outputs are often ungrounded, making it difficult to determine whether\nimagined transitions are physically feasible or informative. To bridge the gap\nbetween imagination and execution, we present IVE (Imagine, Verify, Execute),\nan agentic exploration framework inspired by human curiosity. Human exploration\nis often driven by the desire to discover novel scene configurations and to\ndeepen understanding of the environment. Similarly, IVE leverages VLMs to\nabstract RGB-D observations into semantic scene graphs, imagine novel scenes,\npredict their physical plausibility, and generate executable skill sequences\nthrough action tools. We evaluate IVE in both simulated and real-world tabletop\nenvironments. The results show that IVE enables more diverse and meaningful\nexploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the\nentropy of visited states. Moreover, the collected experience supports\ndownstream learning, producing policies that closely match or exceed the\nperformance of those trained on human-collected demonstrations.\n","authors":["Seungjae Lee","Daniel Ekpo","Haowen Liu","Furong Huang","Abhinav Shrivastava","Jia-Bin Huang"],"pdf_url":"https://arxiv.org/pdf/2505.07815v3.pdf","comment":"Project webpage: https://ive-robot.github.io/"},{"id":"http://arxiv.org/abs/2506.00455v3","updated":"2025-09-11T03:02:39Z","published":"2025-05-31T08:22:09Z","title":"Diffusion Graph Neural Networks for Robustness in Olfaction Sensors and\n  Datasets","summary":"  Robotic odour source localization (OSL) is a critical capability for\nautonomous systems operating in complex environments. However, current OSL\nmethods often suffer from ambiguities, particularly when robots misattribute\nodours to incorrect objects due to limitations in olfactory datasets and sensor\nresolutions. To address this challenge, we introduce a novel machine learning\nmethod using diffusion-based molecular generation to enhance odour localization\naccuracy that can be used by itself or with automated olfactory dataset\nconstruction pipelines. This generative process of our diffusion model expands\nthe chemical space beyond the limitations of both current olfactory datasets\nand training methods, enabling the identification of potential odourant\nmolecules not previously documented. The generated molecules can then be more\naccurately validated using advanced olfactory sensors, enabling them to detect\nmore compounds and inform better hardware design. By integrating visual\nanalysis, language processing, and molecular generation, our framework enhances\nthe ability of olfaction-vision models on robots to accurately associate odours\nwith their correct sources, thereby improving navigation and decision-making\nthrough better sensor selection for a target compound in critical applications\nsuch as explosives detection, narcotics screening, and search and rescue. Our\nmethodology represents a foundational advancement in the field of artificial\nolfaction, offering a scalable solution to challenges posed by limited\nolfactory data and sensor ambiguities. Code and data are made available to the\ncommunity at the following URL:\nhttps://github.com/KordelFranceTech/OlfactionVisionLanguage-Dataset.\n","authors":["Kordel K. France","Ovidiu Daescu"],"pdf_url":"https://arxiv.org/pdf/2506.00455v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09106v1","updated":"2025-09-11T02:35:08Z","published":"2025-09-11T02:35:08Z","title":"LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion\n  in Bipedal Robots","summary":"  Achieving stable and robust perceptive locomotion for bipedal robots in\nunstructured outdoor environments remains a critical challenge due to complex\nterrain geometry and susceptibility to external disturbances. In this work, we\npropose a novel reward design inspired by the Linear Inverted Pendulum Model\n(LIPM) to enable perceptive and stable locomotion in the wild. The LIPM\nprovides theoretical guidance for dynamic balance by regulating the center of\nmass (CoM) height and the torso orientation. These are key factors for\nterrain-aware locomotion, as they help ensure a stable viewpoint for the\nrobot's camera. Building on this insight, we design a reward function that\npromotes balance and dynamic stability while encouraging accurate CoM\ntrajectory tracking. To adaptively trade off between velocity tracking and\nstability, we leverage the Reward Fusion Module (RFM) approach that prioritizes\nstability when needed. A double-critic architecture is adopted to separately\nevaluate stability and locomotion objectives, improving training efficiency and\nrobustness. We validate our approach through extensive experiments on a bipedal\nrobot in both simulation and real-world outdoor environments. The results\ndemonstrate superior terrain adaptability, disturbance rejection, and\nconsistent performance across a wide range of speeds and perceptual conditions.\n","authors":["Haokai Su","Haoxiang Luo","Shunpeng Yang","Kaiwen Jiang","Wei Zhang","Hua Chen"],"pdf_url":"https://arxiv.org/pdf/2509.09106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08257v2","updated":"2025-09-11T02:34:59Z","published":"2025-09-10T03:28:18Z","title":"Symmetry-Guided Multi-Agent Inverse Reinforcement Learning","summary":"  In robotic systems, the performance of reinforcement learning depends on the\nrationality of predefined reward functions. However, manually designed reward\nfunctions often lead to policy failures due to inaccuracies. Inverse\nReinforcement Learning (IRL) addresses this problem by inferring implicit\nreward functions from expert demonstrations. Nevertheless, existing methods\nrely heavily on large amounts of expert demonstrations to accurately recover\nthe reward function. The high cost of collecting expert demonstrations in\nrobotic applications, particularly in multi-robot systems, severely hinders the\npractical deployment of IRL. Consequently, improving sample efficiency has\nemerged as a critical challenge in multi-agent inverse reinforcement learning\n(MIRL). Inspired by the symmetry inherent in multi-agent systems, this work\ntheoretically demonstrates that leveraging symmetry enables the recovery of\nmore accurate reward functions. Building upon this insight, we propose a\nuniversal framework that integrates symmetry into existing multi-agent\nadversarial IRL algorithms, thereby significantly enhancing sample efficiency.\nExperimental results from multiple challenging tasks have demonstrated the\neffectiveness of this framework. Further validation in physical multi-robot\nsystems has shown the practicality of our method.\n","authors":["Yongkai Tian","Yirong Qi","Xin Yu","Wenjun Wu","Jie Luo"],"pdf_url":"https://arxiv.org/pdf/2509.08257v2.pdf","comment":"8pages, 6 figures. Accepted for publication in the Proceedings of the\n  2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025) as oral presentation"},{"id":"http://arxiv.org/abs/2507.15155v2","updated":"2025-09-11T02:23:18Z","published":"2025-07-20T23:27:44Z","title":"Learning-Based Modeling of a Magnetically Steerable Soft Suction Device\n  for Endoscopic Endonasal Interventions","summary":"  This letter introduces a novel learning-based modeling framework for a\nmagnetically steerable soft suction device designed for endoscopic endonasal\nbrain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm\ninner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,\nand integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape\nfeedback. Shape reconstruction is represented using four Bezier control points,\nenabling a compact and smooth model of the device's deformation. A data-driven\nmodel was trained on 5,097 experimental samples covering a range of magnetic\nfield magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical\ntip distances (90-100 mm), using both Neural Network (NN) and Random Forest\n(RF) architectures. The RF model outperformed the NN across all metrics,\nachieving a mean root mean square error of 0.087 mm in control point prediction\nand a mean shape reconstruction error of 0.064 mm. Feature importance analysis\nfurther revealed that magnetic field components predominantly influence distal\ncontrol points, while frequency and distance affect the base configuration.\nThis learning-based approach effectively models the complex nonlinear behavior\nof hyperelastic soft robots under magnetic actuation without relying on\nsimplified physical assumptions. By enabling sub-millimeter shape prediction\naccuracy and real-time inference, this work represents an advancement toward\nthe intelligent control of magnetically actuated soft robotic tools in\nminimally invasive neurosurgery.\n","authors":["Majid Roshanfar","Alex Zhang","Changyan He","Amir Hooshiar","Dale J. Podolsky","Thomas Looi","Eric Diller"],"pdf_url":"https://arxiv.org/pdf/2507.15155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.09093v1","updated":"2025-09-11T01:59:13Z","published":"2025-09-11T01:59:13Z","title":"Kinetostatics and Particle-Swarm Optimization of Vehicle-Mounted\n  Underactuated Metamorphic Loading Manipulators","summary":"  Fixed degree-of-freedom (DoF) loading mechanisms often suffer from excessive\nactuators, complex control, and limited adaptability to dynamic tasks. This\nstudy proposes an innovative mechanism of underactuated metamorphic loading\nmanipulators (UMLM), integrating a metamorphic arm with a passively adaptive\ngripper. The metamorphic arm exploits geometric constraints, enabling the\ntopology reconfiguration and flexible motion trajectories without additional\nactuators. The adaptive gripper, driven entirely by the arm, conforms to\ndiverse objects through passive compliance. A structural model is developed,\nand a kinetostatics analysis is conducted to investigate isomorphic grasping\nconfigurations. To optimize performance, Particle-Swarm Optimization (PSO) is\nutilized to refine the gripper's dimensional parameters, ensuring robust\nadaptability across various applications. Simulation results validate the\nUMLM's easily implemented control strategy, operational versatility, and\neffectiveness in grasping diverse objects in dynamic environments. This work\nunderscores the practical potential of underactuated metamorphic mechanisms in\napplications requiring efficient and adaptable loading solutions. Beyond the\nspecific design, this generalized modeling and optimization framework extends\nto a broader class of manipulators, offering a scalable approach to the\ndevelopment of robotic systems that require efficiency, flexibility, and robust\nperformance.\n","authors":["Nan Mao","Guanglu Jia","Junpeng Chen","Emmanouil Spyrakos-Papastavridis","Jian S. Dai"],"pdf_url":"https://arxiv.org/pdf/2509.09093v1.pdf","comment":"50 pages, 19 figures"},{"id":"http://arxiv.org/abs/2509.08775v2","updated":"2025-09-11T01:23:12Z","published":"2025-09-10T17:05:16Z","title":"Joint Model-based Model-free Diffusion for Planning with Constraints","summary":"  Model-free diffusion planners have shown great promise for robot motion\nplanning, but practical robotic systems often require combining them with\nmodel-based optimization modules to enforce constraints, such as safety.\nNaively integrating these modules presents compatibility challenges when\ndiffusion's multi-modal outputs behave adversarially to optimization-based\nmodules. To address this, we introduce Joint Model-based Model-free Diffusion\n(JM2D), a novel generative modeling framework. JM2D formulates module\nintegration as a joint sampling problem to maximize compatibility via an\ninteraction potential, without additional training. Using importance sampling,\nJM2D guides modules outputs based only on evaluations of the interaction\npotential, thus handling non-differentiable objectives commonly arising from\nnon-convex optimization modules. We evaluate JM2D via application to aligning\ndiffusion planners with safety modules on offline RL and robot manipulation.\nJM2D significantly improves task performance compared to conventional safety\nfilters without sacrificing safety. Further, we show that conditional\ngeneration is a special case of JM2D and elucidate key design choices by\ncomparing with SOTA gradient-based and projection-based diffusion planners.\nMore details at: https://jm2d-corl25.github.io/.\n","authors":["Wonsuhk Jung","Utkarsh A. Mishra","Nadun Ranawaka Arachchige","Yongxin Chen","Danfei Xu","Shreyas Kousik"],"pdf_url":"https://arxiv.org/pdf/2509.08775v2.pdf","comment":"The first two authors contributed equally. Last three authors advised\n  equally. Accepted to CoRL 2025"},{"id":"http://arxiv.org/abs/2509.09074v1","updated":"2025-09-11T00:42:01Z","published":"2025-09-11T00:42:01Z","title":"KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for\n  Motion Planning","summary":"  In this work, we propose a novel flow field-based motion planning method that\ndrives a robot from any initial state to a desired reference trajectory such\nthat it converges to the trajectory's end point. Despite demonstrated efficacy\nin using Koopman operator theory for modeling dynamical systems, Koopman does\nnot inherently enforce convergence to desired trajectories nor to specified\ngoals -- a requirement when learning from demonstrations (LfD). We present\nKoopMotion which represents motion flow fields as dynamical systems,\nparameterized by Koopman Operators to mimic desired trajectories, and leverages\nthe divergence properties of the learnt flow fields to obtain smooth motion\nfields that converge to a desired reference trajectory when a robot is placed\naway from the desired trajectory, and tracks the trajectory until the end\npoint. To demonstrate the effectiveness of our approach, we show evaluations of\nKoopMotion on the LASA human handwriting dataset and a 3D manipulator\nend-effector trajectory dataset, including spectral analysis. We also perform\nexperiments on a physical robot, verifying KoopMotion on a miniature autonomous\nsurface vehicle operating in a non-static fluid flow environment. Our approach\nis highly sample efficient in both space and time, requiring only 3\\% of the\nLASA dataset to generate dense motion plans. Additionally, KoopMotion provides\na significant improvement over baselines when comparing metrics that measure\nspatial and temporal dynamics modeling efficacy.\n","authors":["Alice Kate Li","Thales C Silva","Victoria Edwards","Vijay Kumar","M. Ani Hsieh"],"pdf_url":"https://arxiv.org/pdf/2509.09074v1.pdf","comment":"Accepted to CoRL 2025 (Conference on Robot Learning). 15 pages 11\n  figures"}]},"2025-09-10T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2312.10647v3","updated":"2025-09-10T22:19:29Z","published":"2023-12-17T08:19:59Z","title":"Single-Stage Optimization of Open-loop Stable Limit Cycles with Smooth,\n  Symbolic Derivatives","summary":"  Open-loop stable limit cycles are foundational to legged robotics, providing\ninherent self-stabilization that minimizes the need for computationally\nintensive feedback-based gait correction. While previous methods have primarily\ntargeted specific robotic models, this paper introduces a general framework for\nrapidly generating limit cycles across various dynamical systems, with the\nflexibility to impose arbitrarily tight stability bounds. We formulate the\nproblem as a single-stage constrained optimization problem and use Direct\nCollocation to transcribe it into a nonlinear program with closed-form\nexpressions for constraints, objectives, and their gradients.\n  Our method supports multiple stability formulations. In particular, we tested\ntwo popular formulations for limit cycle stability in robotics: (1) based on\nthe spectral radius of a discrete return map, and (2) based on the spectral\nradius of the monodromy matrix, and tested five different\nconstraint-satisfaction formulations of the eigenvalue problem to bound the\nspectral radius. We compare the performance and solution quality of the various\nformulations on a robotic swing-leg model, highlighting the Schur decomposition\nof the monodromy matrix as a method with broader applicability due to weaker\nassumptions and stronger numerical convergence properties.\n  As a case study, we apply our method on a hopping robot model, generating\nopen-loop stable gaits in under 2 seconds on an Intel Core i7-6700K, while\nsimultaneously minimizing energy consumption even under tight stability\nconstraints.\n","authors":["Muhammad Saud Ul Hassan","Christian Hubicki"],"pdf_url":"https://arxiv.org/pdf/2312.10647v3.pdf","comment":"Accepted at IEEE International Conference on Robotics and Automation\n  (ICRA) 2025"},{"id":"http://arxiv.org/abs/2502.02657v2","updated":"2025-09-10T21:52:40Z","published":"2025-02-04T19:00:49Z","title":"SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with\n  Uncertainty Quantification","summary":"  We present a neural radiance field (NeRF) based large-scale reconstruction\nsystem that fuses lidar and vision data to generate high-quality\nreconstructions that are geometrically accurate and capture photorealistic\ntexture. Our system adopts the state-of-the-art NeRF representation to\nincorporate lidar. Adding lidar data adds strong geometric constraints on the\ndepth and surface normals, which is particularly useful when modelling uniform\ntexture surfaces which contain ambiguous visual reconstruction cues. A key\ncontribution of this work is a novel method to quantify the epistemic\nuncertainty of the lidar-visual NeRF reconstruction by estimating the spatial\nvariance of each point location in the radiance field given the sensor\nobservations from the cameras and lidar. This provides a principled approach to\nevaluate the contribution of each sensor modality to the final reconstruction.\nIn this way, reconstructions that are uncertain (due to e.g. uniform visual\ntexture, limited observation viewpoints, or little lidar coverage) can be\nidentified and removed. Our system is integrated with a real-time lidar SLAM\nsystem which is used to bootstrap a Structure-from-Motion (SfM) reconstruction\nprocedure. It also helps to properly constrain the overall metric scale which\nis essential for the lidar depth loss. The refined SLAM trajectory can then be\ndivided into submaps using Spectral Clustering to group sets of co-visible\nimages together. This submapping approach is more suitable for visual\nreconstruction than distance-based partitioning. Our uncertainty estimation is\nparticularly effective when merging submaps as their boundaries often contain\nartefacts due to limited observations. We demonstrate the reconstruction system\nusing a multi-camera, lidar sensor suite in experiments involving both\nrobot-mounted and handheld scanning. Our test datasets cover a total area of\nmore than 20,000 square metres.\n","authors":["Yifu Tao","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2502.02657v2.pdf","comment":"Accepted by T-RO. Webpage:\n  https://dynamic.robots.ox.ac.uk/projects/silvr/"},{"id":"http://arxiv.org/abs/2509.09024v1","updated":"2025-09-10T21:43:19Z","published":"2025-09-10T21:43:19Z","title":"Rapid Manufacturing of Lightweight Drone Frames Using Single-Tow\n  Architected Composites","summary":"  The demand for lightweight and high-strength composite structures is rapidly\ngrowing in aerospace and robotics, particularly for optimized drone frames.\nHowever, conventional composite manufacturing methods struggle to achieve\ncomplex 3D architectures for weight savings and rely on assembling separate\ncomponents, which introduce weak points at the joints. Additionally,\nmaintaining continuous fiber reinforcement remains challenging, limiting\nstructural efficiency. In this study, we demonstrate the lightweight Face\nCentered Cubic (FFC) lattice structured conceptualization of drone frames for\nweight reduction and complex topology fabrication through 3D Fiber Tethering\n(3DFiT) using continuous single tow fiber ensuring precise fiber alignment,\neliminating weak points associated with traditional composite assembly.\nMechanical testing demonstrates that the fabricated drone frame exhibits a high\nspecific strength of around four to eight times the metal and thermoplastic,\noutperforming other conventional 3D printing methods. The drone frame weighs\nonly 260 g, making it 10% lighter than the commercial DJI F450 frame, enhancing\nstructural integrity and contributing to an extended flight time of three\nminutes, while flight testing confirms its stability and durability under\noperational conditions. The findings demonstrate the potential of single tow\nlattice truss-based drone frames, with 3DFiT serving as a scalable and\nefficient manufacturing method.\n","authors":["Md Habib Ullah Khan","Kaiyue Deng","Ismail Mujtaba Khan","Kelvin Fu"],"pdf_url":"https://arxiv.org/pdf/2509.09024v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.10546v2","updated":"2025-09-10T20:11:59Z","published":"2024-11-15T19:43:24Z","title":"The Oxford Spires Dataset: Benchmarking Large-Scale LiDAR-Visual\n  Localisation, Reconstruction and Radiance Field Methods","summary":"  This paper introduces a large-scale multi-modal dataset captured in and\naround well-known landmarks in Oxford using a custom-built multi-sensor\nperception unit as well as a millimetre-accurate map from a Terrestrial LiDAR\nScanner (TLS). The perception unit includes three synchronised global shutter\ncolour cameras, an automotive 3D LiDAR scanner, and an inertial sensor - all\nprecisely calibrated. We also establish benchmarks for tasks involving\nlocalisation, reconstruction, and novel-view synthesis, which enable the\nevaluation of Simultaneous Localisation and Mapping (SLAM) methods,\nStructure-from-Motion (SfM) and Multi-view Stereo (MVS) methods as well as\nradiance field methods such as Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting. To evaluate 3D reconstruction the TLS 3D models are used as ground\ntruth. Localisation ground truth is computed by registering the mobile LiDAR\nscans to the TLS 3D models. Radiance field methods are evaluated not only with\nposes sampled from the input trajectory, but also from viewpoints that are from\ntrajectories which are distant from the training poses. Our evaluation\ndemonstrates a key limitation of state-of-the-art radiance field methods: we\nshow that they tend to overfit to the training poses/images and do not\ngeneralise well to out-of-sequence poses. They also underperform in 3D\nreconstruction compared to MVS systems using the same visual inputs. Our\ndataset and benchmarks are intended to facilitate better integration of\nradiance field methods and SLAM systems. The raw and processed data, along with\nsoftware for parsing and evaluation, can be accessed at\nhttps://dynamic.robots.ox.ac.uk/datasets/oxford-spires/.\n","authors":["Yifu Tao","Miguel Ángel Muñoz-Bañón","Lintong Zhang","Jiahao Wang","Lanke Frank Tarimo Fu","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2411.10546v2.pdf","comment":"Accepted by IJRR. Website:\n  https://dynamic.robots.ox.ac.uk/datasets/oxford-spires/"},{"id":"http://arxiv.org/abs/2509.08820v1","updated":"2025-09-10T17:52:09Z","published":"2025-09-10T17:52:09Z","title":"RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical\n  Experimentation","summary":"  Robotic chemists promise to both liberate human experts from repetitive tasks\nand accelerate scientific discovery, yet remain in their infancy. Chemical\nexperiments involve long-horizon procedures over hazardous and deformable\nsubstances, where success requires not only task completion but also strict\ncompliance with experimental norms. To address these challenges, we propose\n\\textit{RoboChemist}, a dual-loop framework that integrates Vision-Language\nModels (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based\nsystems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with\ntransparent labware, and existing VLA systems (e.g., RDT, pi0) that lack\nsemantic-level feedback for complex tasks, our method leverages a VLM to serve\nas (1) a planner to decompose tasks into primitive actions, (2) a visual prompt\ngenerator to guide VLA models, and (3) a monitor to assess task success and\nregulatory compliance. Notably, we introduce a VLA interface that accepts\nimage-based visual targets from the VLM, enabling precise, goal-conditioned\ncontrol. Our system successfully executes both primitive actions and complete\nmulti-step chemistry protocols. Results show 23.57% higher average success rate\nand a 0.298 average increase in compliance rate over state-of-the-art VLA\nbaselines, while also demonstrating strong generalization to objects and tasks.\n","authors":["Zongzheng Zhang","Chenghao Yue","Haobo Xu","Minwen Liao","Xianglin Qi","Huan-ang Gao","Ziwei Wang","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.08820v1.pdf","comment":"Accepted to CoRL 2025, Project Page:\n  https://zzongzheng0918.github.io/RoboChemist.github.io/"},{"id":"http://arxiv.org/abs/2509.08813v1","updated":"2025-09-10T17:45:16Z","published":"2025-09-10T17:45:16Z","title":"Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and\n  3D Metric-Scaled Scene Reconstruction","summary":"  Robots often rely on RGB images for tasks like manipulation and navigation.\nHowever, reliable interaction typically requires a 3D scene representation that\nis metric-scaled and aligned with the robot reference frame. This depends on\naccurate camera-to-robot calibration and dense 3D reconstruction, tasks usually\ntreated separately, despite both relying on geometric correspondences from RGB\ndata. Traditional calibration needs patterns, while RGB-based reconstruction\nyields geometry with an unknown scale in an arbitrary frame. Multi-camera\nsetups add further complexity, as data must be expressed in a shared reference\nframe. We present Calib3R, a patternless method that jointly performs\ncamera-to-robot calibration and metric-scaled 3D reconstruction via unified\noptimization. Calib3R handles single- and multi-camera setups on robot arms or\nmobile robots. It builds on the 3D foundation model MASt3R to extract pointmaps\nfrom RGB images, which are combined with robot poses to reconstruct a scaled 3D\nscene aligned with the robot. Experiments on diverse datasets show that Calib3R\nachieves accurate calibration with less than 10 images, outperforming\ntarget-less and marker-based methods.\n","authors":["Davide Allegro","Matteo Terreran","Stefano Ghidoni"],"pdf_url":"https://arxiv.org/pdf/2509.08813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08757v1","updated":"2025-09-10T16:47:00Z","published":"2025-09-10T16:47:00Z","title":"SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot\n  Navigation","summary":"  Robot navigation in dynamic, human-centered environments requires\nsocially-compliant decisions grounded in robust scene understanding. Recent\nVision-Language Models (VLMs) exhibit promising capabilities such as object\nrecognition, common-sense reasoning, and contextual understanding-capabilities\nthat align with the nuanced requirements of social robot navigation. However,\nit remains unclear whether VLMs can accurately understand complex social\nnavigation scenes (e.g., inferring the spatial-temporal relations among agents\nand human intentions), which is essential for safe and socially compliant robot\nnavigation. While some recent works have explored the use of VLMs in social\nrobot navigation, no existing work systematically evaluates their ability to\nmeet these necessary conditions. In this paper, we introduce the Social\nNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question\nAnswering (VQA) dataset and benchmark designed to evaluate VLMs for scene\nunderstanding in real-world social robot navigation scenarios. SocialNav-SUB\nprovides a unified framework for evaluating VLMs against human and rule-based\nbaselines across VQA tasks requiring spatial, spatiotemporal, and social\nreasoning in social robot navigation. Through experiments with state-of-the-art\nVLMs, we find that while the best-performing VLM achieves an encouraging\nprobability of agreeing with human answers, it still underperforms simpler\nrule-based approach and human consensus baselines, indicating critical gaps in\nsocial scene understanding of current VLMs. Our benchmark sets the stage for\nfurther research on foundation models for social robot navigation, offering a\nframework to explore how VLMs can be tailored to meet real-world social robot\nnavigation needs. An overview of this paper along with the code and data can be\nfound at https://larg.github.io/socialnav-sub .\n","authors":["Michael J. Munje","Chen Tang","Shuijing Liu","Zichao Hu","Yifeng Zhu","Jiaxun Cui","Garrett Warnell","Joydeep Biswas","Peter Stone"],"pdf_url":"https://arxiv.org/pdf/2509.08757v1.pdf","comment":"Conference on Robot Learning (CoRL) 2025 Project site:\n  https://larg.github.io/socialnav-sub"},{"id":"http://arxiv.org/abs/2509.08743v1","updated":"2025-09-10T16:34:12Z","published":"2025-09-10T16:34:12Z","title":"Parallel, Asymptotically Optimal Algorithms for Moving Target Traveling\n  Salesman Problems","summary":"  The Moving Target Traveling Salesman Problem (MT-TSP) seeks an agent\ntrajectory that intercepts several moving targets, within a particular time\nwindow for each target. In the presence of generic nonlinear target\ntrajectories or kinematic constraints on the agent, no prior algorithm\nguarantees convergence to an optimal MT-TSP solution. Therefore, we introduce\nthe Iterated Random Generalized (IRG) TSP framework. The key idea behind IRG is\nto alternate between randomly sampling a set of agent configuration-time\npoints, corresponding to interceptions of targets, and finding a sequence of\ninterception points by solving a generalized TSP (GTSP). This alternation\nenables asymptotic convergence to the optimum. We introduce two parallel\nalgorithms within the IRG framework. The first algorithm, IRG-PGLNS, solves\nGTSPs using PGLNS, our parallelized extension of the state-of-the-art solver\nGLNS. The second algorithm, Parallel Communicating GTSPs (PCG), solves GTSPs\ncorresponding to several sets of points simultaneously. We present numerical\nresults for three variants of the MT-TSP: one where intercepting a target only\nrequires coming within a particular distance, another where the agent is a\nvariable-speed Dubins car, and a third where the agent is a redundant robot\narm. We show that IRG-PGLNS and PCG both converge faster than a baseline based\non prior work.\n","authors":["Anoop Bhat","Geordan Gutow","Bhaskar Vundurthy","Zhongqiang Ren","Sivakumar Rathinam","Howie Choset"],"pdf_url":"https://arxiv.org/pdf/2509.08743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13441v2","updated":"2025-09-10T15:53:04Z","published":"2024-01-24T13:30:18Z","title":"Guiding Soft Robots with Motor-Imagery Brain Signals and Impedance\n  Control","summary":"  Integrating Brain-Machine Interfaces into non-clinical applications like\nrobot motion control remains difficult - despite remarkable advancements in\nclinical settings. Specifically, EEG-based motor imagery systems are still\nerror-prone, posing safety risks when rigid robots operate near humans. This\nwork presents an alternative pathway towards safe and effective operation by\ncombining wearable EEG with physically embodied safety in soft robots. We\nintroduce and test a pipeline that allows a user to move a soft robot's end\neffector in real time via brain waves that are measured by as few as three EEG\nchannels. A robust motor imagery algorithm interprets the user's intentions to\nmove the position of a virtual attractor to which the end effector is\nattracted, thanks to a new Cartesian impedance controller. We specifically\nfocus here on planar soft robot-based architected metamaterials, which require\nthe development of a novel control architecture to deal with the peculiar\nnonlinearities - e.g., non-affinity in control. We preliminarily but\nquantitatively evaluate the approach on the task of setpoint regulation. We\nobserve that the user reaches the proximity of the setpoint in 66% of steps and\nthat for successful steps, the average response time is 21.5s. We also\ndemonstrate the execution of simple real-world tasks involving interaction with\nthe environment, which would be extremely hard to perform if it were not for\nthe robot's softness.\n","authors":["Maximilian Stölzle","Sonal Santosh Baberwal","Daniela Rus","Shirley Coyle","Cosimo Della Santina"],"pdf_url":"https://arxiv.org/pdf/2401.13441v2.pdf","comment":"8 pages, presented at 7th IEEE-RAS International Conference on Soft\n  Robotics (2024)"},{"id":"http://arxiv.org/abs/2509.08699v1","updated":"2025-09-10T15:43:32Z","published":"2025-09-10T15:43:32Z","title":"TANGO: Traversability-Aware Navigation with Local Metric Control for\n  Topological Goals","summary":"  Visual navigation in robotics traditionally relies on globally-consistent 3D\nmaps or learned controllers, which can be computationally expensive and\ndifficult to generalize across diverse environments. In this work, we present a\nnovel RGB-only, object-level topometric navigation pipeline that enables\nzero-shot, long-horizon robot navigation without requiring 3D maps or\npre-trained controllers. Our approach integrates global topological path\nplanning with local metric trajectory control, allowing the robot to navigate\ntowards object-level sub-goals while avoiding obstacles. We address key\nlimitations of previous methods by continuously predicting local trajectory\nusing monocular depth and traversability estimation, and incorporating an\nauto-switching mechanism that falls back to a baseline controller when\nnecessary. The system operates using foundational models, ensuring open-set\napplicability without the need for domain-specific fine-tuning. We demonstrate\nthe effectiveness of our method in both simulated environments and real-world\ntests, highlighting its robustness and deployability. Our approach outperforms\nexisting state-of-the-art methods, offering a more adaptable and effective\nsolution for visual navigation in open-set environments. The source code is\nmade publicly available: https://github.com/podgorki/TANGO.\n","authors":["Stefan Podgorski","Sourav Garg","Mehdi Hosseinzadeh","Lachlan Mares","Feras Dayoub","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2509.08699v1.pdf","comment":"9 pages, 5 figures, ICRA 2025"},{"id":"http://arxiv.org/abs/2504.03515v4","updated":"2025-09-10T15:21:33Z","published":"2025-04-04T15:14:38Z","title":"Dexterous Manipulation through Imitation Learning: A Survey","summary":"  Dexterous manipulation, which refers to the ability of a robotic hand or\nmulti-fingered end-effector to skillfully control, reorient, and manipulate\nobjects through precise, coordinated finger movements and adaptive force\nmodulation, enables complex interactions similar to human hand dexterity. With\nrecent advances in robotics and machine learning, there is a growing demand for\nthese systems to operate in complex and unstructured environments. Traditional\nmodel-based approaches struggle to generalize across tasks and object\nvariations due to the high dimensionality and complex contact dynamics of\ndexterous manipulation. Although model-free methods such as reinforcement\nlearning (RL) show promise, they require extensive training, large-scale\ninteraction data, and carefully designed rewards for stability and\neffectiveness. Imitation learning (IL) offers an alternative by allowing robots\nto acquire dexterous manipulation skills directly from expert demonstrations,\ncapturing fine-grained coordination and contact dynamics while bypassing the\nneed for explicit modeling and large-scale trial-and-error. This survey\nprovides an overview of dexterous manipulation methods based on imitation\nlearning, details recent advances, and addresses key challenges in the field.\nAdditionally, it explores potential research directions to enhance IL-driven\ndexterous manipulation. Our goal is to offer researchers and practitioners a\ncomprehensive introduction to this rapidly evolving domain.\n","authors":["Shan An","Ziyu Meng","Chao Tang","Yuning Zhou","Tengyu Liu","Fangqiang Ding","Shufang Zhang","Yao Mu","Ran Song","Wei Zhang","Zeng-Guang Hou","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.03515v4.pdf","comment":"32pages, 6 figures, 9 tables"},{"id":"http://arxiv.org/abs/2509.06932v2","updated":"2025-09-10T14:34:25Z","published":"2025-09-08T17:45:40Z","title":"LLaDA-VLA: Vision Language Diffusion Action Models","summary":"  The rapid progress of auto-regressive vision-language models (VLMs) has\ninspired growing interest in vision-language-action models (VLA) for robotic\nmanipulation. Recently, masked diffusion models, a paradigm distinct from\nautoregressive models, have begun to demonstrate competitive performance in\ntext generation and multimodal applications, leading to the development of a\nseries of diffusion-based VLMs (d-VLMs). However, leveraging such models for\nrobot policy learning remains largely unexplored. In this work, we present\nLLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon\npretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to\nrobotic domain, we introduce two key designs: (1) a localized special-token\nclassification strategy that replaces full-vocabulary classification with\nspecial action token classification, reducing adaptation difficulty; (2) a\nhierarchical action-structured decoding strategy that decodes action sequences\nhierarchically considering the dependencies within and across actions.\nExtensive experiments demonstrate that LLaDA-VLA significantly outperforms\nstate-of-the-art VLAs on both simulation and real-world robots.\n","authors":["Yuqing Wen","Hebei Li","Kefan Gu","Yucheng Zhao","Tiancai Wang","Xiaoyan Sun"],"pdf_url":"https://arxiv.org/pdf/2509.06932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08638v1","updated":"2025-09-10T14:33:58Z","published":"2025-09-10T14:33:58Z","title":"AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models","summary":"  Specialized machine learning models, regardless of architecture and training,\nare susceptible to failures in deployment. With their increasing use in high\nrisk situations, the ability to audit these models by determining their\noperational design domain (ODD) is crucial in ensuring safety and compliance.\nHowever, given the high-dimensional input spaces, this process often requires\nsignificant human resources and domain expertise. To alleviate this, we\nintroduce \\coolname, an LLM-Agent centric framework for automated generation of\nsemantically relevant test cases to search for failure modes in specialized\nblack-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit\na uncertainty-aware failure distribution model on a learned text-embedding\nmanifold by projecting the high-dimension input space to low-dimension\ntext-embedding latent space. The LLM-Agent is tasked with iteratively building\nthe failure landscape by leveraging tools for generating test-cases to probe\nthe model-under-test (MUT) and recording the response. The agent also guides\nthe search using tools to probe uncertainty estimate on the low dimensional\nmanifold. We demonstrate this process in a simple case using models trained\nwith missing digits on the MNIST dataset and in the real world setting of\nvision-based intruder detection for aerial vehicles.\n","authors":["Rebecca Martin","Jay Patrikar","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2509.08638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11260v4","updated":"2025-09-10T14:02:23Z","published":"2025-01-20T04:00:02Z","title":"A Survey of World Models for Autonomous Driving","summary":"  Recent breakthroughs in autonomous driving have been propelled by advances in\nrobust world modeling, fundamentally transforming how vehicles interpret\ndynamic scenes and execute safe decision-making. World models have emerged as a\nlinchpin technology, offering high-fidelity representations of the driving\nenvironment that integrate multi-sensor data, semantic cues, and temporal\ndynamics. This paper systematically reviews recent advances in world models for\nautonomous driving, proposing a three-tiered taxonomy: (i) Generation of Future\nPhysical World, covering Image-, BEV-, OG-, and PC-based generation methods\nthat enhance scene evolution modeling through diffusion models and 4D occupancy\nforecasting; (ii) Behavior Planning for Intelligent Agents, combining\nrule-driven and learning-based paradigms with cost map optimization and\nreinforcement learning for trajectory generation in complex traffic conditions;\n(ii) Interaction between Prediction and Planning, achieving multi-agent\ncollaborative decision-making through latent space diffusion and\nmemory-augmented architectures. The study further analyzes training paradigms,\nincluding self-supervised learning, multimodal pretraining, and generative data\naugmentation, while evaluating world models' performance in scene understanding\nand motion prediction tasks. Future research must address key challenges in\nself-supervised representation learning, multimodal fusion, and advanced\nsimulation to advance the practical deployment of world models in complex urban\nenvironments. Overall, the comprehensive analysis provides a technical roadmap\nfor harnessing the transformative potential of world models in advancing safe\nand reliable autonomous driving solutions.\n","authors":["Tuo Feng","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2501.11260v4.pdf","comment":"Ongoing project. Paper list: https://github.com/FengZicai/AwesomeWMAD\n  Benchmark: https://github.com/FengZicai/WMAD-Benchmarks"},{"id":"http://arxiv.org/abs/2401.11792v8","updated":"2025-09-10T13:18:49Z","published":"2024-01-22T09:44:16Z","title":"Efficient and Generalized end-to-end Autonomous Driving System with\n  Latent Deep Reinforcement Learning and Demonstrations","summary":"  An intelligent driving system should dynamically formulate appropriate\ndriving strategies based on the current environment and vehicle status while\nensuring system security and reliability. However, methods based on\nreinforcement learning and imitation learning often suffer from high sample\ncomplexity, poor generalization, and low safety. To address these challenges,\nthis paper introduces an efficient and generalized end-to-end autonomous\ndriving system (EGADS) for complex and varied scenarios. The RL agent in our\nEGADS combines variational inference with normalizing flows, which are\nindependent of distribution assumptions. This combination allows the agent to\ncapture historical information relevant to driving in latent space effectively,\nthereby significantly reducing sample complexity. Additionally, we enhance\nsafety by formulating robust safety constraints and improve generalization and\nperformance by integrating RL with expert demonstrations. Experimental results\ndemonstrate that, compared to existing methods, EGADS significantly reduces\nsample complexity, greatly improves safety performance, and exhibits strong\ngeneralization capabilities in complex urban scenarios. Particularly, we\ncontributed an expert dataset collected through human expert steering wheel\ncontrol, specifically using the G29 steering wheel.\n","authors":["Zuojin Tang","Xiaoyu Chen","Yongqiang Li","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2401.11792v8.pdf","comment":"Accepted by ECML PKDD 2025 (Research Track)"},{"id":"http://arxiv.org/abs/2306.07569v3","updated":"2025-09-10T12:20:31Z","published":"2023-06-13T06:44:02Z","title":"Ontological Component-based Description of Robot Capabilities","summary":"  A key aspect of a robot's knowledge base is self-awareness about what it is\ncapable of doing. It allows to define which tasks it can be assigned to and\nwhich it cannot. We will refer to this knowledge as the Capability concept. As\ncapabilities stems from the components the robot owns, they can be linked\ntogether. In this work, we hypothesize that this concept can be inferred from\nthe components rather than merely linked to them. Therefore, we introduce an\nontological means of inferring the agent's capabilities based on the components\nit owns as well as low-level capabilities. This inference allows the agent to\nacknowledge what it is able to do in a responsive way and it is generalizable\nto external entities the agent can carry for example. To initiate an action,\nthe robot needs to link its capabilities with external entities. To do so, it\nneeds to infer affordance relations from its capabilities as well as the\nexternal entity's dispositions. This work is part of a broader effort to\nintegrate social affordances into a Human-Robot collaboration context and is an\nextension of an already existing ontology.\n","authors":["Bastien Dussard","Guillaume Sarthou","Aurélie Clodic"],"pdf_url":"https://arxiv.org/pdf/2306.07569v3.pdf","comment":"International Workshop on Working towards Ontology-based Standards\n  for Robotics and Automation (WOSRA 2023 - 2nd Edition), Jun 2023, Londres,\n  United Kingdom"},{"id":"http://arxiv.org/abs/2509.08522v1","updated":"2025-09-10T12:00:21Z","published":"2025-09-10T12:00:21Z","title":"RoboMatch: A Mobile-Manipulation Teleoperation Platform with\n  Auto-Matching Network Architecture for Long-Horizon Manipulation","summary":"  This paper presents RoboMatch, a novel unified teleoperation platform for\nmobile manipulation with an auto-matching network architecture, designed to\ntackle long-horizon tasks in dynamic environments. Our system enhances\nteleoperation performance, data collection efficiency, task accuracy, and\noperational stability. The core of RoboMatch is a cockpit-style control\ninterface that enables synchronous operation of the mobile base and dual arms,\nsignificantly improving control precision and data collection. Moreover, we\nintroduce the Proprioceptive-Visual Enhanced Diffusion Policy (PVE-DP), which\nleverages Discrete Wavelet Transform (DWT) for multi-scale visual feature\nextraction and integrates high-precision IMUs at the end-effector to enrich\nproprioceptive feedback, substantially boosting fine manipulation performance.\nFurthermore, we propose an Auto-Matching Network (AMN) architecture that\ndecomposes long-horizon tasks into logical sequences and dynamically assigns\nlightweight pre-trained models for distributed inference. Experimental results\ndemonstrate that our approach improves data collection efficiency by over 20%,\nincreases task success rates by 20-30% with PVE-DP, and enhances long-horizon\ninference performance by approximately 40% with AMN, offering a robust solution\nfor complex manipulation tasks.\n","authors":["Hanyu Liu","Yunsheng Ma","Jiaxin Huang","Keqiang Ren","Jiayi Wen","Yilin Zheng","Baishu Wan","Pan Li","Jiejun Hou","Haoru Luan","Zhihua Wang","Zhigong Song"],"pdf_url":"https://arxiv.org/pdf/2509.08522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08521v1","updated":"2025-09-10T11:57:56Z","published":"2025-09-10T11:57:56Z","title":"FMT$^{x}$: An Efficient and Asymptotically Optimal Extension of the Fast\n  Marching Tree for Dynamic Replanning","summary":"  Path planning in dynamic environments remains a core challenge in robotics,\nespecially as autonomous systems are deployed in unpredictable spaces such as\nwarehouses and public roads. While algorithms like Fast Marching Tree\n(FMT$^{*}$) offer asymptotically optimal solutions in static settings, their\nsingle-pass design prevents path revisions which are essential for real-time\nadaptation. On the other hand, full replanning is often too computationally\nexpensive. This paper introduces FMT$^{x}$, an extension of the Fast Marching\nTree algorithm that enables efficient and consistent replanning in dynamic\nenvironments. We revisit the neighbor selection rule of FMT$^{*}$ and\ndemonstrate that a minimal change overcomes its single-pass limitation,\nenabling the algorithm to update cost-to-come values upon discovering better\nconnections without sacrificing asymptotic optimality or computational\nefficiency. By maintaining a cost-ordered priority queue and applying a\nselective update condition that uses an expanding neighbor to identify and\ntrigger the re-evaluation of any node with a potentially suboptimal path,\nFMT$^{x}$ ensures that suboptimal routes are efficiently repaired as the\nenvironment evolves. This targeted strategy preserves the inherent efficiency\nof FMT$^{*}$ while enabling robust adaptation to changes in obstacle\nconfiguration. FMT$^{x}$ is proven to recover an asymptotically optimal\nsolution after environmental changes. Experimental results demonstrate that\nFMT$^{x}$ outperforms the influential replanner RRT$^{x}$, reacting more\nswiftly to dynamic events with lower computational overhead and thus offering a\nmore effective solution for real-time robotic navigation in unpredictable\nworlds.\n","authors":["Soheil Espahbodini Nia"],"pdf_url":"https://arxiv.org/pdf/2509.08521v1.pdf","comment":"35 pages, 8 figures, 2 tables, submitted to the International Journal\n  of Robotics Research (IJRR)"},{"id":"http://arxiv.org/abs/2509.08510v1","updated":"2025-09-10T11:34:10Z","published":"2025-09-10T11:34:10Z","title":"Facilitating the Emergence of Assistive Robots to Support Frailty:\n  Psychosocial and Environmental Realities","summary":"  While assistive robots have much potential to help older people with\nfrailty-related needs, there are few in use. There is a gap between what is\ndeveloped in laboratories and what would be viable in real-world contexts.\nThrough a series of co-design workshops (61 participants across 7 sessions)\nincluding those with lived experience of frailty, their carers, and healthcare\nprofessionals, we gained a deeper understanding of everyday issues concerning\nthe place of new technologies in their lives. A persona-based approach surfaced\nemotional, social, and psychological issues. Any assistive solution must be\ndeveloped in the context of this complex interplay of psychosocial and\nenvironmental factors. Our findings, presented as design requirements in direct\nrelation to frailty, can help promote design thinking that addresses people's\nneeds in a more pragmatic way to move assistive robotics closer to real-world\nuse.\n","authors":["Angela Higgins","Stephen Potter","Mauro Dragone","Mark Hawley","Farshid Amirabdollahian","Alessandro Di Nuovo","Praminda Caleb-Solly"],"pdf_url":"https://arxiv.org/pdf/2509.08510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07542v2","updated":"2025-09-10T11:27:40Z","published":"2025-09-09T09:20:09Z","title":"Improving Machine Learning-Based Robot Self-Collision Checking with\n  Input Positional Encoding","summary":"  This manuscript investigates the integration of positional encoding -- a\ntechnique widely used in computer graphics -- into the input vector of a binary\nclassification model for self-collision detection. The results demonstrate the\nbenefits of incorporating positional encoding, which enhances classification\naccuracy by enabling the model to better capture high-frequency variations,\nleading to a more detailed and precise representation of complex collision\npatterns. The manuscript shows that machine learning-based techniques, such as\nlightweight multilayer perceptrons (MLPs) operating in a low-dimensional\nfeature space, offer a faster alternative for collision checking than\ntraditional methods that rely on geometric approaches, such as\ntriangle-to-triangle intersection tests and Bounding Volume Hierarchies (BVH)\nfor mesh-based models.\n","authors":["Bartłomiej Kulecki","Dominik Belter"],"pdf_url":"https://arxiv.org/pdf/2509.07542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21602v2","updated":"2025-09-10T11:26:20Z","published":"2025-04-30T13:00:50Z","title":"Real Time Semantic Segmentation of High Resolution Automotive LiDAR\n  Scans","summary":"  In recent studies, numerous previous works emphasize the importance of\nsemantic segmentation of LiDAR data as a critical component to the development\nof driver-assistance systems and autonomous vehicles. However, many\nstate-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors\nand struggle with real-time constraints. This study introduces a novel semantic\nsegmentation framework tailored for modern high-resolution LiDAR sensors that\naddresses both accuracy and real-time processing demands. We propose a novel\nLiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban\ntraffic scenes. Furthermore, we propose a semantic segmentation method\nutilizing surface normals as strong input features. Our approach is bridging\nthe gap between cutting-edge research and practical automotive applications.\nAdditionaly, we provide a Robot Operating System (ROS2) implementation that we\noperate on our research vehicle. Our dataset and code are publicly available:\nhttps://github.com/kav-institute/SemanticLiDAR.\n","authors":["Hannes Reichert","Benjamin Serfling","Elijah Schüssler","Kerim Turacan","Konrad Doll","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2504.21602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08495v1","updated":"2025-09-10T11:11:12Z","published":"2025-09-10T11:11:12Z","title":"CLAP: Clustering to Localize Across n Possibilities, A Simple, Robust\n  Geometric Approach in the Presence of Symmetries","summary":"  In this paper, we present our localization method called CLAP, Clustering to\nLocalize Across $n$ Possibilities, which helped us win the RoboCup 2024\nadult-sized autonomous humanoid soccer competition. Competition rules limited\nour sensor suite to stereo vision and an inertial sensor, similar to humans. In\naddition, our robot had to deal with varying lighting conditions, dynamic\nfeature occlusions, noise from high-impact stepping, and mistaken features from\nbystanders and neighboring fields. Therefore, we needed an accurate, and most\nimportantly robust localization algorithm that would be the foundation for our\npath-planning and game-strategy algorithms. CLAP achieves these requirements by\nclustering estimated states of our robot from pairs of field features to\nlocalize its global position and orientation. Correct state estimates naturally\ncluster together, while incorrect estimates spread apart, making CLAP resilient\nto noise and incorrect inputs. CLAP is paired with a particle filter and an\nextended Kalman filter to improve consistency and smoothness. Tests of CLAP\nwith other landmark-based localization methods showed similar accuracy.\nHowever, tests with increased false positive feature detection showed that CLAP\noutperformed other methods in terms of robustness with very little divergence\nand velocity jumps. Our localization performed well in competition, allowing\nour robot to shoot faraway goals and narrowly defend our goal.\n","authors":["Gabriel I. Fernandez","Ruochen Hou","Alex Xu","Colin Togashi","Dennis W. Hong"],"pdf_url":"https://arxiv.org/pdf/2509.08495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.06644v3","updated":"2025-09-10T10:42:43Z","published":"2025-09-08T12:59:36Z","title":"T-araVLN: Translator for Agricultural Robotic Agents on\n  Vision-and-Language Navigation","summary":"  Agricultural robotic agents have been becoming powerful helpers in a wide\nrange of agricultural tasks, nevertheless, still heavily rely on manual\noperation or untransportable railway for movement. The AgriVLN method and the\nA2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the\nagricultural domain, enabling agents navigate to the target position following\nthe natural language instructions. AgriVLN effectively understands the simple\ninstructions, however, often misunderstands the complicated instructions. To\nbridge this gap, we propose the method of Translator for Agricultural Robotic\nAgents on Vision-and-Language Navigation (T-araVLN), in which the Instruction\nTranslator module translates the original instruction to be both refined and\nprecise. Being evaluated on the A2A benchmark, our T-araVLN effectively\nimproves Success Rate from 0.47 to 0.63 and reduces Navigation Error from 2.91m\nto 2.28m, demonstrating the state-of-the-art performance in the agricultural\ndomain. Code: https://github.com/AlexTraveling/T-araVLN.\n","authors":["Xiaobei Zhao","Xingqi Lyu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2509.06644v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08460v1","updated":"2025-09-10T10:05:00Z","published":"2025-09-10T10:05:00Z","title":"Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic\n  Environment","summary":"  Recent advances in robotics have enabled the widespread deployment of\nautonomous robotic systems in complex operational environments, presenting both\nunprecedented opportunities and significant security problems. Traditional\nshepherding approaches based on fixed formations are often ineffective or risky\nin urban and obstacle-rich scenarios, especially when facing adversarial agents\nwith unknown and adaptive behaviors. This paper addresses this challenge as an\nextended herding problem, where defensive robotic systems must safely guide\nadversarial agents with unknown strategies away from protected areas and into\npredetermined safe regions, while maintaining collision-free navigation in\ndynamic environments. We propose a hierarchical hybrid framework based on\nreach-avoid game theory and local motion planning, incorporating a virtual\ncontainment boundary and event-triggered pursuit mechanisms to enable scalable\nand robust multi-agent coordination. Simulation results demonstrate that the\nproposed approach achieves safe and efficient guidance of adversarial agents to\ndesignated regions.\n","authors":["Wenqing Wang","Ye Zhang","Haoyu Li","Jingyu Wang"],"pdf_url":"https://arxiv.org/pdf/2509.08460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15108v3","updated":"2025-09-10T09:57:03Z","published":"2025-03-19T11:05:42Z","title":"VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making","summary":"  While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.\n","authors":["Mohamed Salim Aissi","Clemence Grislain","Mohamed Chetouani","Olivier Sigaud","Laure Soulier","Nicolas Thome"],"pdf_url":"https://arxiv.org/pdf/2503.15108v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08440v1","updated":"2025-09-10T09:39:15Z","published":"2025-09-10T09:39:15Z","title":"Augmenting Neural Networks-based Model Approximators in Robotic\n  Force-tracking Tasks","summary":"  As robotics gains popularity, interaction control becomes crucial for\nensuring force tracking in manipulator-based tasks. Typically, traditional\ninteraction controllers either require extensive tuning, or demand expert\nknowledge of the environment, which is often impractical in real-world\napplications. This work proposes a novel control strategy leveraging Neural\nNetworks (NNs) to enhance the force-tracking behavior of a Direct Force\nController (DFC). Unlike similar previous approaches, it accounts for the\nmanipulator's tangential velocity, a critical factor in force exertion,\nespecially during fast motions. The method employs an ensemble of feedforward\nNNs to predict contact forces, then exploits the prediction to solve an\noptimization problem and generate an optimal residual action, which is added to\nthe DFC output and applied to an impedance controller. The proposed\nVelocity-augmented Artificial intelligence Interaction Controller for Ambiguous\nModels (VAICAM) is validated in the Gazebo simulator on a Franka Emika Panda\nrobot. Against a vast set of trajectories, VAICAM achieves superior performance\ncompared to two baseline controllers.\n","authors":["Kevin Saad","Vincenzo Petrone","Enrico Ferrentino","Pasquale Chiacchio","Francesco Braghin","Loris Roveda"],"pdf_url":"https://arxiv.org/pdf/2509.08440v1.pdf","comment":"Accepted for publication at 22nd International Conference on\n  Informatics in Control, Automation and Robotic - ICINCO 2025"},{"id":"http://arxiv.org/abs/2509.08435v1","updated":"2025-09-10T09:31:17Z","published":"2025-09-10T09:31:17Z","title":"PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot\n  Diffusion Planner Flow Matching","summary":"  Diffusion models offer powerful generative capabilities for robot trajectory\nplanning, yet their practical deployment on robots is hindered by a critical\nbottleneck: a reliance on imitation learning from expert demonstrations. This\nparadigm is often impractical for specialized robots where data is scarce and\ncreates an inefficient, theoretically suboptimal training pipeline. To overcome\nthis, we introduce PegasusFlow, a hierarchical rolling-denoising framework that\nenables direct and parallel sampling of trajectory score gradients from\nenvironmental interaction, completely bypassing the need for expert data. Our\ncore innovation is a novel sampling algorithm, Weighted Basis Function\nOptimization (WBFO), which leverages spline basis representations to achieve\nsuperior sample efficiency and faster convergence compared to traditional\nmethods like MPPI. The framework is embedded within a scalable, asynchronous\nparallel simulation architecture that supports massively parallel rollouts for\nefficient data collection. Extensive experiments on trajectory optimization and\nrobotic navigation tasks demonstrate that our approach, particularly\nAction-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start,\nsignificantly outperforms baselines. In a challenging barrier-crossing task,\nour method achieved a 100% success rate and was 18% faster than the next-best\nmethod, validating its effectiveness for complex terrain locomotion planning.\nhttps://masteryip.github.io/pegasusflow.github.io/\n","authors":["Lei Ye","Haibo Gao","Peng Xu","Zhelin Zhang","Junqi Shan","Ao Zhang","Wei Zhang","Ruyi Zhou","Zongquan Deng","Liang Ding"],"pdf_url":"https://arxiv.org/pdf/2509.08435v1.pdf","comment":"8 pages, 7 figures, conference paper"},{"id":"http://arxiv.org/abs/2506.23771v2","updated":"2025-09-10T08:59:58Z","published":"2025-06-30T12:17:42Z","title":"Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior\n  and Control of Autonomous Driving","summary":"  Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.\n","authors":["Guizhe Jin","Zhuoren Li","Bo Leng","Ran Yu","Lu Xiong","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2506.23771v2.pdf","comment":"8 pages, Submitted to IEEE Robotics and Automation Letters (under\n  second-round review)"},{"id":"http://arxiv.org/abs/2509.07683v2","updated":"2025-09-10T07:55:48Z","published":"2025-09-09T12:51:23Z","title":"Robust Radar SLAM for Vehicle Parking Applications","summary":"  We address ego-motion estimation for automated parking, where\ncentimeter-level accuracy is crucial due to tight spaces and nearby obstacles.\nTraditional methods using inertial-measurement units and wheel encoders require\ncalibration, making them costly and time-consuming. To overcome this, we\npropose a radar-based simultaneous localization and mapping (SLAM) approach\nthat leverages the robustness of radar to adverse weather and support for\nonline calibration. Our robocentric formulation fuses feature positions and\nDoppler velocities for robust data association and filter convergence. Key\ncontributions include a Doppler-augmented radar SLAM method, multi-radar\nsupport and an information-based feature-pruning strategy. Experiments\ndemonstrate high-accuracy localization and improved robustness over\nstate-of-the-art methods, meeting the demands of automated parking.\n","authors":["Luis Diener","Jens Kalkkuhl","Markus Enzweiler"],"pdf_url":"https://arxiv.org/pdf/2509.07683v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2509.08354v1","updated":"2025-09-10T07:44:12Z","published":"2025-09-10T07:44:12Z","title":"Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from\n  Human Proprioceptive Sensorimotor Integration","summary":"  Tactile and kinesthetic perceptions are crucial for human dexterous\nmanipulation, enabling reliable grasping of objects via proprioceptive\nsensorimotor integration. For robotic hands, even though acquiring such tactile\nand kinesthetic feedback is feasible, establishing a direct mapping from this\nsensory feedback to motor actions remains challenging. In this paper, we\npropose a novel glove-mediated tactile-kinematic perception-prediction\nframework for grasp skill transfer from human intuitive and natural operation\nto robotic execution based on imitation learning, and its effectiveness is\nvalidated through generalized grasping tasks, including those involving\ndeformable objects. Firstly, we integrate a data glove to capture tactile and\nkinesthetic data at the joint level. The glove is adaptable for both human and\nrobotic hands, allowing data collection from natural human hand demonstrations\nacross different scenarios. It ensures consistency in the raw data format,\nenabling evaluation of grasping for both human and robotic hands. Secondly, we\nestablish a unified representation of multi-modal inputs based on graph\nstructures with polar coordinates. We explicitly integrate the morphological\ndifferences into the designed representation, enhancing the compatibility\nacross different demonstrators and robotic hands. Furthermore, we introduce the\nTactile-Kinesthetic Spatio-Temporal Graph Networks (TK-STGN), which leverage\nmultidimensional subgraph convolutions and attention-based LSTM layers to\nextract spatio-temporal features from graph inputs to predict node-based states\nfor each hand joint. These predictions are then mapped to final commands\nthrough a force-position hybrid mapping.\n","authors":["Ce Guo","Xieyuanli Chen","Zhiwen Zeng","Zirui Guo","Yihong Li","Haoran Xiao","Dewen Hu","Huimin Lu"],"pdf_url":"https://arxiv.org/pdf/2509.08354v1.pdf","comment":"20 pages, 19 figures, accepted by IEEE Transactions on Robotics"},{"id":"http://arxiv.org/abs/2509.08333v1","updated":"2025-09-10T07:15:43Z","published":"2025-09-10T07:15:43Z","title":"Good Deep Features to Track: Self-Supervised Feature Extraction and\n  Tracking in Visual Odometry","summary":"  Visual-based localization has made significant progress, yet its performance\noften drops in large-scale, outdoor, and long-term settings due to factors like\nlighting changes, dynamic scenes, and low-texture areas. These challenges\ndegrade feature extraction and tracking, which are critical for accurate motion\nestimation. While learning-based methods such as SuperPoint and SuperGlue show\nimproved feature coverage and robustness, they still face generalization issues\nwith out-of-distribution data. We address this by enhancing deep feature\nextraction and tracking through self-supervised learning with task specific\nfeedback. Our method promotes stable and informative features, improving\ngeneralization and reliability in challenging environments.\n","authors":["Sai Puneeth Reddy Gottam","Haoming Zhang","Eivydas Keras"],"pdf_url":"https://arxiv.org/pdf/2509.08333v1.pdf","comment":"This short paper has been accepted as a workshop paper at European\n  Conference on Mobile Robots 2025"},{"id":"http://arxiv.org/abs/2502.09960v3","updated":"2025-09-10T07:02:51Z","published":"2025-02-14T07:36:26Z","title":"Global-Local Interface for On-Demand Teleoperation","summary":"  Teleoperation is a critical method for human-robot interface, holds\nsignificant potential for enabling robotic applications in industrial and\nunstructured environments. Existing teleoperation methods have distinct\nstrengths and limitations in flexibility, range of workspace and precision. To\nfuse these advantages, we introduce the Global-Local (G-L) Teleoperation\nInterface. This interface decouples robotic teleoperation into global behavior,\nwhich ensures the robot motion range and intuitiveness, and local behavior,\nwhich enhances human operator's dexterity and capability for performing fine\ntasks. The G-L interface enables efficient teleoperation not only for\nconventional tasks like pick-and-place, but also for challenging fine\nmanipulation and large-scale movements. Based on the G-L interface, we\nconstructed a single-arm and a dual-arm teleoperation system with different\nremote control devices, then demonstrated tasks requiring large motion range,\nprecise manipulation or dexterous end-effector control. Extensive experiments\nvalidated the user-friendliness, accuracy, and generalizability of the proposed\ninterface.\n","authors":["Jianshu Zhou","Boyuan Liang","Junda Huang","Ian Zhang","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2502.09960v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22943v3","updated":"2025-09-10T06:59:24Z","published":"2025-03-29T02:28:32Z","title":"Event Camera Meets Resource-Aware Mobile Computing: Abstraction,\n  Algorithm, Acceleration, Application","summary":"  With the increasing complexity of mobile device applications, these devices\nare evolving toward high agility. This shift imposes new demands on mobile\nsensing, particularly in achieving high-accuracy and low-latency. Event-based\nvision has emerged as a disruptive paradigm, offering high temporal resolution\nand low latency, making it well-suited for high-accuracy and low-latency\nsensing tasks on high-agility platforms. However, the presence of substantial\nnoisy events, lack of stable, persistent semantic information, and large data\nvolume pose challenges for event-based data processing on resource-constrained\nmobile devices. This paper surveys the literature from 2014 to 2025 and\npresents a comprehensive overview of event-based mobile sensing, encompassing\nits fundamental principles, event \\textit{abstraction} methods,\n\\textit{algorithm} advancements, and both hardware and software\n\\textit{acceleration} strategies. We discuss key \\textit{applications} of event\ncameras in mobile sensing, including visual odometry, object tracking, optical\nflow, and 3D reconstruction, while highlighting challenges associated with\nevent data processing, sensor fusion, and real-time deployment. Furthermore, we\noutline future research directions, such as improving the event camera with\nadvanced optics, leveraging neuromorphic computing for efficient processing,\nand integrating bio-inspired algorithms. To support ongoing research, we\nprovide an open-source \\textit{Online Sheet} with recent developments. We hope\nthis survey serves as a reference, facilitating the adoption of event-based\nvision across diverse applications.\n","authors":["Haoyang Wang","Ruishan Guo","Pengtao Ma","Ciyu Ruan","Xinyu Luo","Wenhua Ding","Tianyang Zhong","Jingao Xu","Yunhao Liu","Xinlei Chen"],"pdf_url":"https://arxiv.org/pdf/2503.22943v3.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2509.08302v1","updated":"2025-09-10T05:45:49Z","published":"2025-09-10T05:45:49Z","title":"Foundation Models for Autonomous Driving Perception: A Survey Through\n  Core Capabilities","summary":"  Foundation models are revolutionizing autonomous driving perception,\ntransitioning the field from narrow, task-specific deep learning models to\nversatile, general-purpose architectures trained on vast, diverse datasets.\nThis survey examines how these models address critical challenges in autonomous\nperception, including limitations in generalization, scalability, and\nrobustness to distributional shifts. The survey introduces a novel taxonomy\nstructured around four essential capabilities for robust performance in dynamic\ndriving environments: generalized knowledge, spatial understanding,\nmulti-sensor robustness, and temporal reasoning. For each capability, the\nsurvey elucidates its significance and comprehensively reviews cutting-edge\napproaches. Diverging from traditional method-centric surveys, our unique\nframework prioritizes conceptual design principles, providing a\ncapability-driven guide for model development and clearer insights into\nfoundational aspects. We conclude by discussing key challenges, particularly\nthose associated with the integration of these capabilities into real-time,\nscalable systems, and broader deployment challenges related to computational\ndemands and ensuring model reliability against issues like hallucinations and\nout-of-distribution failures. The survey also outlines crucial future research\ndirections to enable the safe and effective deployment of foundation models in\nautonomous driving systems.\n","authors":["Rajendramayavan Sathyam","Yueqi Li"],"pdf_url":"https://arxiv.org/pdf/2509.08302v1.pdf","comment":"32 pages, 14 figures, accepted at IEEE Open Journal of Vehicular\n  Technology (OJVT)"},{"id":"http://arxiv.org/abs/2403.10397v3","updated":"2025-09-10T04:47:11Z","published":"2024-03-15T15:31:13Z","title":"Collaborative Aquatic Positioning System Utilising Multi-beam Sonar and\n  Depth Sensors","summary":"  Accurate positioning of underwater robots in confined environments is crucial\nfor inspection and mapping tasks and is also a prerequisite for autonomous\noperations. Presently, there are no positioning systems available that are\nsuited for real-world use in confined underwater environments, unconstrained by\nenvironmental lighting and water turbidity levels, and have sufficient accuracy\nfor reliable and repeatable navigation. This shortage presents a significant\nbarrier to enhancing the capabilities of remotely operated vehicles (ROVs) in\nsuch scenarios. This paper introduces an innovative positioning system for ROVs\noperating in confined, cluttered underwater settings, achieved through the\ncollaboration of an omnidirectional surface vehicle and an underwater ROV. A\nmathematical formulation based on the available sensors is proposed and\nevaluated. Experimental results from both a high-fidelity simulation\nenvironment and a mock-up of an industrial tank provide a proof of principle\nfor the system and demonstrate its practical deployability in real-world\nscenarios. Unlike many previous approaches, the system does not rely on fixed\ninfrastructure or tracking of features in the environment and can cover large\nenclosed areas without additional equipment.\n","authors":["Xueliang Cheng","Ognjen Marjanovic","Barry Lennox","Keir Groves"],"pdf_url":"https://arxiv.org/pdf/2403.10397v3.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2504.03120v3","updated":"2025-09-10T04:02:49Z","published":"2025-04-04T02:22:21Z","title":"Distributed Resilience-Aware Control in Multi-Robot Networks","summary":"  Ensuring resilient consensus in multi-robot systems with misbehaving agents\nremains a challenge, as many existing network resilience properties are\ninherently combinatorial and globally defined. While previous works have\nproposed control laws to enhance or preserve resilience in multi-robot\nnetworks, they often assume a fixed topology with known resilience properties,\nor require global state knowledge. These assumptions may be impractical in\nphysically-constrained environments, where safety and resilience requirements\nare conflicting, or when misbehaving agents share inaccurate state information.\nIn this work, we propose a distributed control law that enables each robot to\nguarantee resilient consensus and safety during its navigation without fixed\ntopologies using only locally available information. To this end, we establish\na sufficient condition for resilient consensus in time-varying networks based\non the degree of non-misbehaving or normal agents. Using this condition, we\ndesign a Control Barrier Function (CBF)-based controller that guarantees\nresilient consensus and collision avoidance without requiring estimates of\nglobal state and/or control actions of all other robots. Finally, we validate\nour method through simulations.\n","authors":["Haejoon Lee","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2504.03120v3.pdf","comment":"Accepted and will appear at 2025 IEEE Conference on Decision and\n  Control (CDC)"},{"id":"http://arxiv.org/abs/2509.08242v1","updated":"2025-09-10T02:52:45Z","published":"2025-09-10T02:52:45Z","title":"Behaviorally Heterogeneous Multi-Agent Exploration Using Distributed\n  Task Allocation","summary":"  We study a problem of multi-agent exploration with behaviorally heterogeneous\nrobots. Each robot maps its surroundings using SLAM and identifies a set of\nareas of interest (AoIs) or frontiers that are the most informative to explore\nnext. The robots assess the utility of going to a frontier using Behavioral\nEntropy (BE) and then determine which frontier to go to via a distributed task\nassignment scheme. We convert the task assignment problem into a\nnon-cooperative game and use a distributed algorithm (d-PBRAG) to converge to\nthe Nash equilibrium (which we show is the optimal task allocation solution).\nFor unknown utility cases, we provide robust bounds using approximate rewards.\nWe test our algorithm (which has less communication cost and fast convergence)\nin simulation, where we explore the effect of sensing radii, sensing accuracy,\nand heterogeneity among robotic teams with respect to the time taken to\ncomplete exploration and path traveled. We observe that having a team of agents\nwith heterogeneous behaviors is beneficial.\n","authors":["Nirabhra Mandal","Aamodh Suresh","Carlos Nieto-Granda","Sonia Martínez"],"pdf_url":"https://arxiv.org/pdf/2509.08242v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2509.08241v1","updated":"2025-09-10T02:47:42Z","published":"2025-09-10T02:47:42Z","title":"Sample-Efficient Online Control Policy Learning with Real-Time Recursive\n  Model Updates","summary":"  Data-driven control methods need to be sample-efficient and lightweight,\nespecially when data acquisition and computational resources are limited --\nsuch as during learning on hardware. Most modern data-driven methods require\nlarge datasets and struggle with real-time updates of models, limiting their\nperformance in dynamic environments. Koopman theory formally represents\nnonlinear systems as linear models over observables, and Koopman\nrepresentations can be determined from data in an optimization-friendly setting\nwith potentially rapid model updates. In this paper, we present a highly\nsample-efficient, Koopman-based learning pipeline: Recursive Koopman Learning\n(RKL). We identify sufficient conditions for model convergence and provide\nformal algorithmic analysis supporting our claim that RKL is lightweight and\nfast, with complexity independent of dataset size. We validate our method on a\nsimulated planar two-link arm and a hybrid nonlinear hardware system with soft\nactuators, showing that real-time recursive Koopman model updates improve the\nsample efficiency and stability of data-driven controller synthesis --\nrequiring only <10% of the data compared to benchmarks. The high-performance\nC++ codebase is open-sourced. Website:\nhttps://www.zixinatom990.com/home/robotics/corl-2025-recursive-koopman-learning.\n","authors":["Zixin Zhang","James Avtges","Todd D. Murphey"],"pdf_url":"https://arxiv.org/pdf/2509.08241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08235v1","updated":"2025-09-10T02:30:18Z","published":"2025-09-10T02:30:18Z","title":"Deep Visual Odometry for Stereo Event Cameras","summary":"  Event-based cameras are bio-inspired sensors with pixels that independently\nand asynchronously respond to brightness changes at microsecond resolution,\noffering the potential to handle state estimation tasks involving motion blur\nand high dynamic range (HDR) illumination conditions. However, the versatility\nof event-based visual odometry (VO) relying on handcrafted data association\n(either direct or indirect methods) is still unreliable, especially in field\nrobot applications under low-light HDR conditions, where the dynamic range can\nbe enormous and the signal-to-noise ratio is spatially-and-temporally varying.\nLeveraging deep neural networks offers new possibilities for overcoming these\nchallenges. In this paper, we propose a learning-based stereo event visual\nodometry. Building upon Deep Event Visual Odometry (DEVO), our system (called\nStereo-DEVO) introduces a novel and efficient static-stereo association\nstrategy for sparse depth estimation with almost no additional computational\nburden. By integrating it into a tightly coupled bundle adjustment (BA)\noptimization scheme, and benefiting from the recurrent network's ability to\nperform accurate optical flow estimation through voxel-based event\nrepresentations to establish reliable patch associations, our system achieves\nhigh-precision pose estimation in metric scale. In contrast to the offline\nperformance of DEVO, our system can process event data of \\zs{Video Graphics\nArray} (VGA) resolution in real time. Extensive evaluations on multiple public\nreal-world datasets and self-collected data justify our system's versatility,\ndemonstrating superior performance compared to state-of-the-art event-based VO\nmethods. More importantly, our system achieves stable pose estimation even in\nlarge-scale nighttime HDR scenarios.\n","authors":["Sheng Zhong","Junkai Niu","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2509.08235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08226v1","updated":"2025-09-10T01:59:00Z","published":"2025-09-10T01:59:00Z","title":"Input-gated Bilateral Teleoperation: An Easy-to-implement Force Feedback\n  Teleoperation Method for Low-cost Hardware","summary":"  Effective data collection in contact-rich manipulation requires force\nfeedback during teleoperation, as accurate perception of contact is crucial for\nstable control. However, such technology remains uncommon, largely because\nbilateral teleoperation systems are complex and difficult to implement. To\novercome this, we propose a bilateral teleoperation method that relies only on\na simple feedback controller and does not require force sensors. The approach\nis designed for leader-follower setups using low-cost hardware, making it\nbroadly applicable. Through numerical simulations and real-world experiments,\nwe demonstrate that the method requires minimal parameter tuning, yet achieves\nboth high operability and contact stability, outperforming conventional\napproaches. Furthermore, we show its high robustness: even at low communication\ncycle rates between leader and follower, control performance degradation is\nminimal compared to high-speed operation. We also prove our method can be\nimplemented on two types of commercially available low-cost hardware with zero\nparameter adjustments. This highlights its high ease of implementation and\nversatility. We expect this method will expand the use of force feedback\nteleoperation systems on low-cost hardware. This will contribute to advancing\ncontact-rich task autonomy in imitation learning.\n","authors":["Yoshiki Kanai","Akira Kanazawa","Hideyuki Ichiwara","Hiroshi Ito","Naoaki Noguchi","Tetsuya Ogata"],"pdf_url":"https://arxiv.org/pdf/2509.08226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15837v2","updated":"2025-09-10T01:58:54Z","published":"2024-12-20T12:26:22Z","title":"Traffic-Rule-Compliant Trajectory Repair via Satisfiability Modulo\n  Theories and Reachability Analysis","summary":"  Complying with traffic rules is challenging for automated vehicles, as\nnumerous rules need to be considered simultaneously. If a planned trajectory\nviolates traffic rules, it is common to replan a new trajectory from scratch.\nWe instead propose a trajectory repair technique to save computation time. By\ncoupling satisfiability modulo theories with set-based reachability analysis,\nwe determine if and in what manner the initial trajectory can be repaired.\nExperiments in high-fidelity simulators and in the real world demonstrate the\nbenefits of our proposed approach in various scenarios. Even in complex\nenvironments with intricate rules, we efficiently and reliably repair\nrule-violating trajectories, enabling automated vehicles to swiftly resume\nlegally safe operation in real time.\n","authors":["Yuanfei Lin","Zekun Xing","Xuyuan Han","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2412.15837v2.pdf","comment":"2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2509.08221v1","updated":"2025-09-10T01:38:55Z","published":"2025-09-10T01:38:55Z","title":"A Comprehensive Review of Reinforcement Learning for Autonomous Driving\n  in the CARLA Simulator","summary":"  Autonomous-driving research has recently embraced deep Reinforcement Learning\n(RL) as a promising framework for data-driven decision making, yet a clear\npicture of how these algorithms are currently employed, benchmarked and\nevaluated is still missing. This survey fills that gap by systematically\nanalysing around 100 peer-reviewed papers that train, test or validate RL\npolicies inside the open-source CARLA simulator. We first categorize the\nliterature by algorithmic family model-free, model-based, hierarchical, and\nhybrid and quantify their prevalence, highlighting that more than 80% of\nexisting studies still rely on model-free methods such as DQN, PPO and SAC.\nNext, we explain the diverse state, action and reward formulations adopted\nacross works, illustrating how choices of sensor modality (RGB, LiDAR, BEV,\nsemantic maps, and carla kinematics states), control abstraction (discrete vs.\ncontinuous) and reward shaping are used across various literature. We also\nconsolidate the evaluation landscape by listing the most common metrics\n(success rate, collision rate, lane deviation, driving score) and the towns,\nscenarios and traffic configurations used in CARLA benchmarks. Persistent\nchallenges including sparse rewards, sim-to-real transfer, safety guarantees\nand limited behaviour diversity are distilled into a set of open research\nquestions, and promising directions such as model-based RL, meta-learning and\nricher multi-agent simulations are outlined. By providing a unified taxonomy,\nquantitative statistics and a critical discussion of limitations, this review\naims to serve both as a reference for newcomers and as a roadmap for advancing\nRL-based autonomous driving toward real-world deployment.\n","authors":["Elahe Delavari","Feeza Khan Khanzada","Jaerock Kwon"],"pdf_url":"https://arxiv.org/pdf/2509.08221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.18662v2","updated":"2025-09-10T00:58:38Z","published":"2025-08-26T04:09:13Z","title":"Engineering Automotive Digital Twins on Standardized Architectures: A\n  Case Study","summary":"  Digital twin (DT) technology has become of interest in the automotive\nindustry. There is a growing need for smarter services that utilize the unique\ncapabilities of DTs, ranging from computer-aided remote control to cloud-based\nfleet coordination. Developing such services starts with the software\narchitecture. However, the scarcity of DT architectural guidelines poses a\nchallenge for engineering automotive DTs. Currently, the only DT architectural\nstandard is the one defined in ISO 23247. Though not developed for automotive\nsystems, it is one of the few feasible starting points for automotive DTs. In\nthis work, we investigate the suitability of the ISO 23247 reference\narchitecture for developing automotive DTs. Through the case study of\ndeveloping an Adaptive Cruise Control DT for a 1/10th-scale autonomous vehicle,\nwe identify some strengths and limitations of the reference architecture and\nbegin distilling future directions for researchers, practitioners, and standard\ndevelopers.\n","authors":["Stefan Ramdhan","Winnie Trandinh","Istvan David","Vera Pantelic","Mark Lawford"],"pdf_url":"https://arxiv.org/pdf/2508.18662v2.pdf","comment":"7 pages, 6 figures. Accepted at EDTconf 2025"},{"id":"http://arxiv.org/abs/2509.08197v1","updated":"2025-09-10T00:03:37Z","published":"2025-09-10T00:03:37Z","title":"Online Dynamic SLAM with Incremental Smoothing and Mapping","summary":"  Dynamic SLAM methods jointly estimate for the static and dynamic scene\ncomponents, however existing approaches, while accurate, are computationally\nexpensive and unsuitable for online applications. In this work, we present the\nfirst application of incremental optimisation techniques to Dynamic SLAM. We\nintroduce a novel factor-graph formulation and system architecture designed to\ntake advantage of existing incremental optimisation methods and support online\nestimation. On multiple datasets, we demonstrate that our method achieves equal\nto or better than state-of-the-art in camera pose and object motion accuracy.\nWe further analyse the structural properties of our approach to demonstrate its\nscalability and provide insight regarding the challenges of solving Dynamic\nSLAM incrementally. Finally, we show that our formulation results in problem\nstructure well-suited to incremental solvers, while our system architecture\nfurther enhances performance, achieving a 5x speed-up over existing methods.\n","authors":["Jesse Morris","Yiduo Wang","Viorela Ila"],"pdf_url":"https://arxiv.org/pdf/2509.08197v1.pdf","comment":"8 pages, 8 figures, Submitted RA-L 2025"}]},"2025-09-09T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2509.08177v1","updated":"2025-09-09T22:56:35Z","published":"2025-09-09T22:56:35Z","title":"Quadrotor Navigation using Reinforcement Learning with Privileged\n  Information","summary":"  This paper presents a reinforcement learning-based quadrotor navigation\nmethod that leverages efficient differentiable simulation, novel loss\nfunctions, and privileged information to navigate around large obstacles. Prior\nlearning-based methods perform well in scenes that exhibit narrow obstacles,\nbut struggle when the goal location is blocked by large walls or terrain. In\ncontrast, the proposed method utilizes time-of-arrival (ToA) maps as privileged\ninformation and a yaw alignment loss to guide the robot around large obstacles.\nThe policy is evaluated in photo-realistic simulation environments containing\nlarge obstacles, sharp corners, and dead-ends. Our approach achieves an 86%\nsuccess rate and outperforms baseline strategies by 34%. We deploy the policy\nonboard a custom quadrotor in outdoor cluttered environments both during the\nday and night. The policy is validated across 20 flights, covering 589 meters\nwithout collisions at speeds up to 4 m/s.\n","authors":["Jonathan Lee","Abhishek Rathod","Kshitij Goel","John Stecklein","Wennie Tabib"],"pdf_url":"https://arxiv.org/pdf/2509.08177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08859v1","updated":"2025-09-09T22:11:34Z","published":"2025-09-09T22:11:34Z","title":"Multi Robot Coordination in Highly Dynamic Environments: Tackling\n  Asymmetric Obstacles and Limited Communication","summary":"  Coordinating a fully distributed multi-agent system (MAS) can be challenging\nwhen the communication channel has very limited capabilities in terms of\nsending rate and packet payload. When the MAS has to deal with active obstacles\nin a highly partially observable environment, the communication channel\nacquires considerable relevance. In this paper, we present an approach to deal\nwith task assignments in extremely active scenarios, where tasks need to be\nfrequently reallocated among the agents participating in the coordination\nprocess. Inspired by market-based task assignments, we introduce a novel\ndistributed coordination method to orchestrate autonomous agents' actions\nefficiently in low communication scenarios. In particular, our algorithm takes\ninto account asymmetric obstacles. While in the real world, the majority of\nobstacles are asymmetric, they are usually treated as symmetric ones, thus\nlimiting the applicability of existing methods. To summarize, the presented\narchitecture is designed to tackle scenarios where the obstacles are active and\nasymmetric, the communication channel is poor and the environment is partially\nobservable. Our approach has been validated in simulation and in the real\nworld, using a team of NAO robots during official RoboCup competitions.\nExperimental results show a notable reduction in task overlaps in limited\ncommunication settings, with a decrease of 52% in the most frequent reallocated\ntask.\n","authors":["Vincenzo Suriani","Daniele Affinita","Domenico D. Bloisi","Daniele Nardi"],"pdf_url":"https://arxiv.org/pdf/2509.08859v1.pdf","comment":"The 19th International Conference on Intelligent Autonomous Systems\n  (IAS 19), 2025, Genoa"},{"id":"http://arxiv.org/abs/2509.08160v1","updated":"2025-09-09T21:41:23Z","published":"2025-09-09T21:41:23Z","title":"Diffusion-Guided Multi-Arm Motion Planning","summary":"  Multi-arm motion planning is fundamental for enabling arms to complete\ncomplex long-horizon tasks in shared spaces efficiently but current methods\nstruggle with scalability due to exponential state-space growth and reliance on\nlarge training datasets for learned models. Inspired by Multi-Agent Path\nFinding (MAPF), which decomposes planning into single-agent problems coupled\nwith collision resolution, we propose a novel diffusion-guided multi-arm\nplanner (DG-MAP) that enhances scalability of learning-based models while\nreducing their reliance on massive multi-arm datasets. Recognizing that\ncollisions are primarily pairwise, we train two conditional diffusion models,\none to generate feasible single-arm trajectories, and a second, to model the\ndual-arm dynamics required for effective pairwise collision resolution. By\nintegrating these specialized generative models within a MAPF-inspired\nstructured decomposition, our planner efficiently scales to larger number of\narms. Evaluations against alternative learning-based methods across various\nteam sizes demonstrate our method's effectiveness and practical applicability.\nProject website can be found at https://diff-mapf-mers.csail.mit.edu\n","authors":["Viraj Parimi","Brian C. Williams"],"pdf_url":"https://arxiv.org/pdf/2509.08160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08159v1","updated":"2025-09-09T21:39:13Z","published":"2025-09-09T21:39:13Z","title":"Zero-Shot Metric Depth Estimation via Monocular Visual-Inertial\n  Rescaling for Autonomous Aerial Navigation","summary":"  This paper presents a methodology to predict metric depth from monocular RGB\nimages and an inertial measurement unit (IMU). To enable collision avoidance\nduring autonomous flight, prior works either leverage heavy sensors (e.g.,\nLiDARs or stereo cameras) or data-intensive and domain-specific fine-tuning of\nmonocular metric depth estimation methods. In contrast, we propose several\nlightweight zero-shot rescaling strategies to obtain metric depth from relative\ndepth estimates via the sparse 3D feature map created using a visual-inertial\nnavigation system. These strategies are compared for their accuracy in diverse\nsimulation environments. The best performing approach, which leverages\nmonotonic spline fitting, is deployed in the real-world on a\ncompute-constrained quadrotor. We obtain on-board metric depth estimates at 15\nHz and demonstrate successful collision avoidance after integrating the\nproposed method with a motion primitives-based planner.\n","authors":["Steven Yang","Xiaoyu Tian","Kshitij Goel","Wennie Tabib"],"pdf_url":"https://arxiv.org/pdf/2509.08159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08157v1","updated":"2025-09-09T21:35:55Z","published":"2025-09-09T21:35:55Z","title":"Risk-Bounded Multi-Agent Visual Navigation via Dynamic Budget Allocation","summary":"  Safe navigation is essential for autonomous systems operating in hazardous\nenvironments, especially when multiple agents must coordinate using just visual\ninputs over extended time horizons. Traditional planning methods excel at\nsolving long-horizon tasks but rely on predefined distance metrics, while safe\nReinforcement Learning (RL) can learn complex behaviors using high-dimensional\ninputs yet struggles with multi-agent, goal-conditioned scenarios. Recent work\ncombined these paradigms by leveraging goal-conditioned RL (GCRL) to build an\nintermediate graph from replay buffer states, pruning unsafe edges, and using\nConflict-Based Search (CBS) for multi-agent path planning. Although effective,\nthis graph-pruning approach can be overly conservative, limiting mission\nefficiency by precluding missions that must traverse high-risk regions. To\naddress this limitation, we propose RB-CBS, a novel extension to CBS that\ndynamically allocates and adjusts user-specified risk bound ($\\Delta$) across\nagents to flexibly trade off safety and speed. Our improved planner ensures\nthat each agent receives a local risk budget ($\\delta$) enabling more efficient\nnavigation while still respecting overall safety constraints. Experimental\nresults demonstrate that this iterative risk-allocation framework yields\nsuperior performance in complex environments, allowing multiple agents to find\ncollision-free paths within the user-specified $\\Delta$.\n","authors":["Viraj Parimi","Brian C. Williams"],"pdf_url":"https://arxiv.org/pdf/2509.08157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08147v1","updated":"2025-09-09T21:07:45Z","published":"2025-09-09T21:07:45Z","title":"Mean Field Game-Based Interactive Trajectory Planning Using\n  Physics-Inspired Unified Potential Fields","summary":"  Interactive trajectory planning in autonomous driving must balance safety,\nefficiency, and scalability under heterogeneous driving behaviors. Existing\nmethods often face high computational cost or rely on external safety critics.\nTo address this, we propose an Interaction-Enriched Unified Potential Field\n(IUPF) framework that fuses style-dependent benefit and risk fields through a\nphysics-inspired variational model, grounded in mean field game theory. The\napproach captures conservative, aggressive, and cooperative behaviors without\nadditional safety modules, and employs stochastic differential equations to\nguarantee Nash equilibrium with exponential convergence. Simulations on lane\nchanging and overtaking scenarios show that IUPF ensures safe distances,\ngenerates smooth and efficient trajectories, and outperforms traditional\noptimization and game-theoretic baselines in both adaptability and\ncomputational efficiency.\n","authors":["Zhen Tian","Fujiang Yuan","Chunhong Yuan","Yanhong Peng"],"pdf_url":"https://arxiv.org/pdf/2509.08147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08126v1","updated":"2025-09-09T20:07:51Z","published":"2025-09-09T20:07:51Z","title":"Attribute-based Object Grounding and Robot Grasp Detection with Spatial\n  Reasoning","summary":"  Enabling robots to grasp objects specified through natural language is\nessential for effective human-robot interaction, yet it remains a significant\nchallenge. Existing approaches often struggle with open-form language\nexpressions and typically assume unambiguous target objects without duplicates.\nMoreover, they frequently rely on costly, dense pixel-wise annotations for both\nobject grounding and grasp configuration. We present Attribute-based Object\nGrounding and Robotic Grasping (OGRG), a novel framework that interprets\nopen-form language expressions and performs spatial reasoning to ground target\nobjects and predict planar grasp poses, even in scenes containing duplicated\nobject instances. We investigate OGRG in two settings: (1) Referring Grasp\nSynthesis (RGS) under pixel-wise full supervision, and (2) Referring Grasp\nAffordance (RGA) using weakly supervised learning with only single-pixel grasp\nannotations. Key contributions include a bi-directional vision-language fusion\nmodule and the integration of depth information to enhance geometric reasoning,\nimproving both grounding and grasping performance. Experiment results show that\nOGRG outperforms strong baselines in tabletop scenes with diverse spatial\nlanguage instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX\n2080 Ti GPU, enabling potential use in closed-loop or multi-object sequential\ngrasping, while delivering superior grounding and grasp prediction accuracy\ncompared to all the baselines considered. Under the weakly supervised RGA\nsetting, OGRG also surpasses baseline grasp-success rates in both simulation\nand real-robot trials, underscoring the effectiveness of its spatial reasoning\ndesign. Project page: https://z.umn.edu/ogrg\n","authors":["Houjian Yu","Zheming Zhou","Min Sun","Omid Ghasemalizadeh","Yuyin Sun","Cheng-Hao Kuo","Arnie Sen","Changhyun Choi"],"pdf_url":"https://arxiv.org/pdf/2509.08126v1.pdf","comment":"Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid\n  Robots"},{"id":"http://arxiv.org/abs/2509.08117v1","updated":"2025-09-09T19:46:21Z","published":"2025-09-09T19:46:21Z","title":"Online Learning and Coverage of Unknown Fields Using Random-Feature\n  Gaussian Processes","summary":"  This paper proposes a framework for multi-robot systems to perform\nsimultaneous learning and coverage of the domain of interest characterized by\nan unknown and potentially time-varying density function. To overcome the\nlimitations of Gaussian Process (GP) regression, we employ Random Feature GP\n(RFGP) and its online variant (O-RFGP) that enables online and incremental\ninference. By integrating these with Voronoi-based coverage control and Upper\nConfidence Bound (UCB) sampling strategy, a team of robots can adaptively focus\non important regions while refining the learned spatial field for efficient\ncoverage. Under mild assumptions, we provide theoretical guarantees and\nevaluate the framework through simulations in time-invariant scenarios.\nFurthermore, its effectiveness in time-varying settings is demonstrated through\nadditional simulations and a physical experiment.\n","authors":["Ruijie Du","Ruoyu Lin","Yanning Shen","Magnus Egerstedt"],"pdf_url":"https://arxiv.org/pdf/2509.08117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09918v2","updated":"2025-09-09T19:17:32Z","published":"2024-09-16T01:29:01Z","title":"Hardware-Accelerated Ray Tracing for Discrete and Continuous Collision\n  Detection on GPUs","summary":"  This paper presents a set of simple and intuitive robot collision detection\nalgorithms that show substantial scaling improvements for high geometric\ncomplexity and large numbers of collision queries by leveraging\nhardware-accelerated ray tracing on GPUs. It is the first leveraging\nhardware-accelerated ray-tracing for direct volume mesh-to-mesh discrete\ncollision detection and applying it to continuous collision detection. We\nintroduce two methods: Ray-Traced Discrete-Pose Collision Detection for exact\nrobot mesh to obstacle mesh collision detection, and Ray-Traced Continuous\nCollision Detection for robot sphere representation to obstacle mesh swept\ncollision detection, using piecewise-linear or quadratic B-splines. For robot\nlink meshes totaling 24k triangles and obstacle meshes of over 190k triangles,\nour methods were up to 3 times faster in batched discrete-pose queries than a\nstate-of-the-art GPU-based method using a sphere robot representation. For the\nsame obstacle mesh scene, our sphere-robot continuous collision detection was\nup to 9 times faster depending on trajectory batch size. We also performed a\ndetailed measurement of the volume coverage accuracy of various sphere/mesh\npose/path representations to provide insight into the tradeoffs between speed\nand accuracy of different robot collision detection methods.\n","authors":["Sizhe Sui","Luis Sentis","Andrew Bylard"],"pdf_url":"https://arxiv.org/pdf/2409.09918v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08095v1","updated":"2025-09-09T19:05:37Z","published":"2025-09-09T19:05:37Z","title":"Real-Time Obstacle Avoidance for a Mobile Robot Using CNN-Based Sensor\n  Fusion","summary":"  Obstacle avoidance is a critical component of the navigation stack required\nfor mobile robots to operate effectively in complex and unknown environments.\nIn this research, three end-to-end Convolutional Neural Networks (CNNs) were\ntrained and evaluated offline and deployed on a differential-drive mobile robot\nfor real-time obstacle avoidance to generate low-level steering commands from\nsynchronized color and depth images acquired by an Intel RealSense D415 RGB-D\ncamera in diverse environments. Offline evaluation showed that the NetConEmb\nmodel achieved the best performance with a notably low MedAE of $0.58 \\times\n10^{-3}$ rad/s. In comparison, the lighter NetEmb architecture adopted in this\nstudy, which reduces the number of trainable parameters by approximately 25\\%\nand converges faster, produced comparable results with an RMSE of $21.68 \\times\n10^{-3}$ rad/s, close to the $21.42 \\times 10^{-3}$ rad/s obtained by\nNetConEmb. Real-time navigation further confirmed NetConEmb's robustness,\nachieving a 100\\% success rate in both known and unknown environments, while\nNetEmb and NetGated succeeded only in navigating the known environment.\n","authors":["Lamiaa H. Zain","Raafat E. Shalaby"],"pdf_url":"https://arxiv.org/pdf/2509.08095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.08085v1","updated":"2025-09-09T18:43:19Z","published":"2025-09-09T18:43:19Z","title":"Planar Juggling of a Devil-Stick using Discrete VHCs","summary":"  Planar juggling of a devil-stick using impulsive inputs is addressed using\nthe concept of discrete virtual holonomic constraints (DVHC). The location of\nthe center-of-mass of the devil-stick is specified in terms of its orientation\nat the discrete instants when impulsive control inputs are applied. The\ndiscrete zero dynamics (DZD) resulting from the choice of DVHC provides\nconditions for stable juggling. A control design that enforces the DVHC and an\norbit stabilizing controller are presented. The approach is validated in\nsimulation.\n","authors":["Aakash Khandelwal","Ranjan Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2509.08085v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2509.08069v1","updated":"2025-09-09T18:15:22Z","published":"2025-09-09T18:15:22Z","title":"SVN-ICP: Uncertainty Estimation of ICP-based LiDAR Odometry using Stein\n  Variational Newton","summary":"  This letter introduces SVN-ICP, a novel Iterative Closest Point (ICP)\nalgorithm with uncertainty estimation that leverages Stein Variational Newton\n(SVN) on manifold. Designed specifically for fusing LiDAR odometry in\nmultisensor systems, the proposed method ensures accurate pose estimation and\nconsistent noise parameter inference, even in LiDAR-degraded environments. By\napproximating the posterior distribution using particles within the Stein\nVariational Inference framework, SVN-ICP eliminates the need for explicit noise\nmodeling or manual parameter tuning. To evaluate its effectiveness, we\nintegrate SVN-ICP into a simple error-state Kalman filter alongside an IMU and\ntest it across multiple datasets spanning diverse environments and robot types.\nExtensive experimental results demonstrate that our approach outperforms\nbest-in-class methods on challenging scenarios while providing reliable\nuncertainty estimates.\n","authors":["Shiping Ma","Haoming Zhang","Marc Toussaint"],"pdf_url":"https://arxiv.org/pdf/2509.08069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07962v1","updated":"2025-09-09T17:50:37Z","published":"2025-09-09T17:50:37Z","title":"TA-VLA: Elucidating the Design Space of Torque-aware\n  Vision-Language-Action Models","summary":"  Many robotic manipulation tasks require sensing and responding to force\nsignals such as torque to assess whether the task has been successfully\ncompleted and to enable closed-loop control. However, current\nVision-Language-Action (VLA) models lack the ability to integrate such subtle\nphysical feedback. In this work, we explore Torque-aware VLA models, aiming to\nbridge this gap by systematically studying the design space for incorporating\ntorque signals into existing VLA architectures. We identify and evaluate\nseveral strategies, leading to three key findings. First, introducing torque\nadapters into the decoder consistently outperforms inserting them into the\nencoder.Third, inspired by joint prediction and planning paradigms in\nautonomous driving, we propose predicting torque as an auxiliary output, which\nfurther improves performance. This strategy encourages the model to build a\nphysically grounded internal representation of interaction dynamics. Extensive\nquantitative and qualitative experiments across contact-rich manipulation\nbenchmarks validate our findings.\n","authors":["Zongzheng Zhang","Haobo Xu","Zhuo Yang","Chenghao Yue","Zehao Lin","Huan-ang Gao","Ziwei Wang","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.07962v1.pdf","comment":"Accepted to CoRL 2025, project page:\n  \\url{https://zzongzheng0918.github.io/Torque-Aware-VLA.github.io/}"},{"id":"http://arxiv.org/abs/2509.07957v1","updated":"2025-09-09T17:44:36Z","published":"2025-09-09T17:44:36Z","title":"Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm\n  Robotic Manipulation","summary":"  Acquiring dexterous robotic skills from human video demonstrations remains a\nsignificant challenge, largely due to conventional reliance on low-level\ntrajectory replication, which often fails to generalize across varying objects,\nspatial layouts, and manipulator configurations. To address this limitation, we\nintroduce Graph-Fused Vision-Language-Action (GF-VLA), a unified framework that\nenables dual-arm robotic systems to perform task-level reasoning and execution\ndirectly from RGB-D human demonstrations. GF-VLA employs an\ninformation-theoretic approach to extract task-relevant cues, selectively\nhighlighting critical hand-object and object-object interactions. These cues\nare structured into temporally ordered scene graphs, which are subsequently\nintegrated with a language-conditioned transformer to produce hierarchical\nbehavior trees and interpretable Cartesian motion primitives. To enhance\nefficiency in bimanual execution, we propose a cross-arm allocation strategy\nthat autonomously determines gripper assignment without requiring explicit\ngeometric modeling. We validate GF-VLA on four dual-arm block assembly\nbenchmarks involving symbolic structure construction and spatial\ngeneralization. Empirical results demonstrate that the proposed representation\nachieves over 95% graph accuracy and 93% subtask segmentation, enabling the\nlanguage-action planner to generate robust, interpretable task policies. When\ndeployed on a dual-arm robot, these policies attain 94% grasp reliability, 89%\nplacement accuracy, and 90% overall task success across stacking,\nletter-formation, and geometric reconfiguration tasks, evidencing strong\ngeneralization and robustness under diverse spatial and semantic variations.\n","authors":["Shunlei Li","Longsen Gao","Jiuwen Cao","Yingbai Hu"],"pdf_url":"https://arxiv.org/pdf/2509.07957v1.pdf","comment":"This paper is submitted to IEEE IROS 2025 Workshop AIR4S"},{"id":"http://arxiv.org/abs/2509.07953v1","updated":"2025-09-09T17:41:29Z","published":"2025-09-09T17:41:29Z","title":"RaC: Robot Learning for Long-Horizon Tasks by Scaling Recovery and\n  Correction","summary":"  Modern paradigms for robot imitation train expressive policy architectures on\nlarge amounts of human demonstration data. Yet performance on contact-rich,\ndeformable-object, and long-horizon tasks plateau far below perfect execution,\neven with thousands of expert demonstrations. This is due to the inefficiency\nof existing ``expert'' data collection procedures based on human teleoperation.\nTo address this issue, we introduce RaC, a new phase of training on\nhuman-in-the-loop rollouts after imitation learning pre-training. In RaC, we\nfine-tune a robotic policy on human intervention trajectories that illustrate\nrecovery and correction behaviors. Specifically, during a policy rollout, human\noperators intervene when failure appears imminent, first rewinding the robot\nback to a familiar, in-distribution state and then providing a corrective\nsegment that completes the current sub-task. Training on this data composition\nexpands the robotic skill repertoire to include retry and adaptation behaviors,\nwhich we show are crucial for boosting both efficiency and robustness on\nlong-horizon tasks. Across three real-world bimanual control tasks: shirt\nhanging, airtight container lid sealing, takeout box packing, and a simulated\nassembly task, RaC outperforms the prior state-of-the-art using 10$\\times$ less\ndata collection time and samples. We also show that RaC enables test-time\nscaling: the performance of the trained RaC policy scales linearly in the\nnumber of recovery maneuvers it exhibits. Videos of the learned policy are\navailable at https://rac-scaling-robot.github.io/.\n","authors":["Zheyuan Hu","Robyn Wu","Naveen Enock","Jasmine Li","Riya Kadakia","Zackory Erickson","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2509.07953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07942v1","updated":"2025-09-09T17:24:26Z","published":"2025-09-09T17:24:26Z","title":"Knowledge Isn't Power: The Ethics of Social Robots and the Difficulty of\n  Informed Consent","summary":"  Contemporary robots are increasingly mimicking human social behaviours to\nfacilitate interaction, such as smiling to signal approachability, or\nhesitating before taking an action to allow people time to react. Such\ntechniques can activate a person's entrenched social instincts, triggering\nemotional responses as though they are interacting with a fellow human, and can\nprompt them to treat a robot as if it truly possesses the underlying life-like\nprocesses it outwardly presents, raising significant ethical questions. We\nengage these issues through the lens of informed consent: drawing upon\nprevailing legal principles and ethics, we examine how social robots can\ninfluence user behaviour in novel ways, and whether under those circumstances\nusers can be appropriately informed to consent to these heightened\ninteractions. We explore the complex circumstances of human-robot interaction\nand highlight how it differs from more familiar interaction contexts, and we\napply legal principles relating to informed consent to social robots in order\nto reconceptualize the current ethical debates surrounding the field. From this\ninvestigation, we synthesize design goals for robot developers to achieve more\nethical and informed human-robot interaction.\n","authors":["James M. Berzuk","Lauren Corcoran","Brannen McKenzie-Lefurgey","Katie Szilagyi","James E. Young"],"pdf_url":"https://arxiv.org/pdf/2509.07942v1.pdf","comment":"Submitted to the International Journal of Social Robotics. 18 pages,\n  1 figure"},{"id":"http://arxiv.org/abs/2509.07916v1","updated":"2025-09-09T16:57:10Z","published":"2025-09-09T16:57:10Z","title":"Programmable Locking Cells (PLC) for Modular Robots with High Stiffness\n  Tunability and Morphological Adaptability","summary":"  Robotic systems operating in unstructured environments require the ability to\nswitch between compliant and rigid states to perform diverse tasks such as\nadaptive grasping, high-force manipulation, shape holding, and navigation in\nconstrained spaces, among others. However, many existing variable stiffness\nsolutions rely on complex actuation schemes, continuous input power, or\nmonolithic designs, limiting their modularity and scalability. This paper\npresents the Programmable Locking Cell (PLC)-a modular, tendon-driven unit that\nachieves discrete stiffness modulation through mechanically interlocked joints\nactuated by cable tension. Each unit transitions between compliant and firm\nstates via structural engagement, and the assembled system exhibits high\nstiffness variation-up to 950% per unit-without susceptibility to damage under\nhigh payload in the firm state. Multiple PLC units can be assembled into\nreconfigurable robotic structures with spatially programmable stiffness. We\nvalidate the design through two functional prototypes: (1) a variable-stiffness\ngripper capable of adaptive grasping, firm holding, and in-hand manipulation;\nand (2) a pipe-traversing robot composed of serial PLC units that achieves\nshape adaptability and stiffness control in confined environments. These\nresults demonstrate the PLC as a scalable, structure-centric mechanism for\nprogrammable stiffness and motion, enabling robotic systems with reconfigurable\nmorphology and task-adaptive interaction.\n","authors":["Jianshu Zhou","Wei Chen","Junda Huang","Boyuan Liang","Yunhui Liu","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2509.07916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07873v1","updated":"2025-09-09T15:57:25Z","published":"2025-09-09T15:57:25Z","title":"A Robot That Listens: Enhancing Self-Disclosure and Engagement Through\n  Sentiment-based Backchannels and Active Listening","summary":"  As social robots get more deeply integrated intoour everyday lives, they will\nbe expected to engage in meaningful conversations and exhibit socio-emotionally\nintelligent listening behaviors when interacting with people. Active listening\nand backchanneling could be one way to enhance robots' communicative\ncapabilities and enhance their effectiveness in eliciting deeper\nself-disclosure, providing a sense of empathy,and forming positive rapport and\nrelationships with people.Thus, we developed an LLM-powered social robot that\ncan exhibit contextually appropriate sentiment-based backchannelingand active\nlistening behaviors (active listening+backchanneling) and compared its efficacy\nin eliciting people's self-disclosurein comparison to robots that do not\nexhibit any of these listening behaviors (control) and a robot that only\nexhibitsbackchanneling behavior (backchanneling-only). Through ourexperimental\nstudy with sixty-five participants, we found theparticipants who conversed with\nthe active listening robot per-ceived the interactions more positively, in\nwhich they exhibited the highest self-disclosures, and reported the strongest\nsenseof being listened to. The results of our study suggest that the\nimplementation of active listening behaviors in social robotshas the potential\nto improve human-robot communication andcould further contribute to the\nbuilding of deeper human-robot relationships and rapport.\n","authors":["Hieu Tran","Go-Eum Cha","Sooyeon Jeong"],"pdf_url":"https://arxiv.org/pdf/2509.07873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04949v2","updated":"2025-09-09T15:26:22Z","published":"2025-07-07T12:49:20Z","title":"Monte Carlo Tree Search with Tensor Factorization for Robot Optimization","summary":"  Many robotic tasks, such as inverse kinematics, motion planning, and optimal\ncontrol, can be formulated as optimization problems. Solving these problems\ninvolves addressing nonlinear kinematics, complex contact dynamics,\nlong-horizon correlation, and multi-modal landscapes, each posing distinct\nchallenges for state-of-the-art optimization methods. Monte Carlo Tree Search\nis a powerful approach that can strategically explore the solution space and\ncan be applied to a wide range of tasks across varying scenarios. However, it\ntypically suffers from combinatorial complexity when applied to robotics,\nresulting in slow convergence and high memory demands. To address this\nlimitation, we propose \\emph{Tensor Train Tree Search} (TTTS), which leverages\ntensor factorization to exploit correlations among decision variables arising\nfrom common kinematic structures, dynamic constraints, and environmental\ninteractions in robot decision-making. This yields a compact, linear-complexity\nrepresentation that significantly reduces both computation time and storage\nrequirements. We prove that TTTS can efficiently reach the bounded global\noptimum within a finite time. Experimental results across inverse kinematics,\nmotion planning around obstacles, legged robot manipulation, multi-stage motion\nplanning, and bimanual whole-body manipulation demonstrate the efficiency of\nTTTS on a diverse set of robotic tasks.\n","authors":["Teng Xue","Yan Zhang","Amirreza Razmjoo","Sylvain Calinon"],"pdf_url":"https://arxiv.org/pdf/2507.04949v2.pdf","comment":"21 pages, 11 figures"},{"id":"http://arxiv.org/abs/2509.07812v1","updated":"2025-09-09T14:51:41Z","published":"2025-09-09T14:51:41Z","title":"Unlocking Stopped-Rotor Flight: Development and Validation of SPERO, a\n  Novel UAV Platform","summary":"  Stop-rotor aircraft have long been proposed as the ideal vertical takeoff and\nlanding (VTOL) aircraft for missions with equal time spent in both flight\nregimes, such as agricultural monitoring, search and rescue, and last-mile\ndelivery. Featuring a central lifting surface that rotates in VTOL to generate\nvertical thrust and locks in forward flight to generate passive lift, the\nstop-rotor offers the potential for high efficiency across both modes. However,\npractical implementation has remained infeasible due to aerodynamic and\nstability conflicts between flight modes. In this work, we present SPERO\n(Stopped-Penta Rotor), a stop-rotor uncrewed aerial vehicle (UAV) featuring a\nflipping and latching wing, an active center of pressure mechanism, thrust\nvectored counterbalances, a five-rotor architecture, and an eleven-state\nmachine flight controller coordinating geometric and controller\nreconfiguration. Furthermore, SPERO establishes a generalizable design and\ncontrol framework for stopped-rotor UAVs. Together, these innovations overcome\nlongstanding challenges in stop-rotor flight and enable the first stable,\nbidirectional transition between VTOL and forward flight.\n","authors":["Kristan Hilby","Ian Hunter"],"pdf_url":"https://arxiv.org/pdf/2509.07812v1.pdf","comment":"15 pages, 11 figures, 5 tables"},{"id":"http://arxiv.org/abs/2508.21112v3","updated":"2025-09-09T14:36:49Z","published":"2025-08-28T17:26:15Z","title":"EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control","summary":"  The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.\n","authors":["Delin Qu","Haoming Song","Qizhi Chen","Zhaoqing Chen","Xianqiang Gao","Xinyi Ye","Qi Lv","Modi Shi","Guanghui Ren","Cheng Ruan","Maoqing Yao","Haoran Yang","Jiacheng Bao","Bin Zhao","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2508.21112v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.18206v2","updated":"2025-09-09T13:34:50Z","published":"2025-07-24T09:02:13Z","title":"MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial\n  Navigation","summary":"  A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application.\n","authors":["Arup Kumar Sahoo","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2507.18206v2.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2509.07707v1","updated":"2025-09-09T13:10:58Z","published":"2025-09-09T13:10:58Z","title":"Fault Tolerant Control of a Quadcopter using Reinforcement Learning","summary":"  This study presents a novel reinforcement learning (RL)-based control\nframework aimed at enhancing the safety and robustness of the quadcopter, with\na specific focus on resilience to in-flight one propeller failure. Addressing\nthe critical need of a robust control strategy for maintaining a desired\naltitude for the quadcopter to safe the hardware and the payload in physical\napplications. The proposed framework investigates two RL methodologies Dynamic\nProgramming (DP) and Deep Deterministic Policy Gradient (DDPG), to overcome the\nchallenges posed by the rotor failure mechanism of the quadcopter. DP, a\nmodel-based approach, is leveraged for its convergence guarantees, despite high\ncomputational demands, whereas DDPG, a model-free technique, facilitates rapid\ncomputation but with constraints on solution duration. The research challenge\narises from training RL algorithms on large dimensions and action domains. With\nmodifications to the existing DP and DDPG algorithms, the controllers were\ntrained not only to cater for large continuous state and action domain and also\nachieve a desired state after an inflight propeller failure. To verify the\nrobustness of the proposed control framework, extensive simulations were\nconducted in a MATLAB environment across various initial conditions and\nunderscoring its viability for mission-critical quadcopter applications. A\ncomparative analysis was performed between both RL algorithms and their\npotential for applications in faulty aerial systems.\n","authors":["Muzaffar Habib","Adnan Maqsood","Adnan Fayyaz ud Din"],"pdf_url":"https://arxiv.org/pdf/2509.07707v1.pdf","comment":"e-ISSN: 1946-3901, ISSN: 1946-3855,\n  https://www.sae.org/publications/technical-papers/content/01-18-01-0006/"},{"id":"http://arxiv.org/abs/2411.11683v4","updated":"2025-09-09T13:01:07Z","published":"2024-11-18T16:09:26Z","title":"TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic\n  Manipulation","summary":"  Robotic manipulation in the physical world is increasingly empowered by\n\\textit{large language models} (LLMs) and \\textit{vision-language models}\n(VLMs), leveraging their understanding and perception capabilities. Recently,\nvarious attacks against such robotic policies have been proposed, with backdoor\nattacks drawing considerable attention for their high stealth and strong\npersistence capabilities. However, existing backdoor efforts are limited to\nsimulators and suffer from physical-world realization. To address this, we\npropose \\textit{TrojanRobot}, a highly stealthy and broadly effective robotic\nbackdoor attack in the physical world. Specifically, we introduce a\nmodule-poisoning approach by embedding a backdoor module into the modular\nrobotic policy, enabling backdoor control over the policy's visual perception\nmodule thereby backdooring the entire robotic policy. Our vanilla\nimplementation leverages a backdoor-finetuned VLM to serve as the backdoor\nmodule. To enhance its generalization in physical environments, we propose a\nprime implementation, leveraging the LVLM-as-a-backdoor paradigm and developing\nthree types of prime attacks, \\ie, \\textit{permutation}, \\textit{stagnation},\nand \\textit{intentional} attacks, thus achieving finer-grained backdoors.\nExtensive experiments on the UR3e manipulator with 18 task instructions using\nrobotic policies based on four VLMs demonstrate the broad effectiveness and\nphysical-world stealth of TrojanRobot. Our attack's video demonstrations are\navailable via a github link https://trojanrobot.github.io.\n","authors":["Xianlong Wang","Hewen Pan","Hangtao Zhang","Minghui Li","Shengshan Hu","Ziqi Zhou","Lulu Xue","Peijin Guo","Aishan Liu","Leo Yu Zhang","Xiaohua Jia"],"pdf_url":"https://arxiv.org/pdf/2411.11683v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07674v1","updated":"2025-09-09T12:40:08Z","published":"2025-09-09T12:40:08Z","title":"Temporal Counterfactual Explanations of Behaviour Tree Decisions","summary":"  Explainability is a critical tool in helping stakeholders understand robots.\nIn particular, the ability for robots to explain why they have made a\nparticular decision or behaved in a certain way is useful in this regard.\nBehaviour trees are a popular framework for controlling the decision-making of\nrobots and other software systems, and thus a natural question to ask is\nwhether or not a system driven by a behaviour tree is capable of answering\n\"why\" questions. While explainability for behaviour trees has seen some prior\nattention, no existing methods are capable of generating causal, counterfactual\nexplanations which detail the reasons for robot decisions and behaviour.\nTherefore, in this work, we introduce a novel approach which automatically\ngenerates counterfactual explanations in response to contrastive \"why\"\nquestions. Our method achieves this by first automatically building a causal\nmodel from the structure of the behaviour tree as well as domain knowledge\nabout the state and individual behaviour tree nodes. The resultant causal model\nis then queried and searched to find a set of diverse counterfactual\nexplanations. We demonstrate that our approach is able to correctly explain the\nbehaviour of a wide range of behaviour tree structures and states. By being\nable to answer a wide range of causal queries, our approach represents a step\ntowards more transparent, understandable and ultimately trustworthy robotic\nsystems.\n","authors":["Tamlin Love","Antonio Andriella","Guillem Alenyà"],"pdf_url":"https://arxiv.org/pdf/2509.07674v1.pdf","comment":"23 pages, 6 figures, submitted to Engineering Applications of\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2509.07655v1","updated":"2025-09-09T12:22:21Z","published":"2025-09-09T12:22:21Z","title":"Collaborative Exploration with a Marsupial Ground-Aerial Robot Team\n  through Task-Driven Map Compression","summary":"  Efficient exploration of unknown environments is crucial for autonomous\nrobots, especially in confined and large-scale scenarios with limited\ncommunication. To address this challenge, we propose a collaborative\nexploration framework for a marsupial ground-aerial robot team that leverages\nthe complementary capabilities of both platforms. The framework employs a\ngraph-based path planning algorithm to guide exploration and deploy the aerial\nrobot in areas where its expected gain significantly exceeds that of the ground\nrobot, such as large open spaces or regions inaccessible to the ground\nplatform, thereby maximizing coverage and efficiency. To facilitate large-scale\nspatial information sharing, we introduce a bandwidth-efficient, task-driven\nmap compression strategy. This method enables each robot to reconstruct\nresolution-specific volumetric maps while preserving exploration-critical\ndetails, even at high compression rates. By selectively compressing and sharing\nkey data, communication overhead is minimized, ensuring effective map\nintegration for collaborative path planning. Simulation and real-world\nexperiments validate the proposed approach, demonstrating its effectiveness in\nimproving exploration efficiency while significantly reducing data\ntransmission.\n","authors":["Angelos Zacharia","Mihir Dharmadhikari","Kostas Alexis"],"pdf_url":"https://arxiv.org/pdf/2509.07655v1.pdf","comment":"Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2509.07646v1","updated":"2025-09-09T12:15:04Z","published":"2025-09-09T12:15:04Z","title":"Decoding RobKiNet: Insights into Efficient Training of Robotic\n  Kinematics Informed Neural Network","summary":"  In robots task and motion planning (TAMP), it is crucial to sample within the\nrobot's configuration space to meet task-level global constraints and enhance\nthe efficiency of subsequent motion planning. Due to the complexity of joint\nconfiguration sampling under multi-level constraints, traditional methods often\nlack efficiency. This paper introduces the principle of RobKiNet, a\nkinematics-informed neural network, for end-to-end sampling within the\nContinuous Feasible Set (CFS) under multiple constraints in configuration\nspace, establishing its Optimization Expectation Model. Comparisons with\ntraditional sampling and learning-based approaches reveal that RobKiNet's\nkinematic knowledge infusion enhances training efficiency by ensuring stable\nand accurate gradient optimization.Visualizations and quantitative analyses in\na 2-DOF space validate its theoretical efficiency, while its application on a\n9-DOF autonomous mobile manipulator robot(AMMR) demonstrates superior\nwhole-body and decoupled control, excelling in battery disassembly tasks.\nRobKiNet outperforms deep reinforcement learning with a training speed 74.29\ntimes faster and a sampling accuracy of up to 99.25%, achieving a 97.33% task\ncompletion rate in real-world scenarios.\n","authors":["Yanlong Peng","Zhigang Wang","Ziwen He","Pengxu Chang","Chuangchuang Zhou","Yu Yan","Ming Chen"],"pdf_url":"https://arxiv.org/pdf/2509.07646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07593v1","updated":"2025-09-09T11:05:44Z","published":"2025-09-09T11:05:44Z","title":"Can SSD-Mamba2 Unlock Reinforcement Learning for End-to-End Motion\n  Control?","summary":"  End-to-end reinforcement learning for motion control promises unified\nperception-action policies that scale across embodiments and tasks, yet most\ndeployed controllers are either blind (proprioception-only) or rely on fusion\nbackbones with unfavorable compute-memory trade-offs. Recurrent controllers\nstruggle with long-horizon credit assignment, and Transformer-based fusion\nincurs quadratic cost in token length, limiting temporal and spatial context.\nWe present a vision-driven cross-modal RL framework built on SSD-Mamba2, a\nselective state-space backbone that applies state-space duality (SSD) to enable\nboth recurrent and convolutional scanning with hardware-aware streaming and\nnear-linear scaling. Proprioceptive states and exteroceptive observations\n(e.g., depth tokens) are encoded into compact tokens and fused by stacked\nSSD-Mamba2 layers. The selective state-space updates retain long-range\ndependencies with markedly lower latency and memory use than quadratic\nself-attention, enabling longer look-ahead, higher token resolution, and stable\ntraining under limited compute. Policies are trained end-to-end under curricula\nthat randomize terrain and appearance and progressively increase scene\ncomplexity. A compact, state-centric reward balances task progress, energy\nefficiency, and safety. Across diverse motion-control scenarios, our approach\nconsistently surpasses strong state-of-the-art baselines in return, safety\n(collisions and falls), and sample efficiency, while converging faster at the\nsame compute budget. These results suggest that SSD-Mamba2 provides a practical\nfusion backbone for scalable, foresightful, and efficient end-to-end motion\ncontrol.\n","authors":["Gavin Tao","Yinuo Wang","Jinzhao Zhou"],"pdf_url":"https://arxiv.org/pdf/2509.07593v1.pdf","comment":"4 figures and 6 tables"},{"id":"http://arxiv.org/abs/2509.06469v2","updated":"2025-09-09T10:50:54Z","published":"2025-09-08T09:30:22Z","title":"Interactive Shaping of Granular Media Using Reinforcement Learning","summary":"  Autonomous manipulation of granular media, such as sand, is crucial for\napplications in construction, excavation, and additive manufacturing. However,\nshaping granular materials presents unique challenges due to their\nhigh-dimensional configuration space and complex dynamics, where traditional\nrule-based approaches struggle without extensive engineering efforts.\nReinforcement learning (RL) offers a promising alternative by enabling agents\nto learn adaptive manipulation strategies through trial and error. In this\nwork, we present an RL framework that enables a robotic arm with a cubic\nend-effector and a stereo camera to shape granular media into desired target\nstructures. We show the importance of compact observations and concise reward\nformulations for the large configuration space, validating our design choices\nwith an ablation study. Our results demonstrate the effectiveness of the\nproposed approach for the training of visual policies that manipulate granular\nmedia including their real-world deployment, significantly outperforming two\nbaseline approaches in terms of target shape accuracy.\n","authors":["Benedikt Kreis","Malte Mosbach","Anny Ripke","Muhammad Ehsan Ullah","Sven Behnke","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2509.06469v2.pdf","comment":"Accepted to IEEE-RAS International Conference on Humanoid Robots\n  (Humanoids) 2025"},{"id":"http://arxiv.org/abs/2509.07561v1","updated":"2025-09-09T10:01:15Z","published":"2025-09-09T10:01:15Z","title":"Bio-inspired decision making in swarms under biases from stubborn\n  robots, corrupted communication, and independent discovery","summary":"  Minimalistic robot swarms offer a scalable, robust, and cost-effective\napproach to performing complex tasks with the potential to transform\napplications in healthcare, disaster response, and environmental monitoring.\nHowever, coordinating such decentralised systems remains a fundamental\nchallenge, particularly when robots are constrained in communication,\ncomputation, and memory. In our study, individual robots frequently make errors\nwhen sensing the environment, yet the swarm can rapidly and reliably reach\nconsensus on the best among $n$ discrete options. We compare two canonical\nmechanisms of opinion dynamics -- direct-switch and cross-inhibition -- which\nare simple yet effective rules for collective information processing observed\nin biological systems across scales, from neural populations to insect\ncolonies. We generalise the existing mean-field models by considering asocial\nbiases influencing the opinion dynamics. While swarms using direct-switch\nreliably select the best option in absence of asocial dynamics, their\nperformance deteriorates once such biases are introduced, often resulting in\ndecision deadlocks. In contrast, bio-inspired cross-inhibition enables faster,\nmore cohesive, accurate, robust, and scalable decisions across a wide range of\nbiased conditions. Our findings provide theoretical and practical insights into\nthe coordination of minimal swarms and offer insights that extend to a broad\nclass of decentralised decision-making systems in biology and engineering.\n","authors":["Raina Zakir","Timoteo Carletti","Marco Dorigo","Andreagiovanni Reina"],"pdf_url":"https://arxiv.org/pdf/2509.07561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12363v4","updated":"2025-09-09T09:51:38Z","published":"2025-05-18T10:57:33Z","title":"Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts","summary":"  While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research.\n","authors":["Qi Feng"],"pdf_url":"https://arxiv.org/pdf/2505.12363v4.pdf","comment":"26 pages, 19 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.12312v4","updated":"2025-09-09T09:48:14Z","published":"2025-05-18T08:55:02Z","title":"Visuospatial Cognitive Assistant","summary":"  Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence.\n","authors":["Qi Feng"],"pdf_url":"https://arxiv.org/pdf/2505.12312v4.pdf","comment":"31 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2509.06951v2","updated":"2025-09-09T09:42:16Z","published":"2025-09-08T17:58:30Z","title":"F1: A Vision-Language-Action Model Bridging Understanding and Generation\n  to Actions","summary":"  Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.\n","authors":["Qi Lv","Weijie Kong","Hao Li","Jia Zeng","Zherui Qiu","Delin Qu","Haoming Song","Qizhi Chen","Xiang Deng","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2509.06951v2.pdf","comment":"Homepage: https://aopolin-lv.github.io/F1-VLA/"},{"id":"http://arxiv.org/abs/2410.10803v3","updated":"2025-09-09T09:24:29Z","published":"2024-10-14T17:59:00Z","title":"Generalizable Humanoid Manipulation with 3D Diffusion Policies","summary":"  Humanoid robots capable of autonomous operation in diverse environments have\nlong been a goal for roboticists. However, autonomous manipulation by humanoid\nrobots has largely been restricted to one specific scene, primarily due to the\ndifficulty of acquiring generalizable skills and the expensiveness of\nin-the-wild humanoid robot data. In this work, we build a real-world robotic\nsystem to address this challenging problem. Our system is mainly an integration\nof 1) a whole-upper-body robotic teleoperation system to acquire human-like\nrobot data, 2) a 25-DoF humanoid robot platform with a height-adjustable cart\nand a 3D LiDAR sensor, and 3) an improved 3D Diffusion Policy learning\nalgorithm for humanoid robots to learn from noisy human data. We run more than\n2000 episodes of policy rollouts on the real robot for rigorous policy\nevaluation. Empowered by this system, we show that using only data collected in\none single scene and with only onboard computing, a full-sized humanoid robot\ncan autonomously perform skills in diverse real-world scenarios. Videos are\navailable at https://humanoid-manipulation.github.io .\n","authors":["Yanjie Ze","Zixuan Chen","Wenhao Wang","Tianyi Chen","Xialin He","Ying Yuan","Xue Bin Peng","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2410.10803v3.pdf","comment":"IROS 2025. Project website: https://humanoid-manipulation.github.io"},{"id":"http://arxiv.org/abs/2508.03692v2","updated":"2025-09-09T09:00:35Z","published":"2025-08-05T17:59:56Z","title":"LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences","summary":"  Generative world models have become essential data engines for autonomous\ndriving, yet most existing efforts focus on videos or occupancy grids,\noverlooking the unique LiDAR properties. Extending LiDAR generation to dynamic\n4D world modeling presents challenges in controllability, temporal coherence,\nand evaluation standardization. To this end, we present LiDARCrafter, a unified\nframework for 4D LiDAR generation and editing. Given free-form natural language\ninputs, we parse instructions into ego-centric scene graphs, which condition a\ntri-branch diffusion network to generate object structures, motion\ntrajectories, and geometry. These structured conditions enable diverse and\nfine-grained scene editing. Additionally, an autoregressive module generates\ntemporally coherent 4D LiDAR sequences with smooth transitions. To support\nstandardized evaluation, we establish a comprehensive benchmark with diverse\nmetrics spanning scene-, object-, and sequence-level aspects. Experiments on\nthe nuScenes dataset using this benchmark demonstrate that LiDARCrafter\nachieves state-of-the-art performance in fidelity, controllability, and\ntemporal consistency across all levels, paving the way for data augmentation\nand simulation. The code and benchmark are released to the community.\n","authors":["Ao Liang","Youquan Liu","Yu Yang","Dongyue Lu","Linfeng Li","Lingdong Kong","Huaici Zhao","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2508.03692v2.pdf","comment":"Preprint; 28 pages, 18 figures, 12 tables; Project Page at\n  https://lidarcrafter.github.io"},{"id":"http://arxiv.org/abs/2502.05752v2","updated":"2025-09-09T08:58:33Z","published":"2025-02-09T03:06:19Z","title":"PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based\n  Implicit Neural Map","summary":"  Robots benefit from high-fidelity reconstructions of their environment, which\nshould be geometrically accurate and photorealistic to support downstream\ntasks. While this can be achieved by building distance fields from range\nsensors and radiance fields from cameras, realising scalable incremental\nmapping of both fields consistently and at the same time with high quality is\nchallenging. In this paper, we propose a novel map representation that unifies\na continuous signed distance field and a Gaussian splatting radiance field\nwithin an elastic and compact point-based implicit neural map. By enforcing\ngeometric consistency between these fields, we achieve mutual improvements by\nexploiting both modalities. We present a novel LiDAR-visual SLAM system called\nPINGS using the proposed map representation and evaluate it on several\nchallenging large-scale datasets. Experimental results demonstrate that PINGS\ncan incrementally build globally consistent distance and radiance fields\nencoded with a compact set of neural points. Compared to state-of-the-art\nmethods, PINGS achieves superior photometric and geometric rendering at novel\nviews by constraining the radiance field with the distance field. Furthermore,\nby utilizing dense photometric cues and multi-view consistency from the\nradiance field, PINGS produces more accurate distance fields, leading to\nimproved odometry estimation and mesh reconstruction. We also provide an\nopen-source implementation of PING at: https://github.com/PRBonn/PINGS.\n","authors":["Yue Pan","Xingguang Zhong","Liren Jin","Louis Wiesmann","Marija Popović","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2502.05752v2.pdf","comment":"15 pages, 8 figures, presented at RSS 2025"},{"id":"http://arxiv.org/abs/2509.07500v1","updated":"2025-09-09T08:26:11Z","published":"2025-09-09T08:26:11Z","title":"OmniMap: A General Mapping Framework Integrating Optics, Geometry, and\n  Semantics","summary":"  Robotic systems demand accurate and comprehensive 3D environment perception,\nrequiring simultaneous capture of photo-realistic appearance (optical), precise\nlayout shape (geometric), and open-vocabulary scene understanding (semantic).\nExisting methods typically achieve only partial fulfillment of these\nrequirements while exhibiting optical blurring, geometric irregularities, and\nsemantic ambiguities. To address these challenges, we propose OmniMap. Overall,\nOmniMap represents the first online mapping framework that simultaneously\ncaptures optical, geometric, and semantic scene attributes while maintaining\nreal-time performance and model compactness. At the architectural level,\nOmniMap employs a tightly coupled 3DGS-Voxel hybrid representation that\ncombines fine-grained modeling with structural stability. At the implementation\nlevel, OmniMap identifies key challenges across different modalities and\nintroduces several innovations: adaptive camera modeling for motion blur and\nexposure compensation, hybrid incremental representation with normal\nconstraints, and probabilistic fusion for robust instance-level understanding.\nExtensive experiments show OmniMap's superior performance in rendering\nfidelity, geometric accuracy, and zero-shot semantic segmentation compared to\nstate-of-the-art methods across diverse scenes. The framework's versatility is\nfurther evidenced through a variety of downstream applications, including\nmulti-domain scene Q&A, interactive editing, perception-guided manipulation,\nand map-assisted navigation.\n","authors":["Yinan Deng","Yufeng Yue","Jianyu Dou","Jingyu Zhao","Jiahui Wang","Yujie Tang","Yi Yang","Mengyin Fu"],"pdf_url":"https://arxiv.org/pdf/2509.07500v1.pdf","comment":"Accepted by IEEE Transactions on Robotics (TRO), project website:\n  https://omni-map.github.io/"},{"id":"http://arxiv.org/abs/2509.07496v1","updated":"2025-09-09T08:22:10Z","published":"2025-09-09T08:22:10Z","title":"Flexible Morphing Aerial Robot with Inflatable Structure for\n  Perching-based Human-Robot Interaction","summary":"  Birds in nature perform perching not only for rest but also for interaction\nwith human such as the relationship with falconers. Recently, researchers\nachieve perching-capable aerial robots as a way to save energy, and deformable\nstructure demonstrate significant advantages in efficiency of perching and\ncompactness of configuration. However, ensuring flight stability remains\nchallenging for deformable aerial robots due to the difficulty of controlling\nflexible arms. Furthermore, perching for human interaction requires high\ncompliance along with safety. Thus, this study aims to develop a deformable\naerial robot capable of perching on humans with high flexibility and grasping\nability. To overcome the challenges of stability of both flight and perching,\nwe propose a hybrid morphing structure that combines a unilateral flexible arm\nand a pneumatic inflatable actuators. This design allows the robot's arms to\nremain rigid during flight and soft while perching for more effective grasping.\nWe also develop a pneumatic control system that optimizes pressure regulation\nwhile integrating shock absorption and adjustable grasping forces, enhancing\ninteraction capabilities and energy efficiency. Besides, we focus on the\nstructural characteristics of the unilateral flexible arm and identify\nsufficient conditions under which standard quadrotor modeling and control\nremain effective in terms of flight stability. Finally, the developed prototype\ndemonstrates the feasibility of compliant perching maneuvers on humans, as well\nas the robust recovery even after arm deformation caused by thrust reductions\nduring flight. To the best of our knowledge, this work is the first to achieve\nan aerial robot capable of perching on humans for interaction.\n","authors":["Ayano Miyamichi","Moju Zhao","Kazuki Sugihara","Junichiro Sugihara","Masanori Konishi","Kunio Kojima","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2509.07496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07464v1","updated":"2025-09-09T07:43:10Z","published":"2025-09-09T07:43:10Z","title":"Safe and Non-Conservative Contingency Planning for Autonomous Vehicles\n  via Online Learning-Based Reachable Set Barriers","summary":"  Autonomous vehicles must navigate dynamically uncertain environments while\nbalancing the safety and driving efficiency. This challenge is exacerbated by\nthe unpredictable nature of surrounding human-driven vehicles (HVs) and\nperception inaccuracies, which require planners to adapt to evolving\nuncertainties while maintaining safe trajectories. Overly conservative planners\ndegrade driving efficiency, while deterministic approaches may encounter\nserious issues and risks of failure when faced with sudden and unexpected\nmaneuvers. To address these issues, we propose a real-time contingency\ntrajectory optimization framework in this paper. By employing event-triggered\nonline learning of HV control-intent sets, our method dynamically quantifies\nmulti-modal HV uncertainties and refines the forward reachable set (FRS)\nincrementally. Crucially, we enforce invariant safety through FRS-based barrier\nconstraints that ensure safety without reliance on accurate trajectory\nprediction of HVs. These constraints are embedded in contingency trajectory\noptimization and solved efficiently through consensus alternative direction\nmethod of multipliers (ADMM). The system continuously adapts to the\nuncertainties in HV behaviors, preserving feasibility and safety without\nresorting to excessive conservatism. High-fidelity simulations on highway and\nurban scenarios, as well as a series of real-world experiments demonstrate\nsignificant improvements in driving efficiency and passenger comfort while\nmaintaining safety under uncertainty. The project page is available at\nhttps://pathetiue.github.io/frscp.github.io/.\n","authors":["Rui Yang","Lei Zheng","Shuzhi Sam Ge","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2509.07464v1.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2509.07463v1","updated":"2025-09-09T07:42:07Z","published":"2025-09-09T07:42:07Z","title":"DepthVision: Robust Vision-Language Understanding through GAN-Based\n  LiDAR-to-RGB Synthesis","summary":"  Ensuring reliable robot operation when visual input is degraded or\ninsufficient remains a central challenge in robotics. This letter introduces\nDepthVision, a framework for multimodal scene understanding designed to address\nthis problem. Unlike existing Vision-Language Models (VLMs), which use only\ncamera-based visual input alongside language, DepthVision synthesizes RGB\nimages from sparse LiDAR point clouds using a conditional generative\nadversarial network (GAN) with an integrated refiner network. These synthetic\nviews are then combined with real RGB data using a Luminance-Aware Modality\nAdaptation (LAMA), which blends the two types of data dynamically based on\nambient lighting conditions. This approach compensates for sensor degradation,\nsuch as darkness or motion blur, without requiring any fine-tuning of\ndownstream vision-language models. We evaluate DepthVision on real and\nsimulated datasets across various models and tasks, with particular attention\nto safety-critical tasks. The results demonstrate that our approach improves\nperformance in low-light conditions, achieving substantial gains over RGB-only\nbaselines while preserving compatibility with frozen VLMs. This work highlights\nthe potential of LiDAR-guided RGB synthesis for achieving robust robot\noperation in real-world environments.\n","authors":["Sven Kirchner","Nils Purschke","Ross Greer","Alois C. Knoll"],"pdf_url":"https://arxiv.org/pdf/2509.07463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07445v1","updated":"2025-09-09T07:10:39Z","published":"2025-09-09T07:10:39Z","title":"Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward\n  Functions","summary":"  Large language models (LLMs) are beginning to automate reward design for\ndexterous manipulation. However, no prior work has considered tactile sensing,\nwhich is known to be critical for human-like dexterity. We present Text2Touch,\nbringing LLM-crafted rewards to the challenging task of multi-axis in-hand\nobject rotation with real-world vision based tactile sensing in palm-up and\npalm-down configurations. Our prompt engineering strategy scales to over 70\nenvironment variables, and sim-to-real distillation enables successful policy\ntransfer to a tactile-enabled fully actuated four-fingered dexterous robot\nhand. Text2Touch significantly outperforms a carefully tuned human-engineered\nbaseline, demonstrating superior rotation speed and stability while relying on\nreward functions that are an order of magnitude shorter and simpler. These\nresults illustrate how LLM-designed rewards can significantly reduce the time\nfrom concept to deployable dexterous tactile skills, supporting more rapid and\nscalable multimodal robot learning. Project website:\nhttps://hpfield.github.io/text2touch-website\n","authors":["Harrison Field","Max Yang","Yijiong Lin","Efi Psomopoulou","David Barton","Nathan F. Lepora"],"pdf_url":"https://arxiv.org/pdf/2509.07445v1.pdf","comment":"Accepted at CoRL 2025"},{"id":"http://arxiv.org/abs/2509.07438v1","updated":"2025-09-09T06:53:48Z","published":"2025-09-09T06:53:48Z","title":"Timing the Message: Language-Based Notifications for Time-Critical\n  Assistive Settings","summary":"  In time-critical settings such as assistive driving, assistants often rely on\nalerts or haptic signals to prompt rapid human attention, but these cues\nusually leave humans to interpret situations and decide responses\nindependently, introducing potential delays or ambiguity in meaning.\nLanguage-based assistive systems can instead provide instructions backed by\ncontext, offering more informative guidance. However, current approaches (e.g.,\nsocial assistive robots) largely prioritize content generation while\noverlooking critical timing factors such as verbal conveyance duration, human\ncomprehension delays, and subsequent follow-through duration. These timing\nconsiderations are crucial in time-critical settings, where even minor delays\ncan substantially affect outcomes. We aim to study this inherent trade-off\nbetween timeliness and informativeness by framing the challenge as a sequential\ndecision-making problem using an augmented-state Markov Decision Process. We\ndesign a framework combining reinforcement learning and a generated offline\ntaxonomy dataset, where we balance the trade-off while enabling a scalable\ntaxonomy dataset generation pipeline. Empirical evaluation with synthetic\nhumans shows our framework improves success rates by over 40% compared to\nmethods that ignore time delays, while effectively balancing timeliness and\ninformativeness. It also exposes an often-overlooked trade-off between these\ntwo factors, opening new directions for optimizing communication in\ntime-critical human-AI assistance.\n","authors":["Ya-Chuan Hsu","Jonathan DeCastro","Andrew Silva","Guy Rosman"],"pdf_url":"https://arxiv.org/pdf/2509.07438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.22626v2","updated":"2025-09-09T06:38:19Z","published":"2025-05-28T17:45:05Z","title":"SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale\n  Imitation Learning","summary":"  Imitation learning advances robot capabilities by enabling the acquisition of\ndiverse behaviors from human demonstrations. However, large-scale datasets used\nfor policy training often introduce substantial variability in quality, which\ncan negatively impact performance. As a result, automatically curating datasets\nby filtering low-quality samples to improve quality becomes essential. Existing\nrobotic curation approaches rely on costly manual annotations and perform\ncuration at a coarse granularity, such as the dataset or trajectory level,\nfailing to account for the quality of individual state-action pairs. To address\nthis, we introduce SCIZOR, a self-supervised data curation framework that\nfilters out low-quality state-action pairs to improve the performance of\nimitation learning policies. SCIZOR targets two complementary sources of\nlow-quality data: suboptimal data, which hinders learning with undesirable\nactions, and redundant data, which dilutes training with repetitive patterns.\nSCIZOR leverages a self-supervised task progress predictor for suboptimal data\nto remove samples lacking task progression, and a deduplication module\noperating on joint state-action representation for samples with redundant\npatterns. Empirically, we show that SCIZOR enables imitation learning policies\nto achieve higher performance with less data, yielding an average improvement\nof 15.4% across multiple benchmarks. More information is available at:\nhttps://ut-austin-rpl.github.io/SCIZOR/\n","authors":["Yu Zhang","Yuqi Xie","Huihan Liu","Rutav Shah","Michael Wan","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.22626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.07084v3","updated":"2025-09-09T06:37:28Z","published":"2025-05-11T18:14:33Z","title":"DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language\n  Models","summary":"  Human drivers possess spatial and causal intelligence, enabling them to\nperceive driving scenarios, anticipate hazards, and react to dynamic\nenvironments. In contrast, autonomous vehicles lack these abilities, making it\nchallenging to manage perception-related Safety of the Intended Functionality\n(SOTIF) risks, especially under complex or unpredictable driving conditions. To\naddress this gap, we propose fine-tuning multimodal large language models\n(MLLMs) on a customized dataset specifically designed to capture\nperception-related SOTIF scenarios. Benchmarking results show that fine-tuned\nMLLMs achieve an 11.8\\% improvement in close-ended VQA accuracy and a 12.0\\%\nincrease in open-ended VQA scores compared to baseline models, while\nmaintaining real-time performance with a 0.59-second average inference time per\nimage. We validate our approach through real-world case studies in Canada and\nChina, where fine-tuned models correctly identify safety risks that challenge\neven experienced human drivers. This work represents the first application of\ndomain-specific MLLM fine-tuning for SOTIF domain in autonomous driving. The\ndataset and related resources are available at\ngithub.com/s95huang/DriveSOTIF.git\n","authors":["Shucheng Huang","Freda Shi","Chen Sun","Jiaming Zhong","Minghao Ning","Yufeng Yang","Yukun Lu","Hong Wang","Amir Khajepour"],"pdf_url":"https://arxiv.org/pdf/2505.07084v3.pdf","comment":"This work has been accepted to IEEE Transactions on Vehicular\n  Technology. Please refer to the copyright notice for additional information"},{"id":"http://arxiv.org/abs/2509.07413v1","updated":"2025-09-09T05:51:13Z","published":"2025-09-09T05:51:13Z","title":"Robust Docking Maneuvers for Autonomous Trolley Collection: An\n  Optimization-Based Visual Servoing Scheme","summary":"  Service robots have demonstrated significant potential for autonomous trolley\ncollection and redistribution in public spaces like airports or warehouses to\nimprove efficiency and reduce cost. Usually, a fully autonomous system for the\ncollection and transportation of multiple trolleys is based on a\nLeader-Follower formation of mobile manipulators, where reliable docking\nmaneuvers of the mobile base are essential to align trolleys into organized\nqueues. However, developing a vision-based robotic docking system faces\nsignificant challenges: high precision requirements, environmental\ndisturbances, and inherent robot constraints. To address these challenges, we\npropose an optimization-based Visual Servoing scheme that incorporates active\ninfrared markers for robust feature extraction across diverse lighting\nconditions. This framework explicitly models nonholonomic kinematics and\nvisibility constraints within the Hybrid Visual Servoing problem, augmented\nwith an observer for disturbance rejection to ensure precise and stable\ndocking. Experimental results across diverse environments demonstrate the\nrobustness of this system, with quantitative evaluations confirming high\ndocking accuracy.\n","authors":["Yuhan Pang","Bingyi Xia","Zhe Zhang","Zhirui Sun","Peijia Xie","Bike Zhu","Wenjun Xu","Jiankun Wang"],"pdf_url":"https://arxiv.org/pdf/2509.07413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07412v1","updated":"2025-09-09T05:49:46Z","published":"2025-09-09T05:49:46Z","title":"Attention and Risk-Aware Decision Framework for Safe Autonomous Driving","summary":"  Autonomous driving has attracted great interest due to its potential\ncapability in full-unsupervised driving. Model-based and learning-based methods\nare widely used in autonomous driving. Model-based methods rely on pre-defined\nmodels of the environment and may struggle with unforeseen events. Proximal\npolicy optimization (PPO), an advanced learning-based method, can adapt to the\nabove limits by learning from interactions with the environment. However,\nexisting PPO faces challenges with poor training results, and low training\nefficiency in long sequences. Moreover, the poor training results are\nequivalent to collisions in driving tasks. To solve these issues, this paper\ndevelops an improved PPO by introducing the risk-aware mechanism, a\nrisk-attention decision network, a balanced reward function, and a\nsafety-assisted mechanism. The risk-aware mechanism focuses on highlighting\nareas with potential collisions, facilitating safe-driving learning of the PPO.\nThe balanced reward function adjusts rewards based on the number of surrounding\nvehicles, promoting efficient exploration of the control strategy during\ntraining. Additionally, the risk-attention network enhances the PPO to hold\nchannel and spatial attention for the high-risk areas of input images.\nMoreover, the safety-assisted mechanism supervises and prevents the actions\nwith risks of collisions during the lane keeping and lane changing. Simulation\nresults on a physical engine demonstrate that the proposed algorithm\noutperforms benchmark algorithms in collision avoidance, achieving higher peak\nreward with less training time, and shorter driving time remaining on the risky\nareas among multiple testing traffic flow scenarios.\n","authors":["Zhen Tian","Fujiang Yuan","Yangfan He","Qinghao Li","Changlin Chen","Huilin Chen","Tianxiang Xu","Jianyu Duan","Yanhong Peng","Zhihao Lin"],"pdf_url":"https://arxiv.org/pdf/2509.07412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07411v1","updated":"2025-09-09T05:49:36Z","published":"2025-09-09T05:49:36Z","title":"Adaptive Evolutionary Framework for Safe, Efficient, and Cooperative\n  Autonomous Vehicle Interactions","summary":"  Modern transportation systems face significant challenges in ensuring road\nsafety, given serious injuries caused by road accidents. The rapid growth of\nautonomous vehicles (AVs) has prompted new traffic designs that aim to optimize\ninteractions among AVs. However, effective interactions between AVs remains\nchallenging due to the absence of centralized control. Besides, there is a need\nfor balancing multiple factors, including passenger demands and overall traffic\nefficiency. Traditional rule-based, optimization-based, and game-theoretic\napproaches each have limitations in addressing these challenges. Rule-based\nmethods struggle with adaptability and generalization in complex scenarios,\nwhile optimization-based methods often require high computational resources.\nGame-theoretic approaches, such as Stackelberg and Nash games, suffer from\nlimited adaptability and potential inefficiencies in cooperative settings. This\npaper proposes an Evolutionary Game Theory (EGT)-based framework for AV\ninteractions that overcomes these limitations by utilizing a decentralized and\nadaptive strategy evolution mechanism. A causal evaluation module (CEGT) is\nintroduced to optimize the evolutionary rate, balancing mutation and evolution\nby learning from historical interactions. Simulation results demonstrate the\nproposed CEGT outperforms EGT and popular benchmark games in terms of lower\ncollision rates, improved safety distances, higher speeds, and overall better\nperformance compared to Nash and Stackelberg games across diverse scenarios and\nparameter settings.\n","authors":["Zhen Tian","Zhihao Lin"],"pdf_url":"https://arxiv.org/pdf/2509.07411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07381v1","updated":"2025-09-09T04:14:42Z","published":"2025-09-09T04:14:42Z","title":"TransMPC: Transformer-based Explicit MPC with Variable Prediction\n  Horizon","summary":"  Traditional online Model Predictive Control (MPC) methods often suffer from\nexcessive computational complexity, limiting their practical deployment.\nExplicit MPC mitigates online computational load by pre-computing control\npolicies offline; however, existing explicit MPC methods typically rely on\nsimplified system dynamics and cost functions, restricting their accuracy for\ncomplex systems. This paper proposes TransMPC, a novel Transformer-based\nexplicit MPC algorithm capable of generating highly accurate control sequences\nin real-time for complex dynamic systems. Specifically, we formulate the MPC\npolicy as an encoder-only Transformer leveraging bidirectional self-attention,\nenabling simultaneous inference of entire control sequences in a single forward\npass. This design inherently accommodates variable prediction horizons while\nensuring low inference latency. Furthermore, we introduce a direct policy\noptimization framework that alternates between sampling and learning phases.\nUnlike imitation-based approaches dependent on precomputed optimal\ntrajectories, TransMPC directly optimizes the true finite-horizon cost via\nautomatic differentiation. Random horizon sampling combined with a replay\nbuffer provides independent and identically distributed (i.i.d.) training\nsamples, ensuring robust generalization across varying states and horizon\nlengths. Extensive simulations and real-world vehicle control experiments\nvalidate the effectiveness of TransMPC in terms of solution accuracy,\nadaptability to varying horizons, and computational efficiency.\n","authors":["Sichao Wu","Jiang Wu","Xingyu Cao","Fawang Zhang","Guangyuan Yu","Junjie Zhao","Yue Qu","Fei Ma","Jingliang Duan"],"pdf_url":"https://arxiv.org/pdf/2509.07381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12520v2","updated":"2025-09-09T03:19:05Z","published":"2024-11-19T14:07:17Z","title":"VMGNet: A Low Computational Complexity Robotic Grasping Network Based on\n  VMamba with Multi-Scale Feature Fusion","summary":"  While deep learning-based robotic grasping technology has demonstrated strong\nadaptability, its computational complexity has also significantly increased,\nmaking it unsuitable for scenarios with high real-time requirements. Therefore,\nwe propose a low computational complexity and high accuracy model named VMGNet\nfor robotic grasping. For the first time, we introduce the Visual State Space\ninto the robotic grasping field to achieve linear computational complexity,\nthereby greatly reducing the model's computational cost. Meanwhile, to improve\nthe accuracy of the model, we propose an efficient and lightweight multi-scale\nfeature fusion module, named Fusion Bridge Module, to extract and fuse\ninformation at different scales. We also present a new loss function\ncalculation method to enhance the importance differences between subtasks,\nimproving the model's fitting ability. Experiments show that VMGNet has only\n8.7G Floating Point Operations and an inference time of 8.1 ms on our devices.\nVMGNet also achieved state-of-the-art performance on the Cornell and Jacquard\npublic datasets. To validate VMGNet's effectiveness in practical applications,\nwe conducted real grasping experiments in multi-object scenarios, and VMGNet\nachieved an excellent performance with a 94.4% success rate in real-world\ngrasping tasks. The video for the real-world robotic grasping experiments is\navailable at https://youtu.be/S-QHBtbmLc4.\n","authors":["Yuhao Jin","Qizhong Gao","Xiaohui Zhu","Yong Yue","Eng Gee Lim","Yuqing Chen","Prudence Wong","Yijie Chu"],"pdf_url":"https://arxiv.org/pdf/2411.12520v2.pdf","comment":"This work is part of ongoing research, and we are further developing\n  new techniques based on these results. To avoid premature disclosure of\n  incomplete content, we request withdrawal of the current version and will\n  resubmit once the study is more complete"},{"id":"http://arxiv.org/abs/2509.07362v1","updated":"2025-09-09T03:17:04Z","published":"2025-09-09T03:17:04Z","title":"Aerial-ground Cross-modal Localization: Dataset, Ground-truth, and\n  Benchmark","summary":"  Accurate visual localization in dense urban environments poses a fundamental\ntask in photogrammetry, geospatial information science, and robotics. While\nimagery is a low-cost and widely accessible sensing modality, its effectiveness\non visual odometry is often limited by textureless surfaces, severe viewpoint\nchanges, and long-term drift. The growing public availability of airborne laser\nscanning (ALS) data opens new avenues for scalable and precise visual\nlocalization by leveraging ALS as a prior map. However, the potential of\nALS-based localization remains underexplored due to three key limitations: (1)\nthe lack of platform-diverse datasets, (2) the absence of reliable ground-truth\ngeneration methods applicable to large-scale urban environments, and (3)\nlimited validation of existing Image-to-Point Cloud (I2P) algorithms under\naerial-ground cross-platform settings. To overcome these challenges, we\nintroduce a new large-scale dataset that integrates ground-level imagery from\nmobile mapping systems with ALS point clouds collected in Wuhan, Hong Kong, and\nSan Francisco.\n","authors":["Yandi Yang","Jianping Li","Youqi Liao","Yuhao Li","Yizhe Zhang","Zhen Dong","Bisheng Yang","Naser El-Sheimy"],"pdf_url":"https://arxiv.org/pdf/2509.07362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19713v3","updated":"2025-09-09T02:42:27Z","published":"2025-03-25T14:39:04Z","title":"Semi-SMD: Semi-Supervised Metric Depth Estimation via Surrounding\n  Cameras for Autonomous Driving","summary":"  In this paper, we introduce Semi-SMD, a novel metric depth estimation\nframework tailored for surrounding cameras equipment in autonomous driving. In\nthis work, the input data consists of adjacent surrounding frames and camera\nparameters. We propose a unified spatial-temporal-semantic fusion module to\nconstruct the visual fused features. Cross-attention components for surrounding\ncameras and adjacent frames are utilized to focus on metric scale information\nrefinement and temporal feature matching. Building on this, we propose a pose\nestimation framework using surrounding cameras, their corresponding estimated\ndepths, and extrinsic parameters, which effectively address the scale ambiguity\nin multi-camera setups. Moreover, semantic world model and monocular depth\nestimation world model are integrated to supervised the depth estimation, which\nimprove the quality of depth estimation. We evaluate our algorithm on DDAD and\nnuScenes datasets, and the results demonstrate that our method achieves\nstate-of-the-art performance in terms of surrounding camera based depth\nestimation quality. The source code will be available on\nhttps://github.com/xieyuser/Semi-SMD.\n","authors":["Yusen Xie","Zhengmin Huang","Shaojie Shen","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2503.19713v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05666v3","updated":"2025-09-09T02:17:46Z","published":"2024-03-08T20:43:57Z","title":"Prepared for the Worst: A Learning-Based Adversarial Attack for\n  Resilience Analysis of the ICP Algorithm","summary":"  This paper presents a novel method for assessing the resilience of the ICP\nalgorithm via learning-based, worst-case attacks on lidar point clouds. For\nsafety-critical applications such as autonomous navigation, ensuring the\nresilience of algorithms before deployments is crucial. The ICP algorithm is\nthe standard for lidar-based localization, but its accuracy can be greatly\naffected by corrupted measurements from various sources, including occlusions,\nadverse weather, or mechanical sensor issues. Unfortunately, the complex and\niterative nature of ICP makes assessing its resilience to corruption\nchallenging. While there have been efforts to create challenging datasets and\ndevelop simulations to evaluate the resilience of ICP, our method focuses on\nfinding the maximum possible ICP error that can arise from corrupted\nmeasurements at a location. We demonstrate that our perturbation-based\nadversarial attacks can be used pre-deployment to identify locations on a map\nwhere ICP is particularly vulnerable to corruptions in the measurements. With\nsuch information, autonomous robots can take safer paths when deployed, to\nmitigate against their measurements being corrupted. The proposed attack\noutperforms baselines more than 88% of the time across a wide range of\nscenarios.\n","authors":["Ziyu Zhang","Johann Laconte","Daniil Lisus","Timothy D. Barfoot"],"pdf_url":"https://arxiv.org/pdf/2403.05666v3.pdf","comment":"9 pages (6 content, 1 reference, 2 appendix). 7 figures, accepted to\n  2025 IEEE International Conference on Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2509.08017v1","updated":"2025-09-09T01:50:08Z","published":"2025-09-09T01:50:08Z","title":"PySensors 2.0: A Python Package for Sparse Sensor Placement","summary":"  PySensors is a Python package for selecting and placing a sparse set of\nsensors for reconstruction and classification tasks. In this major update to\n\\texttt{PySensors}, we introduce spatially constrained sensor placement\ncapabilities, allowing users to enforce constraints such as maximum or exact\nsensor counts in specific regions, incorporate predetermined sensor locations,\nand maintain minimum distances between sensors. We extend functionality to\nsupport custom basis inputs, enabling integration of any data-driven or\nspectral basis. We also propose a thermodynamic approach that goes beyond a\nsingle ``optimal'' sensor configuration and maps the complete landscape of\nsensor interactions induced by the training data. This comprehensive view\nfacilitates integration with external selection criteria and enables assessment\nof sensor replacement impacts. The new optimization technique also accounts for\nover- and under-sampling of sensors, utilizing a regularized least squares\napproach for robust reconstruction. Additionally, we incorporate noise-induced\nuncertainty quantification of the estimation error and provide visual\nuncertainty heat maps to guide deployment decisions. To highlight these\nadditions, we provide a brief description of the mathematical algorithms and\ntheory underlying these new capabilities. We demonstrate the usage of new\nfeatures with illustrative code examples and include practical advice for\nimplementation across various application domains. Finally, we outline a\nroadmap of potential extensions to further enhance the package's functionality\nand applicability to emerging sensing challenges.\n","authors":["Niharika Karnik","Yash Bhangale","Mohammad G. Abdo","Andrei A. Klishin","Joshua J. Cogliati","Bingni W. Brunton","J. Nathan Kutz","Steven L. Brunton","Krithika Manohar"],"pdf_url":"https://arxiv.org/pdf/2509.08017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07321v1","updated":"2025-09-09T01:40:31Z","published":"2025-09-09T01:40:31Z","title":"Performance Characterization of a Point-Cloud-Based Path Planner in\n  Off-Road Terrain","summary":"  We present a comprehensive evaluation of a point-cloud-based navigation\nstack, MUONS, for autonomous off-road navigation. Performance is characterized\nby analyzing the results of 30,000 planning and navigation trials in simulation\nand validated through field testing. Our simulation campaign considers three\nkinematically challenging terrain maps and twenty combinations of seven\npath-planning parameters. In simulation, our MUONS-equipped AGV achieved a 0.98\nsuccess rate and experienced no failures in the field. By statistical and\ncorrelation analysis we determined that the Bi-RRT expansion radius used in the\ninitial planning stages is most correlated with performance in terms of\nplanning time and traversed path length. Finally, we observed that the\nproportional variation due to changes in the tuning parameters is remarkably\nwell correlated to performance in field testing. This finding supports the use\nof Monte-Carlo simulation campaigns for performance assessment and parameter\ntuning.\n","authors":["Casey D. Majhor","Jeremy P. Bos"],"pdf_url":"https://arxiv.org/pdf/2509.07321v1.pdf","comment":"This work has been published in the Journal of Field Robotics"}]},"2025-09-08T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2509.07239v1","updated":"2025-09-08T21:31:50Z","published":"2025-09-08T21:31:50Z","title":"Safe Gap-based Planning in Dynamic Settings","summary":"  This chapter extends the family of perception-informed gap-based local\nplanners to dynamic environments. Existing perception-informed local planners\nthat operate in dynamic environments often rely on emergent or empirical\nrobustness for collision avoidance as opposed to performing formal analysis of\ndynamic obstacles. This proposed planner, dynamic gap, explicitly addresses\ndynamic obstacles through several steps in the planning pipeline. First, polar\nregions of free space known as gaps are tracked and their dynamics are\nestimated in order to understand how the local environment evolves over time.\nThen, at planning time, gaps are propagated into the future through novel gap\npropagation algorithms to understand what regions are feasible for passage.\nLastly, pursuit guidance theory is leveraged to generate local trajectories\nthat are provably collision-free under ideal conditions. Additionally,\nobstacle-centric ungap processing is performed in situations where no gaps\nexist to robustify the overall planning framework. A set of gap-based planners\nare benchmarked against a series of classical and learned motion planners in\ndynamic environments, and dynamic gap is shown to outperform all other\nbaselines in all environments. Furthermore, dynamic gap is deployed on a\nTurtleBot2 platform in several real-world experiments to validate collision\navoidance behaviors.\n","authors":["Max Asselmeier","Abdel Zaro","Dhruv Ahuja","Ye Zhao","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2509.07239v1.pdf","comment":"Accepted to Algorithms for Machine Vision in Navigation and Control -\n  Springer Publishing House"},{"id":"http://arxiv.org/abs/2509.07234v1","updated":"2025-09-08T21:21:27Z","published":"2025-09-08T21:21:27Z","title":"Efficient Multi-Agent Coordination via Dynamic Joint-State Graph\n  Construction","summary":"  Multi-agent pathfinding (MAPF) traditionally focuses on collision avoidance,\nbut many real-world applications require active coordination between agents to\nimprove team performance. This paper introduces Team Coordination on Graphs\nwith Risky Edges (TCGRE), where agents collaborate to reduce traversal costs on\nhigh-risk edges via support from teammates. We reformulate TCGRE as a 3D\nmatching problem-mapping robot pairs, support pairs, and time steps-and\nrigorously prove its NP-hardness via reduction from Minimum 3D Matching. To\naddress this complexity, (in the conference version) we proposed efficient\ndecomposition methods, reducing the problem to tractable subproblems:\nJoint-State Graph (JSG): Encodes coordination as a single-agent shortest-path\nproblem. Coordination-Exhaustive Search (CES): Optimizes support assignments\nvia exhaustive pairing. Receding-Horizon Optimistic Cooperative A* (RHOCA*):\nBalances optimality and scalability via horizon-limited planning. Further in\nthis extension, we introduce a dynamic graph construction method\n(Dynamic-HJSG), leveraging agent homogeneity to prune redundant states and\nreduce computational overhead by constructing the joint-state graph\ndynamically. Theoretical analysis shows Dynamic-HJSG preserves optimality while\nlowering complexity from exponential to polynomial in key cases. Empirical\nresults validate scalability for large teams and graphs, with HJSG\noutperforming baselines greatly in runtime in different sizes and types of\ngraphs. This work bridges combinatorial optimization and multi-agent planning,\noffering a principled framework for collaborative pathfinding with provable\nguarantees, and the key idea of the solution can be widely extended to many\nother collaborative optimization problems, such as MAPF.\n","authors":["Yanlin Zhou","Manshi Limbu","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2509.07234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07216v1","updated":"2025-09-08T20:52:59Z","published":"2025-09-08T20:52:59Z","title":"Quantum Machine Learning and Grover's Algorithm for Quantum Optimization\n  of Robotic Manipulators","summary":"  Optimizing high-degree of freedom robotic manipulators requires searching\ncomplex, high-dimensional configuration spaces, a task that is computationally\nchallenging for classical methods. This paper introduces a quantum native\nframework that integrates quantum machine learning with Grover's algorithm to\nsolve kinematic optimization problems efficiently. A parameterized quantum\ncircuit is trained to approximate the forward kinematics model, which then\nconstructs an oracle to identify optimal configurations. Grover's algorithm\nleverages this oracle to provide a quadratic reduction in search complexity.\nDemonstrated on 1-DoF, 2-DoF, and dual-arm manipulator tasks, the method\nachieves significant speedups-up to 93x over classical optimizers like Nelder\nMead as problem dimensionality increases. This work establishes a foundational,\nquantum-native framework for robot kinematic optimization, effectively bridging\nquantum computing and robotics problems.\n","authors":["Hassen Nigatu","Shi Gaokun","Li Jituo","Wang Jin","Lu Guodong","Howard Li"],"pdf_url":"https://arxiv.org/pdf/2509.07216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.07162v1","updated":"2025-09-08T19:21:42Z","published":"2025-09-08T19:21:42Z","title":"First Plan Then Evaluate: Use a Vectorized Motion Planner for Grasping","summary":"  Autonomous multi-finger grasping is a fundamental capability in robotic\nmanipulation. Optimization-based approaches show strong performance, but tend\nto be sensitive to initialization and are potentially time-consuming. As an\nalternative, the generator-evaluator-planner framework has been proposed. A\ngenerator generates grasp candidates, an evaluator ranks the proposed grasps,\nand a motion planner plans a trajectory to the highest-ranked grasp. If the\nplanner doesn't find a trajectory, a new trajectory optimization is started\nwith the next-best grasp as the target and so on. However, executing\nlower-ranked grasps means a lower chance of grasp success, and multiple\ntrajectory optimizations are time-consuming. Alternatively, relaxing the\nthreshold for motion planning accuracy allows for easier computation of a\nsuccessful trajectory but implies lower accuracy in estimating grasp success\nlikelihood. It's a lose-lose proposition: either spend more time finding a\nsuccessful trajectory or have a worse estimate of grasp success. We propose a\nframework that plans trajectories to a set of generated grasp targets in\nparallel, the evaluator estimates the grasp success likelihood of the resulting\ntrajectories, and the robot executes the trajectory most likely to succeed. To\nplan trajectories to different targets efficiently, we propose the use of a\nvectorized motion planner. Our experiments show our approach improves over the\ntraditional generator-evaluator-planner framework across different objects,\ngenerators, and motion planners, and successfully generalizes to novel\nenvironments in the real world, including different shelves and table heights.\nProject website https://sites.google.com/view/fpte\n","authors":["Martin Matak","Mohanraj Devendran Ashanti","Karl Van Wyk","Tucker Hermans"],"pdf_url":"https://arxiv.org/pdf/2509.07162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16013v2","updated":"2025-09-08T18:54:41Z","published":"2025-03-20T10:32:38Z","title":"GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping\n  under Flexible Language Instructions","summary":"  Flexible instruction-guided 6-DoF grasping is a significant yet challenging\ntask for real-world robotic systems. Existing methods utilize the contextual\nunderstanding capabilities of the large language models (LLMs) to establish\nmappings between expressions and targets, allowing robots to comprehend users'\nintentions in the instructions. However, the LLM's knowledge about objects'\nphysical properties remains underexplored despite its tight relevance to\ngrasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework\nthat integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to\nphysical properties, guided by auxiliary question-answering (QA) tasks.\nParticularly, we design a set of QA templates to enable hierarchical reasoning\nthat includes three stages: target parsing, physical property analysis, and\ngrasp action selection. Moreover, GraspCoT presents a unified multimodal LLM\narchitecture, which encodes multi-view observations of 3D scenes into 3D-aware\nvisual tokens, and then jointly embeds these visual tokens with CoT-derived\ntextual tokens within LLMs to generate grasp pose predictions. Furthermore, we\npresent IntentGrasp, a large-scale benchmark that fills the gap in public\ndatasets for multi-object grasp detection under diverse and indirect verbal\ncommands. Extensive experiments on IntentGrasp demonstrate the superiority of\nour method, with additional validation in real-world robotic applications\nconfirming its practicality. The code is available at\nhttps://github.com/cxmomo/GraspCoT.\n","authors":["Xiaomeng Chu","Jiajun Deng","Guoliang You","Wei Liu","Xingchen Li","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16013v2.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2509.06953v1","updated":"2025-09-08T17:59:35Z","published":"2025-09-08T17:59:35Z","title":"Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for\n  Dynamic Environments","summary":"  Generating collision-free motion in dynamic, partially observable\nenvironments is a fundamental challenge for robotic manipulators. Classical\nmotion planners can compute globally optimal trajectories but require full\nenvironment knowledge and are typically too slow for dynamic scenes. Neural\nmotion policies offer a promising alternative by operating in closed-loop\ndirectly on raw sensory inputs but often struggle to generalize in complex or\ndynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural\nmotion policy designed for reactive motion generation in diverse dynamic\nenvironments, operating directly on point cloud sensory input. At its core is\nIMPACT, a transformer-based neural motion policy pretrained on 10 million\ngenerated expert trajectories across diverse simulation scenarios. We further\nimprove IMPACT's static obstacle avoidance through iterative student-teacher\nfinetuning. We additionally enhance the policy's dynamic obstacle avoidance at\ninference time using DCP-RMP, a locally reactive goal-proposal module. We\nevaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving\nobstacles, and goal obstructions. DRP achieves strong generalization,\noutperforming prior classical and neural methods in success rate across both\nsimulated and real-world settings. Video results and code available at\nhttps://deep-reactive-policy.com\n","authors":["Jiahui Yang","Jason Jingzhou Liu","Yulong Li","Youssef Khaky","Kenneth Shaw","Deepak Pathak"],"pdf_url":"https://arxiv.org/pdf/2509.06953v1.pdf","comment":"Website at \\url{deep-reactive-policy.com}"},{"id":"http://arxiv.org/abs/2509.06934v1","updated":"2025-09-08T17:46:00Z","published":"2025-09-08T17:46:00Z","title":"\"It was Tragic\": Exploring the Impact of a Robot's Shutdown","summary":"  It is well established that people perceive robots as social entities, even\nwhen they are not designed for social interaction. We evaluated whether the\nsocial interpretation of robotic gestures should also be considered when\nturning off a robot. In the experiment, participants engaged in a brief\npreliminary neutral interaction while a robotic arm showed interest in their\nactions. At the end of the task, participants were asked to turn off the\nrobotic arm under two conditions: (1) a Non-designed condition, where all of\nthe robot's engines were immediately and simultaneously turned off, as robots\ntypically shut down; (2) a Designed condition, where the robot's engines\ngradually folded inward in a motion resembling \"falling asleep.\" Our findings\nrevealed that all participants anthropomorphized the robot's movement when it\nwas turned off. In the Non-designed condition, most participants interpreted\nthe robot's turn-off movement negatively, as if the robot had \"died.\" In the\nDesigned condition, most participants interpreted it more neutrally, stating\nthat the robot \"went to sleep.\" The robot's turn-off movement also impacted its\nperception, leading to higher likeability, perceived intelligence, and animacy\nin the Designed condition. We conclude that the impact of common edge\ninteractions, such as turning off a robot, should be carefully designed while\nconsidering people's automatic tendency to perceive robots as social entities.\n","authors":["Agam Oberlender","Hadas Erel"],"pdf_url":"https://arxiv.org/pdf/2509.06934v1.pdf","comment":"8 pages, 4 figures, 1 table, submitted to IEEE RO-MAN 2025"},{"id":"http://arxiv.org/abs/2509.06893v1","updated":"2025-09-08T17:11:59Z","published":"2025-09-08T17:11:59Z","title":"Nanobot Algorithms for Treatment of Diffuse Cancer","summary":"  Motile nanosized particles, or \"nanobots\", promise more effective and less\ntoxic targeted drug delivery because of their unique scale and precision. We\nconsider the case in which the cancer is \"diffuse\", dispersed such that there\nare multiple distinct cancer sites. We investigate the problem of a swarm of\nnanobots locating these sites and treating them by dropping drug payloads at\nthe sites. To improve the success of the treatment, the drug payloads must be\nallocated between sites according to their \"demands\"; this requires extra\nnanobot coordination. We present a mathematical model of the behavior of the\nnanobot agents and of their colloidal environment. This includes a movement\nmodel for agents based upon experimental findings from actual nanoparticles in\nwhich bots noisily ascend and descend chemical gradients. We present three\nalgorithms: The first algorithm, called KM, is the most representative of\nreality, with agents simply following naturally existing chemical signals that\nsurround each cancer site. The second algorithm, KMA, includes an additional\nchemical payload which amplifies the existing natural signals. The third\nalgorithm, KMAR, includes another additional chemical payload which counteracts\nthe other signals, instead inducing negative chemotaxis in agents such that\nthey are repelled from sites that are already sufficiently treated. We present\nsimulation results for all algorithms across different types of cancer\narrangements. For KM, we show that the treatment is generally successful unless\nthe natural chemical signals are weak, in which case the treatment progresses\ntoo slowly. For KMA, we demonstrate a significant improvement in treatment\nspeed but a drop in eventual success, except for concentrated cancer patterns.\nFor KMAR, our results show great performance across all types of cancer\npatterns, demonstrating robustness and adaptability.\n","authors":["Noble Harasha","Nancy Lynch"],"pdf_url":"https://arxiv.org/pdf/2509.06893v1.pdf","comment":"Abridged abstract shown here; 34 pages, 9 figures"},{"id":"http://arxiv.org/abs/2509.06882v1","updated":"2025-09-08T17:01:04Z","published":"2025-09-08T17:01:04Z","title":"Dynamic Modeling and Efficient Data-Driven Optimal Control for Micro\n  Autonomous Surface Vehicles","summary":"  Micro Autonomous Surface Vehicles (MicroASVs) offer significant potential for\noperations in confined or shallow waters and swarm robotics applications.\nHowever, achieving precise and robust control at such small scales remains\nhighly challenging, mainly due to the complexity of modeling nonlinear\nhydrodynamic forces and the increased sensitivity to self-motion effects and\nenvironmental disturbances, including waves and boundary effects in confined\nspaces. This paper presents a physics-driven dynamics model for an\nover-actuated MicroASV and introduces a data-driven optimal control framework\nthat leverages a weak formulation-based online model learning method. Our\napproach continuously refines the physics-driven model in real time, enabling\nadaptive control that adjusts to changing system parameters. Simulation results\ndemonstrate that the proposed method substantially enhances trajectory tracking\naccuracy and robustness, even under unknown payloads and external disturbances.\nThese findings highlight the potential of data-driven online learning-based\noptimal control to improve MicroASV performance, paving the way for more\nreliable and precise autonomous surface vehicle operations.\n","authors":["Zhiheng Chen","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2509.06882v1.pdf","comment":"This work has been accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2509.06819v1","updated":"2025-09-08T15:55:50Z","published":"2025-09-08T15:55:50Z","title":"CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation\n  Policies and Teleoperation","summary":"  Learning-based controllers, such as diffusion policies and vision-language\naction models, often generate low-frequency or discontinuous robot state\nchanges. Achieving smooth reference tracking requires a low-level controller\nthat converts high-level targets commands into joint torques, enabling\ncompliant behavior during contact interactions. We present CRISP, a lightweight\nC++ implementation of compliant Cartesian and joint-space controllers for the\nROS2 control standard, designed for seamless integration with high-level\nlearning-based policies as well as teleoperation. The controllers are\ncompatible with any manipulator that exposes a joint-torque interface. Through\nour Python and Gymnasium interfaces, CRISP provides a unified pipeline for\nrecording data from hardware and simulation and deploying high-level\nlearning-based policies seamlessly, facilitating rapid experimentation. The\nsystem has been validated on hardware with the Franka Robotics FR3 and in\nsimulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid\nintegration, flexible deployment, and real-time performance, our implementation\nprovides a unified pipeline for data collection and policy execution, lowering\nthe barrier to applying learning-based methods on ROS2-compatible manipulators.\nDetailed documentation is available at the project website -\nhttps://utiasDSL.github.io/crisp_controllers.\n","authors":["Daniel San José Pro","Oliver Hausdörfer","Ralf Römer","Maximilian Dösch","Martin Schuck","Angela P. Schöllig"],"pdf_url":"https://arxiv.org/pdf/2509.06819v1.pdf","comment":"5 pages, 5 figures"}]}}